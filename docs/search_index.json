[["index.html", "R for Social Network Analysis 1 Introduction 1.1 Instructors", " R for Social Network Analysis CORE Lab 1 Introduction This document accompanies the CORE Labs Dynamic Network Analysis (DA4610) course, designed to introduce students to the use of the statistical package R, for engaging in social network analysis (SNA). It is not an introduction to SNA, however. It assumes that students have a background in SNA; in particular, they will be reasonably familiar with the various metrics and algorithms (e.g., topography, subgroups, centrality, and brokerage) that social network analysts regularly use. 1.1 Instructors Instructor Email Office Sean Everton sfeverto@nps.edu RH203 Dan Cunningham dtcunnin@nps.edu RH103i Chris Callaghan c.callaghan@nps.edu RH107 "],["setting-up.html", "2 Setting Up 2.1 R and RStudio Basics 2.2 Project Workflow", " 2 Setting Up In this document we cover the basics of working in R and the RStudio integrative development environment (IDE). The objective is getting you up and running in R as quickly as possible. To do so, this document borrows heavily from the tried and tested resources written by Wickham and Grolemund (2017) and Jenny Bryan and the Stat 545 teaching assistants at UBC (2018). We do not assume this is your first time working with R and RStudio. Consider this exercise and opportunity to to reintroduce yourself to some core workflow basics and set up for the rest of the course. 2.1 R and RStudio Basics Begin by launching RStudio, which will automatically instantiate a new session of R. Notice the default panes: Console (entire left): An R console integrated into the RStudio. Environment/History (upper right): The Environment tab is where R Studio displays all the data sets, objects, functions, etc. in memory. The history tab is a database of commands previously executed in the console. Files/Plots/Packages/Help (lower right): This is catchall of sorts. The Files tab displays the files and folders present in the working directory (more on this later), the Plots tab is the output location for graphics called from the console, the Packages tab allows users to interact (install, update, locate, or load) R packages, and finally the Help tab serves as a space for reviewing code documentation. Figure 2.1: Default RStudio Interface Please note that all panes are movable and can expand or contract. Do not be surprised if they appear or disappear. Also, the order of panes can be rearranged as some R users prefer changing the position these. 2.1.1 Basics of R Coding in RStudio Now turn your attention to the Console tab, which is where we interact with the R instance. Any inputs you type into the console will be evaluated and executed in real time. However, you should get in the habit of storing your code for use at a later time. To do so, open an R script, which is plain text file with R commands in it, in order to store your code. Locate the File drop down menu in the top ribbon, navigate to New File, and finally select R Script. Alternatively, press the keys Ctrl + Shift + N (on Mac Cmd + Shift + N). What happened? A new pane should appear with a blank R script, this is where you will write your code prior to executing it. Now what? Well this document assumes you have some experience working with R. Thus, rather than saturate you with repetitive details on data types and structures, the focus here is on reviewing key functions, best practices, and shortcuts that will improve your efficiency working with R in RStudio. You should consider the following: Learning R is much like learning a new language. You may want to start by focusing on a mastering few crucial words and expressions; then, work your way up to more complex grammar. We recommend that familiarize yourself with the following functions as a staring point: Function Description Example getwd() Return the filepath representing the current working directory of the R process. getwd() setwd() Set a working directory for the R process. setwd(\"~/PATH\") install.packages() Download and install packages from CRAN-like repositories or from local files. install.packages(igraph) library() Load add-on packages. library(igraph) c() Combines arguments to form a vector. c(\"This\", \"is\", \"a\", \"vector\", \".\") data.frame() Creates a data frame. my_df &lt;- data.frame(\"Source\" = c(\"Chris\"), \"Target\" = c(\"Eric\")) dim() Retrieve the dimension of an object. dim(my_df) names() Get or set the names of an object. names(my_df) View() Invoke a data viewer. View(my_df) class() Identify the class an object inherits from. class(my_df) typeof() Determine the type of any object. typeof(my_df) head() Returns the first part of an object (vector, data frame, etc). head(my_df) NROW() Return the number of rows present in a vector, array, or data frame. NROW(my_df) NCOL() Return the number of columns present in a vector, array, or data frame. NCOL(my_df) summary() Produce result summaries. summary(my_df) read.csv() Read file in comma-separated table format and create a data frame from it. read.csv(~/PATH/MY_FILE.csv) write.csv() Writes a data frame to a file. write.csv(my_df, file = \"MY_FILE.csv\") As with any function in R, you may want to look at the documentation for the commands above. This will provide you with additional information the functions purpose, input arguments, and expected output. To access the documentation for a given function, use the ? operator followed by the function name (e.g., ?help or ?getwd). All R statements where an object is created are assignments and look like this: object &lt;- value. You can read it, in your head, as object gets value. For example, x &lt;- 1 is x gets 1. You should always use the &lt;- operator in order to avoid confusion. As a suggestion, use spaces surrounding your assignment operator, make it easy to read your code at a later time. Now that you have wrapped your head around assignments, lets turn to how we name objects. These cannot begin with a digit or contain commas or spaces. Each R user has different a naming convention, we advise you to adopt one of the following: Snake Case: snake_case_object_name for example: my_object &lt;- 1 Camel Case: camelCaseObjectName for example: myObject &lt;- 1 Dots: dot.object.name for example: my.object &lt;- 1 Using dots is usually associated with S3 object method dispatching in R (e.g., plot.igraph() plots igraph class objects); thus, many R users avoid using dots to name objects. However, this is not a rule and will not typically impact your code. It is highly recommended that you document your code using comments. This practice will allow you to return to your code after time away and just as importantly share your code with others. R allows you to add notes and comments in your scripts and documentation. In order to insert notes or comments into your code, you should use the # symbol, which tells R to ignore the content to the right of the symbol. For example, notice the difference between the following two commands: # library(igraph) Vs. library(igraph) # This will load the igraph library into R&#39;s environment What is the difference? Code thinking of your future self. The prior two points have hinted this much. Include comments to help explain your thinking and use spacing to improve your codes readability. For example: my_df&lt;-data.frame(&quot;Source&quot;=c(&quot;Chris&quot;,&quot;Dan&quot;,&quot;Sean&quot;),&quot;Target&quot;=c(&quot;Dan&quot;,&quot;Sean&quot;,&quot;Chris&quot;)) Vs. # Create a data frame with two columns (Source and Target) to use as # an edge list: my_df &lt;- data.frame( &quot;Source&quot; = c(&quot;Chris&quot;, &quot;Dan&quot;, &quot;Sean&quot;), &quot;Target&quot; = c(&quot;Dan&quot;, &quot;Sean&quot;, &quot;Chris&quot;) ) Notice the difference? Reading code is hard on a good day, imagine what it would be like to engage with dense and poorly documented code on a bad one. Remember to leverage the R open-source community! You are probably not the first, nor the last, person learning R. R users are constantly sharing content, collaborating, and asking and answering questions on sites such as StackOverflow or the RStudio Support Site. Google is your friend! Keep in mind that half the battle in solving a problem is finding the right verbiage to describe the issue you have encountered to a search bar. While you should not be afraid to ask questions, you should do your due diligence before starting a new question on either StackOverflow or the RStudio Support Site. Otherwise, you may encounter a less than pleasant user pointing you to the previously asked and answered entry. Thus far, we have hinted at some keyboard shortcuts built into the RStudio IDE to make the coding experience more pleasant. For example, above we noted the keyboard shortcut to open a new script. There are many more that you may access using Alt + Shift + K, which will bring up a keyboard shortcut reference card. You may want to familiarize yourself with these as they will save you time and improve your experience using RStudio. Here is a list of shortcuts worth knowing: Shortcut Description Ctrl + Shift + N (Cmd + Shift + N on Mac) Start a new R script. Ctrl + S (Cmd + S on Mac) Save a script. Ctrl + Shift + C (Cmd + Shift + C on Mac) Comment or uncomment a line(s) of code. Ctrl + Enter (Cmd + Enter on Mac) Send a line or multiple lines of code from a script to the console. Ctrl + Alt + R (Cmd + Alt + R on Mac) Run the complete code in a script. 2.2 Project Workflow Up to this point, your analysis has lived in the working directory (see getwd()). This is the location where R looks for files to load and write any outputs. The R user community has moved away from setting working directories ad hoc for a variety of reason; namely: - Issues with path separators (e.g., \\ vs. /) across different operation systems - Hardcoding paths hinders sharing as no one else will have the same directory as you To overcome these hurdles, many R users keep all their files for a given project (e.g., class, analysis, etc.) in RStudio Projects. You can create one within RStudio by navigating to the File drop down menu at the top, then selecting New Project. Figure 2.2 shows the step-by-step process of setting up a project. Keep in mind that you can name your project just about anything; however, you should remember two things: Name it in a way that reflects the purpose of the project. For instance, if you are setting a project for a class, name it after a class. Names like my_project or using your name fail to provide context on the purpose of the project. Think carefully about where you put the project, make it easy to find in the future. Figure 2.2: Starting an RStudio Project Once you complete the project setup, check the working directory by executing the following command: getwd() You should be looking at the path to the project. When you are working in an RStudio Project, your working directory is the location of the project. Thus, as long as you place your data, files, or code inside the project, you wont have to set and reset the working directory. For example, lets create some data and save it. Open a new R script, copy the code below, and execute it: # First create a data set with random values: x &lt;- runif(40) y &lt;- x + rnorm(40, sd = 0.5^2) my_df &lt;- data.frame(&quot;x&quot; = x, &quot;y&quot; = y) # If you are curious about the distribution of the data, plot it: plot(my_df$x, my_df$y) # Now save your data: write.csv(x = my_df, file = &quot;test_data.csv&quot;) Where did the data write? As noted, it should be located in the project folder your created. If you dont remember where that may be, use getwd() for a hint. Save the script as lab0-setup.R (Ctrl/Cmd + S) and proceed to close your project. Locate the folder associate with your project, there you should see a file with the extension .Rproj, double click on it to reopen RStudio and load your project 2.3. Figure 2.3: Opening RStudio Project Notice that default, things are restored to where you left them off earlier. Your working directory should still be pointing at the project folder so you could begin your analysis right where you stopped. Furthermore, you wont have to reset you directory in order to access your data. For example, execute the following command: read.csv(&quot;test_data.csv&quot;) Hopefully, you can see the advantage of using RStudio Projects. You may or may not choose to use them in this class. However, you should be aware of them and their added advantages as they are commonly used by R users to power their analysis. References "],["importing-and-visualizing-one--and-two-mode-social-network-data-in-igraph.html", "3 Importing and Visualizing One- and Two-Mode Social Network Data in igraph 3.1 Setup 3.2 Load Libraries 3.3 One-mode Social Network Data in igraph: Koschade Network 3.4 Two-mode Social Network Data in igraph: Davis Southern Women", " 3 Importing and Visualizing One- and Two-Mode Social Network Data in igraph In this lab well explore a variety of methods for importing social network data into R, manipulating one- and two-mode network data, and visualizing social networks. Well be using a variety of social networks, some of which youll recognize from other classes. Well also illustrate a variety of ways to import network data, something that should be easy to do but often turns out to be challenging because a number of resources jump over this important step. Note: This lab has gone through many iterations and reflects the influence from a variety of individuals, including Phil Murphy, and Brendan Knapp. 3.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ####################################################################### # What: Importing and Visualizing One- and Two-Mode Social Network Data # File: lab1_igraph.R # Created: 02.28.14 # Revised: 01.06.22 ####################################################################### If you have not set up your RStudio Project to clear the workspace on exit, your environment contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (davis.csv, davis.net, davisedge.csv, Koschade Bali (Edge).csv, Koschade Bali (Matrix).csv, and Koschade Bali.net) also inside your R Project folder. We have placed it in a sub-folder titled data for organizational purposes; however, this is not necessary. 3.2 Load Libraries We need to load the libraries we plan to use. Here we will use igraph. Because igraph and statnet conflict with one another sometimes, we do not want to have them loaded at the same time, so you may want to detach it. Alternatively, you may choose to namespace functions using the :: operator as needed (e.g., igraph::betweenness() vs. sna::betweenness()). Of course, this applies only if you had the statnet package loaded already. The intergraph package allows users to transform network data back and forth between igraph and statnet. # If you haven&#39;t done so, install the required packages: # install.packages(&quot;igraph&quot;) # install.packages(&quot;intergraph&quot;) # Now load them: library(igraph) library(intergraph) Note: igraph imports the %&gt;% (piping) operator on load (library(igraph)). This lab leverages the operator because we find it very useful in chaining functions, although in doing so we will sometimes illustrate how to carry out the same operations using base R. 3.3 One-mode Social Network Data in igraph: Koschade Network Here, we will use data collected by Stuart Koschade of the 17 individuals who participated in the first Bali bombing. Koschade (2006) recorded both the ties between the individuals, as well as the strength of the tie between them. 3.3.1 Importing One-Mode Social Network Data 3.3.1.1 Option 1: Importing One-Mode Network Data in Matrix Format One way is to read network in from a matrix saved as a csv file. # First, read the csv file using the required arguments koschade_dat &lt;- read.csv(file = &quot;data/Koschade Bali (Matrix).csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) # Now, coerce the data.frame to a matrix koschade_mat &lt;- as.matrix(koschade_dat) The two-step operation above could be combined with and without piping. First, without: koschade_mat &lt;- as.matrix(read.csv(file = &quot;data/Koschade Bali (Matrix).csv&quot;, header = TRUE, row.names = 1, check.names = FALSE)) And now with: koschade_mat &lt;- read.csv(file = &quot;data/Koschade Bali (Matrix).csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) %&gt;% as.matrix() Turn the matrix into an igraph object using the graph_from_adjacency_matrix() function from the igraph library: koschade1_ig &lt;- graph_from_adjacency_matrix(adjmatrix = koschade_mat, mode = &quot;undirected&quot;, weighted = TRUE) Now that the matrix has been imported, lets examine the object. First, take a look at its class: class(koschade1_ig) [1] &quot;igraph&quot; What is it? If you ran the code above the printout on your console should read igraph. Many R objects have a class, which describes a type of object, the properties it possesses, how it behaves, and how it relates to other objects and functions (Wickham 2019). An igraph class denotes that this object is an igraph graph and that it will work with the functions from this library. One key characteristic of igraph graphs is that they are printed to the screen in a special format: koschade1_ig IGRAPH 0344ec3 UNW- 17 63 -- + attr: name (v/c), weight (e/n) + edges from 0344ec3 (vertex names): [1] Muklas --Amrozi Muklas --Imron Muklas --Samudra Muklas --Dulmatin [5] Muklas --Idris Muklas --Azahari Muklas --Ghoni Muklas --Patek [9] Muklas --Sarijo Amrozi --Samudra Amrozi --Idris Amrozi --Mubarok [13] Imron --Samudra Imron --Dulmatin Imron --Idris Imron --Azahari [17] Imron --Ghoni Imron --Patek Imron --Feri Imron --Sarijo [21] Samudra--Dulmatin Samudra--Idris Samudra--Mubarok Samudra--Azahari [25] Samudra--Ghoni Samudra--Arnasan Samudra--Rauf Samudra--Octavia [29] Samudra--Hidayat Samudra--Junaedi Samudra--Patek Samudra--Sarijo + ... omitted several edges This printout provides important information about the object. The first line starts with IGRAPH, which denotes that this is an igraph graph. The following seven character code is the unique id for the graph. The following four letters distinguish whether: The graph is directed (D) or undirected (U) The graph is named (e.g., vertex names are set) (N) The graph is weighted (W) The graph is bipartite (B) The second line (prefixed with +attr:) includes the attributes of the graph (g), the vertices (v), and edges (e). For instance, in this example, the name attribute is a vertex attribute, while weight is an edge attribute. The remainder of the printout (prefixed with + edges) includes a sample of the relationships in the graph. One advantage of storing data as an igraph object is that the library has functions to transform relational records into a variety of formats. For example, from igraph to an edge list. To do such, pass the koschade1_ig object to the get.data.frame() function. get.data.frame(koschade1_ig) %&gt;% head(5) from to weight 1 Muklas Amrozi 2 2 Muklas Imron 2 3 Muklas Samudra 1 4 Muklas Dulmatin 1 5 Muklas Idris 5 Similarly, edge and node attributes can be fetched back from the igraph object. For example, extract a vector of edge weights, which we can use later vary edge width in our visualizations. edge_weight_1 &lt;- get.edge.attribute(koschade1_ig, name = &quot;weight&quot;) # Attribute name 3.3.1.2 Option 2: Importing One-Mode Network Data as an Edge List Here is how we can import an edge list, and then check the first few rows with the head() command. koschade2_el &lt;- read.csv(file = &quot;data/Koschade Bali (Edge).csv&quot;, header = TRUE) head(koschade2_el) Source Target Weight 1 Muklas Amrozi 2 2 Muklas Imron 2 3 Muklas Samudra 1 4 Muklas Dulmatin 1 5 Muklas Idris 5 6 Muklas Azahari 1 Convert the edge list to an igraph object and check for basic information. koschade2_ig &lt;- graph_from_data_frame(d = koschade2_el, directed = FALSE) # Look at the printout koschade2_ig IGRAPH 036e1f0 UN-- 17 63 -- + attr: name (v/c), Weight (e/n) + edges from 036e1f0 (vertex names): [1] Muklas --Amrozi Muklas --Imron Muklas --Samudra Muklas --Dulmatin [5] Muklas --Idris Muklas --Azahari Muklas --Ghoni Muklas --Patek [9] Muklas --Sarijo Amrozi --Samudra Amrozi --Idris Amrozi --Mubarok [13] Imron --Samudra Imron --Dulmatin Imron --Idris Imron --Azahari [17] Imron --Ghoni Imron --Patek Imron --Feri Imron --Sarijo [21] Samudra--Dulmatin Samudra--Idris Samudra--Mubarok Samudra--Azahari [25] Samudra--Ghoni Samudra--Arnasan Samudra--Rauf Samudra--Octavia [29] Samudra--Hidayat Samudra--Junaedi Samudra--Patek Samudra--Sarijo + ... omitted several edges Heres how to do all that using piping: koschade2_ig &lt;- read.csv(file = &quot;data/Koschade Bali (Edge).csv&quot;, header = TRUE) %&gt;% graph_from_data_frame(directed = FALSE) Look at the printout koschade2_ig IGRAPH 03739d5 UN-- 17 63 -- + attr: name (v/c), Weight (e/n) + edges from 03739d5 (vertex names): [1] Muklas --Amrozi Muklas --Imron Muklas --Samudra Muklas --Dulmatin [5] Muklas --Idris Muklas --Azahari Muklas --Ghoni Muklas --Patek [9] Muklas --Sarijo Amrozi --Samudra Amrozi --Idris Amrozi --Mubarok [13] Imron --Samudra Imron --Dulmatin Imron --Idris Imron --Azahari [17] Imron --Ghoni Imron --Patek Imron --Feri Imron --Sarijo [21] Samudra--Dulmatin Samudra--Idris Samudra--Mubarok Samudra--Azahari [25] Samudra--Ghoni Samudra--Arnasan Samudra--Rauf Samudra--Octavia [29] Samudra--Hidayat Samudra--Junaedi Samudra--Patek Samudra--Sarijo + ... omitted several edges 3.3.1.3 Option 3: Importing One-Mode Network Data in Pajek Format Another way to bring the data into igraph is to import the data from the Pajek file (*.net). The read_graph() function is able to read graphs from multiple foreign formats. koschade3_ig &lt;- read.graph(file = &quot;data/Koschade Bali.net&quot;, format = &quot;pajek&quot;) # Look at the printout koschade3_ig IGRAPH 037e99d UNW- 17 63 -- + attr: id (v/c), name (v/c), x (v/n), y (v/n), z (v/n), weight (e/n) + edges from 037e99d (vertex names): [1] Muklas --Amrozi Muklas --Imron Muklas --Samudra Muklas --Dulmatin [5] Muklas --Idris Muklas --Azahari Muklas --Ghoni Muklas --Patek [9] Muklas --Sarijo Amrozi --Samudra Amrozi --Idris Amrozi --Mubarok [13] Imron --Samudra Imron --Dulmatin Imron --Idris Imron --Azahari [17] Imron --Ghoni Imron --Patek Imron --Feri Imron --Sarijo [21] Samudra--Dulmatin Samudra--Idris Samudra--Mubarok Samudra--Azahari [25] Samudra--Ghoni Samudra--Arnasan Samudra--Rauf Samudra--Octavia [29] Samudra--Hidayat Samudra--Junaedi Samudra--Patek Samudra--Sarijo + ... omitted several edges Transform the graph object to a data.frame and view it (View()). koschade3_ig %&gt;% get.data.frame(what = &quot;edges&quot;) %&gt;% head() from to weight 1 Muklas Amrozi 2 2 Muklas Imron 2 3 Muklas Samudra 1 4 Muklas Dulmatin 1 5 Muklas Idris 5 6 Muklas Azahari 1 3.3.1.4 Option 4: Importing One-Mode Network Data into statnet Format using intergraph If you first worked with the data in statnet, you can use intergraph to convert a statnet network object to an igraph object. The intergraph library lets you jump pretty smoothly between the data classes required by each library. To be clear: this command only works if you already have a statnet network object koschade_ig &lt;- asIgraph(koschade_net) Of course, you may begin working with data in igraph and have to convert it to statnet. Here we will take an igraph object and convert it to a network class object required by the statnet suite. Then, we will return that object from network to igraph class. # Transform an igraph object to network class koschade_network &lt;- asNetwork(koschade1_ig) # Print it koschade_network Network attributes: vertices = 17 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 63 missing edges= 0 non-missing edges= 63 Vertex attribute names: vertex.names Edge attribute names: weight Note the different printout. Also, you can verify the class change using the class() function. class(koschade_network) [1] &quot;network&quot; Now, return the network object back into igraph and view the data as an edge list. koschade_ig &lt;- asIgraph(koschade_network) koschade_ig %&gt;% get.data.frame(what = &quot;edges&quot;) %&gt;% head() from to na weight 1 1 2 FALSE 2 2 1 3 FALSE 2 3 1 4 FALSE 1 4 1 5 FALSE 1 5 1 6 FALSE 5 6 1 8 FALSE 1 What changed? Note that some variables and entries may have changed in the transition. 3.3.2 Plotting (Visualizing) the Koschade Network Heres a simple plot using igraph. plot(koschade1_ig) # Note that you can also plot the other two graph: # plot(koschade2_ig) # plot(koschade3_ig) Lets try making some more sophisticated plots. Before we do that, however, lets save the coordinates so that the remaining plots will have the same layout. Here, we will use the Fruchterman Reingold layout algorithm (layout_with_kk()). coords &lt;- layout_with_fr(koschade1_ig) Now, visualize the network with some additional parameters, such as changing the node color (i.e., vertex.color), the label size (i.e., vertex.label.cex), and the node label color (i.e., vertex.label.color). From here on out, well just use the koschade1_ig network. plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;) Now, lets size the edges by tie strength and plot again. plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, # Recall the edge weight vector previously created edge.width = edge_weight_1) Note that the layout above is the same as the previous layout. This is helpful when presenting successive graphs in your papers and theses. It makes it easier for readers to compare the network graphs. We can also change the edges to curved edges. plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, edge.width = edge_weight_1, edge.curved = TRUE) 3.3.3 Saving Network Plots (e.g., pdf, jpeg, png, tiff) Save final plot in various formats. Begin by saving the output in PDF format. To do such, use the pdf() function, which starts the graphics driver for producing PDFs. # Start the graphic driver, name output file, and set size pdf(file = &quot;koschade1.pdf&quot;, width = 4, height = 4) # Plot the output into the file plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, edge.width = edge_weight_1) # Turn off the graphics driver dev.off() To store the image as a JPEG, use the jpeg() function. The bg = \"transparent option saves the graphs with a transparent background (rather than white), which can be helpful when placing in slides or on non-white backgrounds. jpeg(file = &quot;koschade1.jpg&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 600, bg = &quot;transparent&quot;) plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, edge.width = edge_weight_1) dev.off() To store the image as a PNG, use the png() function. png(file = &quot;koschade1.png&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, edge.width = edge_weight_1) dev.off() To store the image as a TIFF, use the tiff() function. tiff(file = &quot;koschade3.tif&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) plot(koschade1_ig, layout = coords, vertex.color = &quot;Skyblue2&quot;, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, edge.width = edge_weight_1) dev.off() 3.3.4 Saving Network Data Finally, it doesnt hurt to save the data that youve imported and created. Perhaps not all (e.g., coordinates) but it is helpful to save those that you may want to use in another setting. save(koschade_dat, koschade_mat, koschade1_ig, koschade2_ig, koschade3_ig, file = &quot;koschade_igraph.RData&quot;) 3.4 Two-mode Social Network Data in igraph: Davis Southern Women We will now switch to another data set to import, manipulate, and visualize two-mode network data in igraph. The data that we will use here is what is known as Davis Southern Club Women. Davis and her colleagues recorded the observed attendance of 18 Southern women at 14 different social events. 3.4.1 Importing Two-Mode Social Network Data into igraph 3.4.1.1 Option 1: Importing Two-Mode Social Network Data in Matrix Format Once again, begin by reading the data from a CSV using read.csv(). This then transformed into a matrix, which, in turn, is turned into a bipartite (two-mode) network using the graph_from_incidence_matrix() igraph function. As before, we demonstrate how to do this with and without piping. First, without piping: davis1_dat &lt;- read.csv(file = &quot;data/davis.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) davis1_mat &lt;- as.matrix(davis1_dat) davis1_ig &lt;- graph_from_incidence_matrix(davis1_mat, directed = FALSE, weighted = NULL) Now, with piping: davis1_ig &lt;- read.csv(file = &quot;data/davis.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) %&gt;% as.matrix() %&gt;% graph_from_incidence_matrix(directed = FALSE, weighted = NULL) Now look at the graph printout: davis1_ig IGRAPH 04927ba UN-B 32 89 -- + attr: type (v/l), name (v/c) + edges from 04927ba (vertex names): [1] EVELYN --E1 EVELYN --E2 EVELYN --E3 EVELYN --E4 EVELYN --E5 [6] EVELYN --E6 EVELYN --E8 EVELYN --E9 LAURA --E1 LAURA --E2 [11] LAURA --E3 LAURA --E5 LAURA --E6 LAURA --E7 LAURA --E8 [16] THERESA --E2 THERESA --E3 THERESA --E4 THERESA --E5 THERESA --E6 [21] THERESA --E7 THERESA --E8 THERESA --E9 BRENDA --E1 BRENDA --E3 [26] BRENDA --E4 BRENDA --E5 BRENDA --E6 BRENDA --E7 BRENDA --E8 [31] CHARLOTTE--E3 CHARLOTTE--E4 CHARLOTTE--E5 CHARLOTTE--E7 FRANCES --E3 [36] FRANCES --E5 FRANCES --E6 FRANCES --E8 ELEANOR --E5 ELEANOR --E6 + ... omitted several edges Note the B letter in the four letter code string that describes the network qualities. Since the network was created using a function designed to create bipartite graphs, this feature is automatically added. 3.4.1.2 Option 2: Importing Two-Mode Social Network Data as an Edge List Note: This section is adapted from code written by Phil Murphy and Brendan Knapp. You may also begin creating your analysis of two-mode networks from an edge list. To do so, leverage the read.csv() function to ingest data. Then, pass the edge list to igraphs graph_from_data_frame() function. Again, first without piping and then with piping. Without: davis2_el &lt;- read.csv(file = &quot;data/davisedge.csv&quot;, header = TRUE) davis2_ig &lt;- graph_from_data_frame(davis2_el, directed = FALSE) With: davis2_ig &lt;- read.csv(file = &quot;data/davisedge.csv&quot;, header = TRUE) %&gt;% graph_from_data_frame(directed = FALSE) Now look at the graph printout: davis2_ig IGRAPH 04a58e8 UN-- 32 89 -- + attr: name (v/c), Weight (e/n) + edges from 04a58e8 (vertex names): [1] EVELYN --E1 EVELYN --E2 EVELYN --E3 EVELYN --E4 EVELYN --E5 [6] EVELYN --E6 EVELYN --E8 EVELYN --E9 LAURA --E1 LAURA --E2 [11] LAURA --E3 LAURA --E5 LAURA --E6 LAURA --E7 LAURA --E8 [16] THERESA --E2 THERESA --E3 THERESA --E4 THERESA --E5 THERESA --E6 [21] THERESA --E7 THERESA --E8 THERESA --E9 BRENDA --E1 BRENDA --E3 [26] BRENDA --E4 BRENDA --E5 BRENDA --E6 BRENDA --E7 BRENDA --E8 [31] CHARLOTTE--E3 CHARLOTTE--E4 CHARLOTTE--E5 CHARLOTTE--E7 FRANCES --E3 [36] FRANCES --E5 FRANCES --E6 FRANCES --E8 ELEANOR --E5 ELEANOR --E6 + ... omitted several edges This time, the B letter in the four letter code is not present. If you need further evidence that davis2_ig is not a bipartite graph, use the is_bipartite() function, which checks whether the graph is two-mode or not by checking if the nodes in the graph have an attribute called type. is_bipartite(davis2_ig) [1] FALSE At this point, the network is not a two-mode (bipartite) network. To tell igraph that it is, we can begin by using the bipartite.mapping() function, which can tell us whether the network meets the criteria of a two-mode network. Those criteria are that there are (1) two sets of nodes in the network, and (2) there are only ties between node sets and not within them. If the network meets the criteria, igraph will identify which nodes belong in each mode. bipartite_mapping(davis2_ig) $res [1] TRUE $type [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [13] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE [25] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE The function returns two responses. The first, denotes whether the network meets the criteria of a two-mode network ($res). The second, ($type) returns a logical vector denoting the mode to which each node belongs to. Thus, we can assign the type vector to the node attributes in the davis2_ig. V(davis2_ig)$type &lt;- bipartite_mapping(davis2_ig)[[&quot;type&quot;]] Once again, take a look at the printout: davis2_ig IGRAPH 04a58e8 UN-B 32 89 -- + attr: name (v/c), type (v/l), Weight (e/n) + edges from 04a58e8 (vertex names): [1] EVELYN --E1 EVELYN --E2 EVELYN --E3 EVELYN --E4 EVELYN --E5 [6] EVELYN --E6 EVELYN --E8 EVELYN --E9 LAURA --E1 LAURA --E2 [11] LAURA --E3 LAURA --E5 LAURA --E6 LAURA --E7 LAURA --E8 [16] THERESA --E2 THERESA --E3 THERESA --E4 THERESA --E5 THERESA --E6 [21] THERESA --E7 THERESA --E8 THERESA --E9 BRENDA --E1 BRENDA --E3 [26] BRENDA --E4 BRENDA --E5 BRENDA --E6 BRENDA --E7 BRENDA --E8 [31] CHARLOTTE--E3 CHARLOTTE--E4 CHARLOTTE--E5 CHARLOTTE--E7 FRANCES --E3 [36] FRANCES --E5 FRANCES --E6 FRANCES --E8 ELEANOR --E5 ELEANOR --E6 + ... omitted several edges Notice the B in the first line of the output. This tells us that igraph now recognizes the network as a bipartite/two-mode network. We can check it using a function again. is_bipartite(davis2_ig) [1] TRUE 3.4.1.3 Option 3: Importing Two-Mode Social Network Data in Pajek Format Read in the Pajek file using igraphs read.graph() function. davis3_ig &lt;- read.graph(&quot;data/davis.net&quot;, format = &quot;pajek&quot;) Once again, ensure that the network was read in correctly as two-mode (bipartite). is_bipartite(davis3_ig) [1] TRUE 3.4.2 Plotting Two-Mode Social Network Data in igraph Like with one-mode data, two-mode data can be plotted using igraph. Once again, we can use the plot() function to graph igraph objects. plot(davis1_ig) Once again, we can store the coordinates as a separate object and use it to compare networks. Here we will plot networks side-by-side using the par() function. Additionally, we can make some aesthetic improvements through adding arguments (e.g., vertex.label.cex, etc.). # Set graph parameters to 1 row and 3 columns par(mfrow = c(1, 3)) # Store node coordinates coordfr &lt;- layout_with_fr(davis1_ig) # Plot graphs plot(davis1_ig, layout = coordfr, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) plot(davis2_ig, layout = coordfr, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) plot(davis3_ig, layout = coordfr, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) Now, lets make a few adjustments to the graph; for instance, we can change the node colors to light blue and yellow to reflect node types. First, we need to determine what nodes belong to which mode. davis1_ig %&gt;% # Pull node list, which should include a &#39;type&#39; node attribute get.data.frame(&quot;vertices&quot;) %&gt;% # Cross tabulate the &#39;name&#39; and &#39;type&#39; variables table() name type BRENDA CHARLOTTE DOROTHY E1 E10 E11 E12 E13 E14 E2 E3 E4 E5 E6 E7 E8 E9 FALSE 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 TRUE 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 name type ELEANOR EVELYN FLORA FRANCES HELEN KATHERINE LAURA MYRNA NORA OLIVIA FALSE 1 1 1 1 1 1 1 1 1 1 TRUE 0 0 0 0 0 0 0 0 0 0 name type PEARL RUTH SYLVIA THERESA VERNE FALSE 1 1 1 1 1 TRUE 0 0 0 0 0 The output indicates that the women are assigned to the FALSE category, while the events fall under TRUE. Thus, we can assign colors using a conditional statement (e.g., ifelse()). plot(davis1_ig, layout = coordfr, # Get the vertex attribute vector, if the attribute is TRUE assign &#39;yellow&#39; # as the vertex color. Otherwise, assign &#39;lightblue&#39;. vertex.color = ifelse(get.vertex.attribute(davis1_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) Now, re-plot the networks with the new colors and saved the coordinate while were at it. # Set graph parameters to 1 row and 3 columns par(mfrow = c(1, 3)) # Plot graphs plot(davis1_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis1_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = 10) plot(davis2_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis2_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = 10) plot(davis3_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis3_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = 10) We may want to rescale nodes to reflect those with more adjacent nodes. To do so, we can use the degree() function to calculate a nodes degree centrality. If this measure is not familiar to you yet, dont worry, we will expand on this topic later in the class. degree(davis1_ig) EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL 8 7 8 7 4 4 4 3 RUTH VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY 4 4 4 6 7 8 5 2 OLIVIA FLORA E1 E2 E3 E4 E5 E6 2 2 3 3 6 4 8 8 E7 E8 E9 E10 E11 E12 E13 E14 10 14 12 5 4 6 3 3 As you can see, the output of the degree() function is a named vector with a score for the number of edges a given node has. Now, lets calculate degree centrality and then plot the graphs again but adjust the node size to reflect degree which weve rescaled in order to make the nodes more visible. # Set graph parameters to 1 row and 3 columns par(mfrow = c(1, 3)) # Plot graphs plot(davis1_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis1_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis1_ig)) plot(davis2_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis2_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis2_ig)) plot(davis3_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis3_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis3_ig)) 3.4.3 Projecting (Folding) Two-Mode Networks into One-Mode Networks in igraph For this section, we will just work with the davis1_ig network object. 3.4.3.1 Multiplying Matrices To transform the network into two one-mode networks, we first convert the two-mode igraph object to a matrix. The key function here is get.incidence(). Note the first command makes sure that the matrix has labels once it is transformed. davis1_mat &lt;- davis1_ig %&gt;% # Set vertex attribute &#39;id&#39; using the &#39;name&#39; attribute set_vertex_attr(name = &quot;id&quot;, value = V(.)$name) %&gt;% # The . represents davis1_ig get.incidence() View the matrix: davis1_mat Next, multiply the matrices by their transpose. To do so, we use the %*% operator to multiply networks and the t() function to transpose one matrix during the multiplication. First, lets create a one-mode matrix of women-to-women based on shared events. davis_women_mat &lt;- davis1_mat %*% t(davis1_mat) Repeat the process, this time switch the order of the transposed matrix to generate an events-to-events matrix. davis_events_mat &lt;- t(davis1_mat) %*% davis1_mat Take a look at one or both of the matrices: # Woman-to-woman matrix davis_women_mat # Event-to-event matrix davis_events_mat With the matrices projected, you can now convert both to igraph objects using the graph.adjacency() function. davis_women_ig &lt;- graph.adjacency(davis_women_mat, mode = &quot;undirected&quot;, weighted = TRUE) The next step is to remove loops and multiple edges; that is acomplished using the simplify() function. davis_women_ig &lt;- simplify(davis_women_ig, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = sum) davis_women_ig IGRAPH 05e2f18 UNW- 18 139 -- + attr: name (v/c), weight (e/n) + edges from 05e2f18 (vertex names): [1] EVELYN --LAURA EVELYN --THERESA EVELYN --BRENDA EVELYN --CHARLOTTE [5] EVELYN --FRANCES EVELYN --ELEANOR EVELYN --PEARL EVELYN --RUTH [9] EVELYN --VERNE EVELYN --MYRNA EVELYN --KATHERINE EVELYN --SYLVIA [13] EVELYN --NORA EVELYN --HELEN EVELYN --DOROTHY EVELYN --OLIVIA [17] EVELYN --FLORA LAURA --THERESA LAURA --BRENDA LAURA --CHARLOTTE [21] LAURA --FRANCES LAURA --ELEANOR LAURA --PEARL LAURA --RUTH [25] LAURA --VERNE LAURA --MYRNA LAURA --KATHERINE LAURA --SYLVIA [29] LAURA --NORA LAURA --HELEN LAURA --DOROTHY THERESA--BRENDA + ... omitted several edges Keep in mind that many steps can be put into a pipeline to reduce the lines of code. davis_events_ig &lt;- graph.adjacency(davis_events_mat, mode = &quot;undirected&quot;, weighted = TRUE) %&gt;% simplify(remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = sum) davis_events_ig IGRAPH 05e896b UNW- 14 66 -- + attr: name (v/c), weight (e/n) + edges from 05e896b (vertex names): [1] E1 --E2 E1 --E3 E1 --E4 E1 --E5 E1 --E6 E1 --E7 E1 --E8 E1 --E9 [9] E2 --E3 E2 --E4 E2 --E5 E2 --E6 E2 --E7 E2 --E8 E2 --E9 E3 --E4 [17] E3 --E5 E3 --E6 E3 --E7 E3 --E8 E3 --E9 E4 --E5 E4 --E6 E4 --E7 [25] E4 --E8 E4 --E9 E5 --E6 E5 --E7 E5 --E8 E5 --E9 E6 --E7 E6 --E8 [33] E6 --E9 E6 --E10 E6 --E11 E6 --E12 E6 --E13 E6 --E14 E7 --E8 E7 --E9 [41] E7 --E10 E7 --E11 E7 --E12 E7 --E13 E7 --E14 E8 --E9 E8 --E10 E8 --E11 [49] E8 --E12 E8 --E13 E8 --E14 E9 --E10 E9 --E11 E9 --E12 E9 --E13 E9 --E14 [57] E10--E11 E10--E12 E10--E13 E10--E14 E11--E12 E11--E13 E11--E14 E12--E13 + ... omitted several edges 3.4.3.2 Projecting Two-Mode igraph Graphs Beyond multiplying matrices, the process of transforming two-mode data to one-mode can be fully accomplished using functions from the igraph library. Remember that two-mode graph objects in igraph have a type vertex attribute, which can be called using the get.vertex.attribute() and can be used (under the hood) to determine if a graph is two-mode using the is_biparite(). # Pull &#39;type&#39; vertex attribute get.vertex.attribute(davis1_ig, name = &quot;type&quot;) # Test if it is two-mode is_bipartite(davis1_ig) If a graph is in fact two mode, we can transform it to one-mode using the bipartite_projection() function. The events are assigned to the TRUE mode. As such, we can specify which mode will be extracted from the two-mode network setting the which argument to true davis_events_ig &lt;- bipartite_projection(davis1_ig, which = &quot;true&quot;) davis_events_ig IGRAPH 065c677 UNW- 14 66 -- + attr: name (v/c), weight (e/n) + edges from 065c677 (vertex names): [1] E1 --E2 E1 --E3 E1 --E4 E1 --E5 E1 --E6 E1 --E8 E1 --E9 E1 --E7 [9] E2 --E3 E2 --E4 E2 --E5 E2 --E6 E2 --E8 E2 --E9 E2 --E7 E3 --E4 [17] E3 --E5 E3 --E6 E3 --E8 E3 --E9 E3 --E7 E4 --E5 E4 --E6 E4 --E8 [25] E4 --E9 E4 --E7 E5 --E6 E5 --E8 E5 --E9 E5 --E7 E6 --E8 E6 --E9 [33] E6 --E7 E6 --E10 E6 --E11 E6 --E12 E6 --E13 E6 --E14 E7 --E8 E7 --E9 [41] E7 --E12 E7 --E10 E7 --E13 E7 --E14 E7 --E11 E8 --E9 E8 --E12 E8 --E10 [49] E8 --E13 E8 --E14 E8 --E11 E9 --E12 E9 --E10 E9 --E13 E9 --E14 E9 --E11 [57] E10--E12 E10--E13 E10--E14 E10--E11 E11--E12 E11--E13 E11--E14 E12--E13 + ... omitted several edges Now extract the women one-mode network setting the which argument to false in the bipartite_projection() function. davis_women_ig &lt;- bipartite_projection(davis1_ig, which = &quot;false&quot;) davis_women_ig IGRAPH 06631e6 UNW- 18 139 -- + attr: name (v/c), weight (e/n) + edges from 06631e6 (vertex names): [1] EVELYN --LAURA EVELYN --BRENDA EVELYN --THERESA EVELYN --CHARLOTTE [5] EVELYN --FRANCES EVELYN --ELEANOR EVELYN --RUTH EVELYN --PEARL [9] EVELYN --NORA EVELYN --VERNE EVELYN --MYRNA EVELYN --KATHERINE [13] EVELYN --SYLVIA EVELYN --HELEN EVELYN --DOROTHY EVELYN --OLIVIA [17] EVELYN --FLORA LAURA --BRENDA LAURA --THERESA LAURA --CHARLOTTE [21] LAURA --FRANCES LAURA --ELEANOR LAURA --RUTH LAURA --PEARL [25] LAURA --NORA LAURA --VERNE LAURA --SYLVIA LAURA --HELEN [29] LAURA --MYRNA LAURA --KATHERINE LAURA --DOROTHY THERESA--BRENDA + ... omitted several edges 3.4.4 Plotting Projected One-Mode Networks Now that we have extracted the one-mode networks, plot the two new graphs using plot() and the additional arguments used previously. # Set graph parameters to 1 row and 2 columns par(mfrow = c(1, 2)) # Store node coordinates coords_women &lt;- layout_with_fr(davis_women_ig) coords_events &lt;- layout_with_fr(davis_events_ig) # Plot graphs plot(davis_women_ig, layout = coords_women, vertex.color = &quot;light blue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis_women_ig)) plot(davis_events_ig, layout = coords_events, vertex.color = &quot;yellow&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis_events_ig)) 3.4.5 Saving Network Plots Now, save plots of the two-mode network and the two one-mode networks produced. png(file = &quot;davis1.png&quot;,width = 4,height = 4,units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) plot(davis1_ig, layout = coordfr, vertex.color = ifelse(get.vertex.attribute(davis1_ig, name = &quot;type&quot;), &quot;yellow&quot;, &quot;lightblue&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis1_ig)) dev.off() png(file = &quot;daviswomen.png&quot;,width = 4,height = 4,units = &#39;in&#39;,res = 300, bg = &quot;transparent&quot;) plot(davis_women_ig, layout = coords_women, vertex.color = &quot;light blue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis_women_ig)) dev.off() png(file = &quot;davisevents.png&quot;,width = 4,height = 4,units = &#39;in&#39;,res = 300, bg = &quot;transparent&quot;) plot(davis_events_ig, layout = coords_events, vertex.color = &quot;yellow&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = degree(davis_events_ig)) dev.off() 3.4.6 Saving Network Data Once again, it doesnt hurt to save the data that youve imported and created. save(davis_mat, davis1_mat, davis1_ig, davis2_ig, davis3_ig, davis_events_ig, davis_events_mat, davis_women_ig, davis_women_mat, file = &quot;data/davis_igraph.RData&quot;) Thats all for igraph for now. References "],["importing-and-visualizing-one--and-two-mode-social-network-data-in-statnet.html", "4 Importing and Visualizing One- and Two-Mode Social Network Data in statnet 4.1 Setup 4.2 Load Libraries 4.3 One-mode Social Network Data in statnet: Koschade Network 4.4 Two-Mode Social Network Data in statnet: Davis Southern Women", " 4 Importing and Visualizing One- and Two-Mode Social Network Data in statnet In this lab well explore a variety of methods for importing social network data into R, manipulating one- and two-mode network data, and visualizing social networks. Well be using a variety of social networks, some of which youll recognize from other classes. Well also illustrate a variety of ways to import network data, something that should be easy to do but often turns out to be challenging because a number of resources jump over this important step. Note: This lab has gone through many iterations and reflects the influence from a variety of individuals, including Phil Murphy, and Brendan Knapp. 4.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ####################################################################### # What: Importing and Visualizing One- and Two-Mode Social Network Data # File: lab1_statnet.R # Created: 02.28.14 # Revised: 01.05.22 ####################################################################### If you have not set up your RStudio Project to clear the workspace on exit, your environment contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (davis.csv, davis.net, davisedge.csv, Koschade Bali (Edge).csv, Koschade Bali (Matrix).csv, and Koschade Bali.net) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. 4.2 Load Libraries We need to load the libraries we plan to use. Here we will use statnet. Because igraph and statnet conflict with one another sometimes, we do not want to have them loaded at the same time, so you may want to detach it. Alternatively, you may choose to namespace functions using the :: operator as needed (e.g., igraph::betweenness() vs. sna::betweenness()). Of course, this applies only if you had the igraph package loaded already. The intergraph package allows users to transform network data back and forth between igraph and statnet. # If you haven&#39;t done so, install the required packages: # install.packages(&quot;statnet&quot;) # install.packages(&quot;intergraph&quot;) # Now load them: library(statnet) library(intergraph) 4.3 One-mode Social Network Data in statnet: Koschade Network Here, we will use data collected by Stuart Koschade of the 17 individuals who participated in the first Bali bombing. Koschade (2006) recorded both the ties between the individuals, as well as the strength of the tie between them. 4.3.1 Importing One-Mode Social Network Data 4.3.1.1 Option 1: Importing One-Mode Social Network Data in Matrix Format We can import the network data as a matrix, first using the as.matrix() function, and then transforming into a network object, which is the object class used by statnet, using the as.network() function. # Here we are nesting functions, the inner functions are evaluated first. koschade1_net &lt;- as.network( as.matrix( read.csv(&quot;data/Koschade Bali (Matrix).csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) ), # Arguments for the as.network() function. directed = FALSE, ignore.eval = FALSE ) Heres another way to write the same command: koschade1_net &lt;- as.network( as.matrix( read.csv(&quot;data/Koschade Bali (Matrix).csv&quot;, header = TRUE, row.names = 1, check.names = FALSE)), # Arguments for the as.network() function. directed = FALSE, ignore.eval = FALSE) Now that the data has been imported, lets examine the object. First, take a look at its class: class(koschade1_net) [1] &quot;network&quot; What is it? The printout should read network which is a statnet graph object that works with the functions from this library. Many R objects have a class, which describes a type of object, describing the properties it possesses, how it behaves, and how it relates to other objects and functions (Wickham 2019). By typing the name of the network object into the console, we can get basic information about it. koschade1_net Network attributes: vertices = 17 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 63 missing edges= 0 non-missing edges= 63 Vertex attribute names: vertex.names Edge attribute names: 1 Like with igraph we can retrieve and store attribute data for the graph, vertices (e.g., actor names) or edges (e.g., edge weight) on the graph object. Note that there are multiple ways of retrieving the vertex attributes, such as actor names. get.vertex.attribute(koschade1_net, &quot;vertex.names&quot;) [1] &quot;Muklas&quot; &quot;Amrozi&quot; &quot;Imron&quot; &quot;Samudra&quot; &quot;Dulmatin&quot; &quot;Idris&quot; [7] &quot;Mubarok&quot; &quot;Azahari&quot; &quot;Ghoni&quot; &quot;Arnasan&quot; &quot;Rauf&quot; &quot;Octavia&quot; [13] &quot;Hidayat&quot; &quot;Junaedi&quot; &quot;Patek&quot; &quot;Feri&quot; &quot;Sarijo&quot; network.vertex.names(koschade1_net) [1] &quot;Muklas&quot; &quot;Amrozi&quot; &quot;Imron&quot; &quot;Samudra&quot; &quot;Dulmatin&quot; &quot;Idris&quot; [7] &quot;Mubarok&quot; &quot;Azahari&quot; &quot;Ghoni&quot; &quot;Arnasan&quot; &quot;Rauf&quot; &quot;Octavia&quot; [13] &quot;Hidayat&quot; &quot;Junaedi&quot; &quot;Patek&quot; &quot;Feri&quot; &quot;Sarijo&quot; There could be more vertex attributes, which can be called using the attribute name and the get.vertex.attribute() function. If you are not certain what the attribute is named, use the list.vertex.attributes() function to get a printout of the possible variable names. list.vertex.attributes(koschade1_net) [1] &quot;na&quot; &quot;vertex.names&quot; Similarly, we can access edge attribute data using statnet functions. # The edge weights are stored in a variable named &#39;1&#39; get.edge.attribute(koschade1_net, &quot;1&quot;) [1] 2 2 1 1 5 1 1 1 1 2 4 5 3 5 3 5 5 5 1 5 2 5 2 2 2 2 2 2 2 2 2 2 2 5 5 5 1 5 [39] 2 2 2 2 2 5 2 1 2 5 1 5 2 2 2 2 2 2 2 2 2 2 1 5 1 Once again, you can always get a list of potential edge attribute variable names. network::list.edge.attributes(koschade1_net) [1] &quot;1&quot; &quot;na&quot; 4.3.1.2 Option 2: Importing One-Mode Social Network Data as an Edge List We can also begin by importing an edge list. Like before, we will begin by reading the data into R using the base function read.csv() and examining the data. Then, we will pass the data to the statnet function as.network(), which constructs a network object. koschade_el &lt;- read.csv(&quot;data/Koschade Bali (Edge).csv&quot;, header = TRUE) # Examine top 5 rows head(koschade_el, 5) Source Target Weight 1 Muklas Amrozi 2 2 Muklas Imron 2 3 Muklas Samudra 1 4 Muklas Dulmatin 1 5 Muklas Idris 5 Next, we convert it to a network object. koschade2_net &lt;- as.network(koschade_el, matrix.type = &quot;edgelist&quot;, directed = FALSE, ignore.eval = FALSE) Type the object name to get a printout with basic information about the network. Note that here the edge weight attribute is called Weight. koschade2_net Network attributes: vertices = 17 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 63 missing edges= 0 non-missing edges= 63 Vertex attribute names: vertex.names Edge attribute names: Weight 4.3.1.3 Option 3: Importing One-Mode Social Network Data in Pajek Format We can also read network data in from a Pajek file (*.net extension) and retrieve/check basic information about the network. koschade3_net &lt;- read.paj(&quot;data/Koschade Bali.net&quot;) koschade3_net Network attributes: vertices = 17 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE title = Koschade Bali total edges= 63 missing edges= 0 non-missing edges= 63 Vertex attribute names: vertex.names x y z Edge attribute names: Koschade Bali Here, the edge weight attribute is imported by default as Koschade Bali, which may be misleading. Luckily, the read.paj() function has an optional argument to provide the name for the edge variable read from the file. koschade3_net &lt;- read.paj(&quot;data/Koschade Bali.net&quot;, edge.name = &quot;Weight&quot;) koschade3_net Network attributes: vertices = 17 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE title = Koschade Bali total edges= 63 missing edges= 0 non-missing edges= 63 Vertex attribute names: vertex.names x y z Edge attribute names: Weight Note that in addition to edge attributes, importing Pajek files includes coordinates of the Pajek layout, stored as x, y, and z. Once again, you can access these attributes using statnets get.vertex.attribute()function. get.vertex.attribute(koschade3.net, &quot;x&quot;) get.vertex.attribute(koschade3.net, &quot;y&quot;) get.vertex.attribute(koschade3.net, &quot;z&quot;) 4.3.1.4 Option 4: Importing One-Mode Social Network Data in igraph Format using intergraph You may find yourself working with data in statnet and have to convert it to igraph. Luckily, the intergraph library lets you jump pretty smoothly between the data classes required by each library. Here we will take an network object and convert it to a igraph class object required by the igraph library. Then, we will return that object from igraph to network class. # Transform an igraph object to network class koschade1_ig &lt;- asIgraph(koschade1_net) # Print it koschade1_ig IGRAPH 0858871 U--- 17 63 -- + attr: na (v/l), vertex.names (v/c), X1 (e/n), na (e/l) + edges from 0858871: [1] 1-- 2 1-- 3 1-- 4 1-- 5 1-- 6 1-- 8 1-- 9 1--15 1--17 2-- 4 [11] 2-- 6 2-- 7 3-- 4 3-- 5 3-- 6 3-- 8 3-- 9 3--15 3--16 3--17 [21] 4-- 5 4-- 6 4-- 7 4-- 8 4-- 9 4--10 4--11 4--12 4--13 4--14 [31] 4--15 4--17 5-- 6 5-- 8 5-- 9 5--15 5--16 5--17 6-- 7 6-- 8 [41] 6-- 9 6--15 6--17 8-- 9 8--15 8--16 8--17 9--15 9--16 9--17 [51] 10--11 10--12 10--13 10--14 11--12 11--13 11--14 12--13 12--14 13--14 [61] 15--16 15--17 16--17 Note the different printout. Also, you can verify the class change using the class() function. class(koschade1_ig) [1] &quot;igraph&quot; Now, return the igraph object back into network, extract the edge list and print the top 5 rows. koschade_network &lt;- asNetwork(koschade1_ig) # 1. Extract the edge list with as.data.frame.network() # 2. Print only top rows with head() head(as.data.frame.network(koschade_network)) .tail .head X1 1 Muklas Amrozi 2 2 Muklas Imron 2 3 Muklas Samudra 1 4 Muklas Dulmatin 1 5 Muklas Idris 5 6 Muklas Azahari 1 What changed? Note that some variables and entries may have changed in the transition. 4.3.2 Plotting (Visualizing) the Koschade Network Plotting in statnet is fairly straight forward. The primary function is gplot(), which produces a two dimensional network visualization and allows you to control vertex placements, edge characteristics, colors, etc. We suggest that you take a quick look at the documentation using the command ?gplot. To get us started with lets compare a base visualization against a much more refined graph. The first uses statnets defaults, for the second we will modify many of the arguments. In particular, the second tells R that the network is a one-mode network (gmode = graph rather than digraph, which is the default), adds labels using the network.vertex.names() function , colors the labels black, places them in the center of the nodes (label.pos = 5), and changes their size (label.cex = 1.6). The next series of arguments set the size and color of the vertices, hides the arrows, and colors the ties (edges) gray. Note that we saved the coordinates so both plots would have the same layout # Set graph parameters to 1 row and 2 columns par(mfrow = c(1, 2)) # Save coordinates in an object coords &lt;- network.layout.kamadakawai(koschade1_net, # The function expects a list of parameters # pass a NULL to use defaults layout.par = NULL) # Plot base graph gplot(koschade1_net, coord = coords) # Plot graph using vertex coordinates and additional arugments gplot(koschade1_net, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(koschade1_net), label.col = &quot;black&quot;, label.pos = 5, label.cex = 0.5, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) In the previous visualizations, we used the Kamada and Kawai algorithm to layout the nodes. By default, gplot() uses the Fruchterman and Reingold algorithm to determine the positions of nodes. Lets compare the visual output of three layout algorithms: Kamada and Kawai, Fruchterman and Reigold, and circle. Please note that many other layouts exist, for a more indepth list look at the documenation ?gplot.layout. # Set graph parameters to 1 row and 3 columns par(mfrow = c(1, 3)) # Kamada and Kawai gplot(koschade1_net, gmode = &quot;graph&quot;, mode = &quot;kamadakawai&quot;, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) # Kamada and Kawai gplot(koschade1_net, gmode = &quot;graph&quot;, mode = &quot;fruchtermanreingold&quot;, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) # Circle gplot(koschade1_net, gmode = &quot;graph&quot;, mode = &quot;circle&quot;, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) Before we move forward, lets take a look at three more arguments that can grately improve the look of your graphs. First, the jitter argument insures that gplot() does not draw vertices on top of one another. Second, remember that the edge and vertex attributes can be called and used to aid the visuals. Here we use the get.edge.attribute() function to call the edge weight vector (1) and rescale the thickness of these. Finally, we can curve edges setting usecurve to TRUE. gplot(koschade1_net, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(koschade1_net), label.col = &quot;black&quot;, label.pos = 5, label.cex = 0.5, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;, # New arguments jitter = TRUE, edge.lwd = get.edge.attribute(koschade1_net, &quot;1&quot;), usecurve = TRUE, edge.curve = .1) 4.3.3 Saving Network Plots (e.g., pdf, jpeg, png, tiff) Save final plot in various formats. Begin by saving the output in PDF format. To do such, use the pdf() function, which starts the graphics driver for producing PDFs. # Start the graphic driver, name output file, and set size pdf(file = &quot;koschade1.pdf&quot;, width = 4, height = 4) # Plot the output into the file gplot(koschade1_net, gmode = &quot;graph&quot;, coord = coords, jitter = TRUE, label = network.vertex.names(koschade1_net), label.col = &quot;black&quot;, label.pos = 5, label.cex = 0.5, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) # Turn off the graphics driver dev.off() To store the image as a JPEG, use the jpeg() function. The bg = \"transparent\" option saves the graphs with a transparent background (rather than white), which can be helpful when placing in slides or on non-white backgrounds. jpeg(file = &quot;koschade1.jpg&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 600, bg = &quot;transparent&quot;) gplot(koschade1_net, gmode = &quot;graph&quot;, coord = coords, jitter = TRUE, label = network.vertex.names(koschade1_net), label.col = &quot;black&quot;, label.pos = 5, label.cex = 0.5, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) dev.off() To store the image as a PNG, use the png() function. png(file = &quot;koschade1.png&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) gplot(koschade1_net, gmode = &quot;graph&quot;, coord = coords, jitter = TRUE, label = network.vertex.names(koschade1_net), label.col = &quot;black&quot;, label.pos = 5, label.cex = 0.5, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) dev.off() To store the image as a TIFF, use the tiff() function. tiff(file = &quot;koschade3.tif&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) gplot(koschade1_net, gmode = &quot;graph&quot;, coord = coords, jitter = TRUE, label = network.vertex.names(koschade1_net), label.col = &quot;black&quot;, label.pos = 5, label.cex = 0.5, vertex.cex = 1.6, vertex.col = &quot;light blue&quot;, usearrows = FALSE, edge.col = &quot;gray&quot;) dev.off() 4.3.4 Saving Network Data Finally, it doesnt hurt to save the data that youve imported and created. Perhaps not all (e.g., coordinates) but it is helpful to save those that you may want to use in another setting. save(koschade_el, koschade1_net, koschade2_net, koschade3_net, file = &quot;koschade_statnet.RData&quot;) 4.4 Two-Mode Social Network Data in statnet: Davis Southern Women We will now switch to another data set to import, manipulate, and visualize two-mode network data in statnet. The data that we will use here is what is known as Davis Southern Club Women. Davis and her colleagues recorded the observed attendance of 18 Southern women at 14 different social events. Recall that in two-mode network ties only exist between modes. That means that ties are only possible between women and events, not between women and women or between events and events. Any direct ties between nodes within a mode may be derived (projected), as we will do below. But they should not appear within the original network. 4.4.1 Importing Two-Mode Network Data 4.4.1.1 Option 1: Importing Two-Mode Network Data in Matrix Format Lets begin my importing two-mode network data thats recorded in matrix format (i.e., an incidence matrix). davis_mat &lt;- as.matrix( read.csv(&quot;data/davis.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) ) Convert the matrix into a network object with the as.network() function, specifying that the network is bipartite and directed through the appropriate arguments. davis1_net &lt;- as.network(davis_mat, # Should the network be interpreted as bipartite? bipartite = TRUE, # Should the edges be interpreted as directed? directed = FALSE, # Ignore edge values? ignore.eval = FALSE, # Optional edgeset constructor argument: matrix.type = &quot;incidence&quot;) davis1_net Network attributes: vertices = 32 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = 18 total edges= 89 missing edges= 0 non-missing edges= 89 Vertex attribute names: vertex.names Edge attribute names: NULL Note that you can use the is.bipartite() function to make sure the object is indeed a bipartite (two-mode) network. is.bipartite(davis1_net) [1] TRUE 4.4.1.2 Option 2: Importing Two-Mode Network Data as an Edge List Lets begin by importing an edge list and then check the first few rows with the head() command. davis_el &lt;- read.csv(&quot;data/davisedge.csv&quot;, header = TRUE) head(davis_el) Source Target Weight 1 EVELYN E1 1 2 EVELYN E2 1 3 EVELYN E3 1 4 EVELYN E4 1 5 EVELYN E5 1 6 EVELYN E6 1 As you can see, the first column is the women and the second is the events they attended. This is how a two-mode edge list should be organized: the first mode will be whatever is represented in the first column and the second mode represented in the second. To read a bipartite edge list to statnet use the as.network() function like before. Specify the matrix.type as edgelist, directed = TRUE, and bipartite = TRUE. davis2_net &lt;- as.network(davis_el, matrix.type = &quot;edgelist&quot;, directed = FALSE, bipartite = TRUE) davis2_net Network attributes: vertices = 32 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = 18 total edges= 89 missing edges= 0 non-missing edges= 89 Vertex attribute names: vertex.names Edge attribute names: Weight Lets check the graph object by plotting it. Once again, you will have to specify the type of graph being evaluated by gplot(); to do so, set the gmode argument to twomode. Note that the women are colored red and events are colored red. gplot(davis2_net, gmode = &quot;twomode&quot;, usearrows = FALSE, displaylabels = TRUE, label.pos = 5, label.cex = .6) 4.4.1.3 Option 3: Importing Two-Mode Social Network Data in Pajek Format We can also read two-mode network data into R from a Pajek network file. To do so, we will use the read.paj() function, then look at the printout to ensure the import worked. davis3_net &lt;- read.paj(&quot;data/davis.net&quot;) davis3_net Network attributes: vertices = 32 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = 18 title = davis total edges= 89 missing edges= 0 non-missing edges= 89 Vertex attribute names: vertex.names x y z Edge attribute names: davis Is it bipartite? is.bipartite(davis3_net) [1] TRUE Notice again that statnet has imported the coordinates from the Pajek layout. Additionally, the file was imported as bipartite, but not as directed. To solve this, use the set.network.attribute() function to overwrite the directed attribute from FALSE to TRUE. davis3_net &lt;- set.network.attribute(davis3_net, attrname = &quot;directed&quot;, value = TRUE) davis3_net Network attributes: vertices = 32 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = 18 title = davis total edges= 89 missing edges= 0 non-missing edges= 89 Vertex attribute names: vertex.names x y z Edge attribute names: davis Finally, remember you can list attributes and actor (vertex) names. list.vertex.attributes(davis3_net) [1] &quot;na&quot; &quot;vertex.names&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; network.vertex.names(davis3_net) [1] &quot;EVELYN&quot; &quot;LAURA&quot; &quot;THERESA&quot; &quot;BRENDA&quot; &quot;CHARLOTTE&quot; &quot;FRANCES&quot; [7] &quot;ELEANOR&quot; &quot;PEARL&quot; &quot;RUTH&quot; &quot;VERNE&quot; &quot;MYRNA&quot; &quot;KATHERINE&quot; [13] &quot;SYLVIA&quot; &quot;NORA&quot; &quot;HELEN&quot; &quot;DOROTHY&quot; &quot;OLIVIA&quot; &quot;FLORA&quot; [19] &quot;E1&quot; &quot;E2&quot; &quot;E3&quot; &quot;E4&quot; &quot;E5&quot; &quot;E6&quot; [25] &quot;E7&quot; &quot;E8&quot; &quot;E9&quot; &quot;E10&quot; &quot;E11&quot; &quot;E12&quot; [31] &quot;E13&quot; &quot;E14&quot; 4.4.2 Plotting Two-Mode Networks At this point, we have already plotted one bipartite network. Here we will compare a few plots using only the davis1_net network. Note that we need to tell gplot() that it is a two-mode network (with the argument gmode = \"twomode\"). Like before, we will compare layout algorithms side-by-side on the same row. # Set graph parameters to 1 row and 3 columns par(mfrow = c(1, 3)) # Store coordinates coords_fr &lt;- gplot.layout.fruchtermanreingold(davis1_net, layout.par = NULL) coords_kk &lt;- gplot.layout.kamadakawai(davis1_net, layout.par = NULL) coords_cr &lt;- gplot.layout.circle(davis1_net, layout.par = NULL) # Plot graphs gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_fr, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, usearrows = FALSE) gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_kk, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, usearrows = FALSE) gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_cr, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, usearrows = FALSE) The default colors for statnet are blue and red, so if we want to assign different colors we can do so by creating a separate color vector. # First, create a vector of length 18 with the value &quot;light blue&quot; women &lt;- rep(&quot;light blue&quot;, times = 18) # Next, create a vector of length 14 with the value &quot;yellow&quot; events &lt;- rep(&quot;yellow&quot;, times = 14) # Now, combine both into a single vector color &lt;- c(women, events) Now, replot the same networks as above. The only difference in the following commands from those above is that they use the stored coordinates and the color vector we just created. par(mfrow = c(1, 3)) gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_fr, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = color, usearrows = FALSE) gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_kk, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = color, usearrows = FALSE) gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_cr, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = color, usearrows = FALSE) Lets calculate two-mode degree centrality and then assign the scores as actor attributes. First, lets take a look at how to calculate node degree. degree(davis1_net) [1] 16 14 16 14 8 8 8 6 8 8 8 12 14 16 10 4 4 4 6 6 12 8 16 16 20 [26] 28 24 10 8 12 6 6 Note that you can assign that vector of scores as a vertex attributes. davis1_net &lt;- set.vertex.attribute(davis1_net, attrname = &quot;degree&quot;, value = degree(davis1_net)) davis1_net Network attributes: vertices = 32 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = 18 total edges= 89 missing edges= 0 non-missing edges= 89 Vertex attribute names: degree vertex.names Edge attribute names: NULL You can always call this attribute back. get.vertex.attribute(davis1_net, &quot;degree&quot;) [1] 16 14 16 14 8 8 8 6 8 8 8 12 14 16 10 4 4 4 6 6 12 8 16 16 20 [26] 28 24 10 8 12 6 6 Plot graph with node size reflecting two-mode degree centrality. The degree scores are rescaled so that the vertices dont overwhelm the graph. gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_fr, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = color, vertex.cex = get.vertex.attribute(davis1_net, attrname = &quot;degree&quot;)/10, usearrows = FALSE) 4.4.3 Projecting (Folding) Two-Mode Networks into One-Mode Networks in statnet. For this section, we will just use the davis1_net network ojbect. 4.4.3.1 Multiplying Matrices We can transform the network into a one-mode network of the women by multiplying the matrix (not the graph) by its transpose in order to get a one-mode of the women-to-women. First, lets take a look at how to extract an adjacency matrix from the graph. as.matrix.network.adjacency(davis1_net) E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 E13 E14 EVELYN 1 1 1 1 1 1 0 1 1 0 0 0 0 0 LAURA 1 1 1 0 1 1 1 1 0 0 0 0 0 0 THERESA 0 1 1 1 1 1 1 1 1 0 0 0 0 0 BRENDA 1 0 1 1 1 1 1 1 0 0 0 0 0 0 CHARLOTTE 0 0 1 1 1 0 1 0 0 0 0 0 0 0 FRANCES 0 0 1 0 1 1 0 1 0 0 0 0 0 0 ELEANOR 0 0 0 0 1 1 1 1 0 0 0 0 0 0 PEARL 0 0 0 0 0 1 0 1 1 0 0 0 0 0 RUTH 0 0 0 0 1 0 1 1 1 0 0 0 0 0 VERNE 0 0 0 0 0 0 1 1 1 0 0 1 0 0 MYRNA 0 0 0 0 0 0 0 1 1 1 0 1 0 0 KATHERINE 0 0 0 0 0 0 0 1 1 1 0 1 1 1 SYLVIA 0 0 0 0 0 0 1 1 1 1 0 1 1 1 NORA 0 0 0 0 0 1 1 0 1 1 1 1 1 1 HELEN 0 0 0 0 0 0 1 1 0 1 1 1 0 0 DOROTHY 0 0 0 0 0 0 0 1 1 0 0 0 0 0 OLIVIA 0 0 0 0 0 0 0 0 1 0 1 0 0 0 FLORA 0 0 0 0 0 0 0 0 1 0 1 0 0 0 Now, lets generate a one-mode matrix of women-to-women relations based on shared participation in an event. To do so, we will multiply the adjacency matrix using the %*% operator times its transpose (t()). davis_women_mat &lt;- as.matrix.network.adjacency(davis1_net) %*% t(as.matrix.network.adjacency(davis1_net)) View the matrix: davis_women_mat Repeat the process, this time switch the order of the transposed matrix to generate an events-to-events matrix. davis_events_mat &lt;- t(as.matrix.network.adjacency(davis1_net)) %*% as.matrix.network.adjacency(davis1_net) Finally, convert the new matrices into network objects. davis_women_net &lt;- as.network(davis_women_mat) davis_events_net &lt;- as.network(davis_events_mat) 4.4.4 Plotting Projected One-Mode Networks Now that we have extracted the one-mode networks, plot the two new graphs using gplot() and the additional arguments used previously. par(mfrow = c(1, 2)) # Save coordinates coords_women_kk &lt;- gplot.layout.kamadakawai(davis_women_net, layout.par = NULL) coords_events_kk &lt;- gplot.layout.kamadakawai(davis_events_net, layout.par = NULL) # Plot graphs gplot(dat = davis_women_net, gmode = &quot;onemode&quot;, coord = coords_women_kk, label = network.vertex.names(davis_women_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = &quot;light blue&quot;, usearrows = FALSE) gplot(dat = davis_events_net, gmode = &quot;onemode&quot;, coord = coords_events_kk, label = network.vertex.names(davis_events_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = &quot;yellow&quot;, usearrows = FALSE) Resize the nodes by degree centrality. This time, we will not store the value as a vertex attribute. par(mfrow = c(1, 2)) # Plot graphs gplot(dat = davis_women_net, gmode = &quot;onemode&quot;, coord = coords_women_kk, label = network.vertex.names(davis_women_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = &quot;light blue&quot;, vertex.cex = degree(davis_women_net, gmode = &quot;graph&quot;)/10, usearrows = FALSE) gplot(dat = davis_events_net, gmode = &quot;onemode&quot;, coord = coords_events_kk, label = network.vertex.names(davis_events_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = &quot;yellow&quot;, vertex.cex = degree(davis_events_net, gmode = &quot;graph&quot;)/10, usearrows = FALSE) 4.4.5 Saving Network Plots Now, save plots of the two-mode network and the two one-mode networks. png(file = &quot;davis1.png&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) gplot(dat = davis1_net, gmode = &quot;twomode&quot;, coord = coords_fr, label = network.vertex.names(davis1_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = color, vertex.cex = get.vertex.attribute(davis1_net, attrname = &quot;degree&quot;)/10, usearrows = FALSE) dev.off() png(file = &quot;daviswomen.png&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) gplot(dat = davis_women_net, gmode = &quot;onemode&quot;, coord = coords_women_kk, label = network.vertex.names(davis_women_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = &quot;light blue&quot;, vertex.cex = degree(davis_women_net, gmode = &quot;graph&quot;)/10, usearrows = FALSE) dev.off() png(file = &quot;davisevents.png&quot;, width = 4, height = 4, units = &#39;in&#39;, res = 300, bg = &quot;transparent&quot;) gplot(dat = davis_events_net, gmode = &quot;onemode&quot;, coord = coords_events_kk, label = network.vertex.names(davis_events_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = &quot;yellow&quot;, vertex.cex = degree(davis_events_net, gmode = &quot;graph&quot;)/10, usearrows = FALSE) dev.off() 4.4.6 Saving Network Data Once again, it doesnt hurt to save the data that youve imported and created. save(davis_el, davis1_net, davis2_net, davis3_net, davis_events_net, davis_women_net, file = &quot;data/davis_statnet.RData&quot;) Thats all for statnet for now. References "],["manipulating-and-simplifying-social-network-data-in-igraph.html", "5 Manipulating and Simplifying Social Network Data in igraph 5.1 Setup 5.2 Load Libraries 5.3 Extracting the Backbones of Two-Mode Network Projections 5.4 Simplifying Networks in igraph: Anabaptist Leadership Network 5.5 Multiple (Stacked) Networks in igraph: Sampson Monastery", " 5 Manipulating and Simplifying Social Network Data in igraph In this lab well explore a variety of methods for manipulating and simplifying social network data. As with the previous lab, well use a variety of social networks, some of which youll recognize from other classes. 5.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ####################################################################### # What: Manipulating and Simplifying Social Network Data # File: lab2_igraph.R # Created: 02.28.14 # Revised: 12.23.21 ####################################################################### If you have not set up your RStudio Project to clear the workspace on exit, your environment contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (Anabaptists.csv, Anabaptists.net, High esteem.csv, Liking3.csv, Positive Influence.csv, Praise.csv, S114 attributes.csv, and S114.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. 5.2 Load Libraries We need to load the libraries we plan to use. Here we will use igraph. Because igraph and sna conflict with one another sometimes, we do not want to have them loaded at the same time, so youll want to detach it. Alternatively, you may choose to namespace functions using the :: operator as needed (e.g., igraph::betweenness() vs. sna::betweenness()). Of course, this applies only if you had the statnet suite loaded already. library(igraph) Note: igraph imports the %&gt;% operator on load (library(igraph)). This series of exercises leverages the operator because we find it very useful in chaining functions. In addition to igraph, we will be introducing and using backbone. Since this may be the first time you are using this tool, please ensure you install it prior to loading it. # If you haven&#39;t done so, install backbone # install.packages(&quot;backbone&quot;) library(backbone) 5.3 Extracting the Backbones of Two-Mode Network Projections In this section, we illustrate how to extract the backbone of a projected two-mode network, a concept that we discussed in class and will not discuss in too much detail here. The basic idea is to use statistical tests to compare an edges observed weight in the two-mode projection to the distribution of its expected weight under a null model, controlling for different factors. An edges observed weight is considered to be statistically significant (and thus an actual tie) if it is in the upper or lower tail of the distribution of possible edge weights. Positive edges (ties) are found in the upper tail, while negative edges are found in the lower tail. For this, we will use the backbone package (Domagalski, Neal, and Sagan 2021). Because we need a much larger network to stochastically (statistically) project the backbone, here we will use bill co-sponsorship in the 114th US Senate (S114.csv) to illustrate this process. The code found in this section is adapted from code written by Zachary Neal, Rachel Domagalski, and Bruce Sagan for the Backbone Workshop delivered on 13 July 2020, which can be viewed at https://youtu.be/qLrUMZp93D0. For complete workshop materials and more details about backbone, visit http://www.zacharyneal.com/backbone or contact Zachary Neal at zpneal@msu.edu. Specify the seeds before starting the analysis. set.seed(19) 5.3.1 Load Data Load and examine the network data. We will begin working with matrix data. As you can see, there are 100 Senators who sponsored a total of 3,549 bills. senate_mat &lt;- as.matrix( read.csv(&quot;data/S114.csv&quot;, row.names = 1, header = TRUE, check.names = FALSE) ) # Retrieve the dimension of a matrix. dim(senate_mat) [1] 100 3589 # Print out the first five rows and columns. senate_mat[1:5, 1:5] sj9 sj8 sj7 sj6 sj5 Alexander, L. (TN-R) 0 1 0 1 0 Boxer, B. (CA-D) 0 0 0 0 1 Cantwell, M. (WA-D) 0 0 0 0 1 Carper, T. (DE-D) 0 0 0 0 1 Cochran, T. (MS-R) 0 1 0 1 0 Now, load the attribute (party) data. S114attributes_mat &lt;- as.matrix( read.csv(&quot;data/S114 attributes.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) ) Take a quick look. # Get first five rows. S114attributes_mat[1:5, ] Alexander, L. (TN-R) Boxer, B. (CA-D) Cantwell, M. (WA-D) 1 2 2 Carper, T. (DE-D) Cochran, T. (MS-R) 2 1 5.3.2 Non-Statistical Projections of the Co-Sponsorhip Data 5.3.2.1 Standard Projection Project the one-mode network the by multiplying the matrix times its transpose. senators_mat &lt;- senate_mat %*% t(senate_mat) As the result from the dim() function indicates, the one-mode network is a 100 x 100 network (100 senators x 100 senators). dim(senators_mat) [1] 100 100 Turn the matrix into an igraph object and then simplify it. Simplifying the network removes loops and multiple lines (summing them together to create weighted edges). This is almost always necessary before computing some metrics, such as centralization. senators_ig &lt;- graph.adjacency(senators_mat, mode = &quot;undirected&quot;, weighted = TRUE) senators_ig &lt;- simplify(senators_ig, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = sum) Heres how to pipe the above commands: senators_ig &lt;- graph.adjacency(senators_mat, mode = &quot;undirected&quot;, weighted = TRUE) %&gt;% simplify(remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = sum) Now, plot the network, but first set the colors to blue (Democrats), red (Republicans), and white (Independents). Bernie Sanders is the white node off to the left. # Create recoding named vector recode &lt;- c(`1` = &quot;red&quot;, `2` = &quot;blue&quot;, `3` = &quot;white&quot;) # Recode and assign strings to vertex attribute V(senators_ig)$color &lt;- recode[as.character(S114attributes_mat[, 1])] # Plot plot(senators_ig, layout = layout_with_fr, vertex.color = V(senators_ig)$color, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = 12, edge.width = 0.1) How densely interconnected are actors? edge_density(senators_ig) [1] 0.999798 As you can see the senators are clearly separated into two groups, but the networks density is almost 100% (0.999798), indicating that essentially every senator has a tie to every other senator, which probably isnt realistic. 5.3.2.2 Universal Threshold Projection A common approach to reducing the number of ties in a projected one-mode network is to choose a threshold where cells greater than the threshold are set to 1 while all others are set to 0. Here, we will use mean and median edge weights as thresholds. First, we need to get the mean and median edge weight. mean(E(senators_ig)$weight) [1] 44.17054 median(E(senators_ig)$weight) [1] 38 Create a projection using mean edge weight as a threshold value. To do so, use the global() function from the backbone library, which will handle the projection of a weighted network. threshold_mean_bb &lt;- global(senate_mat, # Set upper threshold value upper = mean(E(senators_ig)$weight)) # Take a look at the matrix: threshold_mean_bb[1:5, 1:5] Alexander, L. (TN-R) Boxer, B. (CA-D) Cantwell, M. (WA-D) Alexander, L. (TN-R) 0 0 0 Boxer, B. (CA-D) 0 0 1 Cantwell, M. (WA-D) 0 1 0 Carper, T. (DE-D) 0 1 0 Cochran, T. (MS-R) 0 0 0 Carper, T. (DE-D) Cochran, T. (MS-R) Alexander, L. (TN-R) 0 0 Boxer, B. (CA-D) 1 0 Cantwell, M. (WA-D) 0 0 Carper, T. (DE-D) 0 0 Cochran, T. (MS-R) 0 0 Now create an igraph object from this matrix. threshold_mean_ig &lt;- graph.adjacency(threshold_mean_bb, mode = &quot;undirected&quot;, weighted = TRUE) Create a projection using median edge weight. threshold_med_bb &lt;- global(senate_mat, # Set upper threshold value upper = median(E(senators_ig)$weight)) # Create an igraph graph with this object threshold_med_ig &lt;- graph.adjacency(threshold_med_bb, mode = &quot;undirected&quot;, weighted = TRUE) Now, visualize the two projected networks. # Set graph parameters to 1 row and 2 columns par(mfrow = c(1, 2)) # Recode and assign strings to vertex attribute V(threshold_mean_ig)$color &lt;- recode[as.character(S114attributes_mat[, 1])] V(threshold_med_ig)$color &lt;- recode[as.character(S114attributes_mat[, 1])] # Save coordinates coords &lt;- layout_with_fr(threshold_mean_ig) plot(threshold_mean_ig, layout = coords, vertex.label.cex = 0.2, vertex.label.color = &quot;black&quot;, vertex.size = 12) plot(threshold_med_ig, layout = coords, vertex.label.cex = 0.2, vertex.label.color = &quot;black&quot;, vertex.size = 12) Once again, take a look at each graphs edge density. edge_density(threshold_mean_ig) [1] 0.4226263 edge_density(threshold_med_ig) [1] 0.4987879 These are definitely less dense (0.423 and 0.499) than the standard projection and Democrats and Republicans are clearly sorted into separate clusters. Still, these projections arent statistically derived although at least the thresholds arent entirely arbitrary. The next section demonstrates three statistical methods for extracting backbones. 5.3.3 Extracting Backbones As noted above, the backbone package uses statistical tests to compare an edges observed weight in the bipartite (two-mode) projection to the distribution of its weights expected under a null model. And an edges observed weight is statistically significant if it is in the upper or lower tail of the distribution. We will only focus on positive ties, so well only extract backbones based on the upper tail of the distribution. With the co-sponsorship data, edge weights depend on how many bills each senator sponsors (row sums), and how many sponsors each bill has (column sums). (Note: If the rows were terrorists and the columns represented groups with which they were affiliated, then the edge weights would depend on how many groups to which the terrorists belonged and how many terrorists each group has.) 5.3.3.1 Hypergeometric Backbone The Hypergeometric Model (formerly hyperg(), currently fixedrow()) controls exactly for row sums and is the fastest of the three backbone functions. First, we need to compute the probabilities and extract the backbone, saving it as an igraph object. Youll notice that there are numerous arguments for the fixedrow() function: The signed option returns a signed network if set to TRUE (default is FALSE); The alpha option sets the significance test, the fwer indicates whether to apply at familywise error rate correction (default is none); The class indicates what type of object to return (it can return both igraph and network\\statnet objects). # Compute the probabilities and create a backbone object hyperg_probs &lt;- fixedrow(senate_mat) This matrix object is being treated as an unweighted bipartite network of 100 agents and 3589 artifacts. # Extract a backbone network from a backbone object hyperg_ig &lt;- backbone.extract(hyperg_probs, signed = FALSE, alpha = .01, fwer = &quot;none&quot;, class = &quot;igraph&quot;) Now, plot the projected network. Once again the network is clustered into two distinct groups, and the density (0.704) is much lower than the standard projection. V(hyperg_ig)$color &lt;- recode[as.character(S114attributes_mat[, 1])] plot(hyperg_ig, layout = layout_with_fr, vertex.label.cex = 0.4, vertex.label.color = &quot;black&quot;, vertex.size = 12) edge_density(hyperg_ig) [1] 0.7038384 5.3.3.2 Stochastic Degree Sequence Model (SDSM) Backbone The Stochastic Degree Sequence Model (sdsm()) approximately controls for both row and column sums and is slower than the Hypergeometric Model (but probably more accurate). # Compute the probabilities sdsm_probs &lt;- sdsm(senate_mat, # If TRUE the suggested text and citation will be # displayed, which you can use for your write up. narrative = TRUE) This matrix object is being treated as an unweighted bipartite network of 100 agents and 3589 artifacts. # Extract a backbone network from a backbone object sdsm_ig &lt;- backbone.extract(sdsm_probs, signed = FALSE, alpha = .01, fwer = &quot;none&quot;, class = &quot;igraph&quot;) Once again, plot the network and take a look at the edge density. V(sdsm_ig)$color &lt;- recode[as.character(S114attributes_mat[, 1])] plot(sdsm_ig, layout = layout_with_fr, vertex.label.cex = 0.4, vertex.label.color = &quot;black&quot;, vertex.size = 12) edge_density(sdsm_ig) [1] 0.3084848 This network plot clearly differs from what weve seen so far. The density is much lower (0.3085) and now the two parties are more distinct. Moreover, we can see that there are a handful of senators that appear to lie in between the two clusters. 5.3.3.3 Fixed Degree Sequence Model (FDSM) Backbone The Fixed Degree Sequence Model (fdsm()) exactly controls for both row and column sums and is the slowest of the three models (but probably the most accurate). We extract the FDSM backbone like we did the previous two. # Compute the probabilities fdsm_probs &lt;- fdsm(senate_mat, narrative = TRUE) # Extract a backbone network from a backbone object fdsm_ig &lt;- backbone.extract(fdsm_probs, signed = FALSE, alpha = .01, fwer = &quot;none&quot;, class = &quot;igraph&quot;) Now, lets plot this backbone and calculate the edge density. V(fdsm_ig)$color &lt;- recode[as.character(S114attributes_mat[, 1])] plot(fdsm_ig, layout = layout_with_fr, vertex.label.cex = 0.4, vertex.label.color = &quot;black&quot;, vertex.size = 12) edge_density(fdsm_ig) [1] 0.3612121 This network is a bit denser (0.3606) than the previous network, but it looks quite similar to it. That said, in this network map more senators appear as brokers between the Republicans and Democrats. 5.4 Simplifying Networks in igraph: Anabaptist Leadership Network For this section, we will use the Anabaptist Leadership network and its related attribute data in order to see how to shrink (collapse) a network in igraph. The data set includes 67 actors, 55 who were sixteenth century Anabaptist leaders and 12 who were prominent Protestant Reformation leaders (e.g., Martin Luther, John Calvin, Ulrich Zwingli, Martin Bucer, and Philip Melanchthon) who had contact with and influenced some of the Anabaptist leaders included in this dataset. These data build on a smaller data set (Matthews et al. 2013) that did not include some leading Anabaptist leaders, such as Menno Simons, who is generally seen as the founder of the Amish and Mennonites. 5.4.1 Importing Network and Attribute Data Import the leadership network from a Pajek file and indicate that it is undirected. anabaptist_ig &lt;- read.graph(&quot;data/Anabaptist Leaders.net&quot;, format = &quot;pajek&quot;) %&gt;% # Coerce the network to undirected as.undirected() %&gt;% # Simplify the graph simplify(remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = sum) # If you are not comfortable using pipes (%&gt;%) you can use the following code: # anabaptist_ig &lt;- read.graph(&quot;data/Anabaptist Leaders.net&quot;, # format = &quot;pajek&quot;) # anabaptist_ig &lt;- as.undirected(anabaptist_ig) # anabaptist_ig &lt;- simplify(anabaptist_ig, # remove.multiple = TRUE, # remove.loops = TRUE, # edge.attr.comb = sum) # Now inspect the object anabaptist_ig IGRAPH 168d34c UNW- 67 183 -- + attr: id (v/c), name (v/c), x (v/n), y (v/n), z (v/n), weight (e/n) + edges from 168d34c (vertex names): [1] Martin Luther --Ulrich Zwingli Martin Luther --Thomas Muntzer [3] Martin Luther --Andreas Carlstadt Martin Luther --Caspar Schwenckfeld [5] Martin Luther --Melchior Hofmann Martin Luther --Philipp Melanchthon [7] Martin Luther --Martin Bucer John Calvin --Wolfgang Capito [9] John Calvin --Martin Bucer Ulrich Zwingli--Joachim Vadian [11] Ulrich Zwingli--Conrad Grebel Ulrich Zwingli--Felix Manz [13] Ulrich Zwingli--George Blaurock Ulrich Zwingli--Wilhelm Reublin [15] Ulrich Zwingli--Johannes Brotli Ulrich Zwingli--Louis Haetzer + ... omitted several edges Lets check to see if the ids (id) and names (name) are the same. Here we will test this using the setdiff() function, which calculates the set difference between two vectors. An output stating character(0) indicates no difference between the values in the first vector and the second. setdiff(V(anabaptist_ig)$id, V(anabaptist_ig)$name) character(0) Now, lets bring in the Anabaptist Attributes.csv data, which includes the node attributes. The first six identify whether someone (1) embraced believers baptism, (2) supported violence, (3) participated in the Münster Rebellion, (4) held apocalyptic beliefs, (5) was an Anabaptist, and/or (6) was a follower of Melchior Hoffman (i.e., a Melchiorite). The last two combine other vectors in order to create a new set of attributes. The first creates vector that distinguishes between Anabaptists who didnt participate in the Münster Rebellion, Anabaptists who did, and non-Anabaptists (e.g, Martin Luther, John Calvin). The values of the resulting vector equal 0 for non-Anabaptists, 1 for Anabaptists who didnt participate in the rebellion, and 2 for those who did. The second creates a vector that distinguishes between Melchiorite Anabaptists, Non-Melchiorite Anabaptists, and non-Anabaptists. The values of the resulting vector equal 0 for non-Anabaptists, 1 for non-Melchiorite Anabaptists, and 2 for Melchiorite Anabaptists. attributes &lt;- read.csv(&quot;data/Anabaptist Attributes.csv&quot;, header = TRUE) Before moving forward to the next step, lets briefly inspect the imported attribute variables in attributes. Begining with looking at the column names. colnames(attributes) [1] &quot;ï..Names&quot; &quot;Believers.Baptism&quot; &quot;Violence&quot; [4] &quot;Munster.Rebellion&quot; &quot;Apocalyptic&quot; &quot;Anabaptist&quot; [7] &quot;Melchiorite&quot; &quot;Swiss.Brethren&quot; &quot;Denck&quot; [10] &quot;Hut&quot; &quot;Hutterite&quot; &quot;Other.Anabaptist&quot; [13] &quot;Lutheran&quot; &quot;Reformed&quot; &quot;Other.Protestant&quot; [16] &quot;Tradition&quot; &quot;Origin..&quot; &quot;Operate..&quot; Each column in the data frame represents a vector of values that correspond with the nodes in our graph. We can subset each or multiple columns using the [ operator. If you are unfamiliar with this form of subsetting, keep in mind that you may provide two sets of values separated by a comma to access either rows or columns. The former will subset rows (e.g., attributes[1:3, ]), the latter will return columns (e.g., attributes[, 12]). However, since we have named columns, we can also supply the name of the column in order to subset (e.g., attributes[, \"Violence\"]). For instance, we can select the first five rows of the Anabaptist column. attributes[1:5, &quot;Anabaptist&quot;] [1] 0 0 0 0 1 What this tells us is that of the first five actors listed in the data, only Conrad Grebel was an Anabaptist. We can also get the same information this way. attributes[1:5, # Rows 1 through 5 c(1, 6)] # Columns 1 and 6 ï..Names Anabaptist 1 Martin Luther 0 2 John Calvin 0 3 Ulrich Zwingli 0 4 Joachim Vadian 0 5 Conrad Grebel 1 The following commands create and add a series of vectors to the attributes data.frame that we can later use with our network. attributes[[&quot;anabmunst&quot;]] &lt;- attributes[, &quot;Munster.Rebellion&quot;] + attributes[, &quot;Anabaptist&quot;] attributes[[&quot;anabmelch&quot;]] &lt;- attributes[, &quot;Melchiorite&quot;] + attributes[, &quot;Anabaptist&quot;] 5.4.2 Plotting the Anabaptist Network Plot the network where color indicates whether the actor is an Anabaptist who didnt participate in the Munster Rebellion, an Anabaptist who did, or a non-Anabaptist (anabmunst variable). # Save coordinates layout_fr &lt;- layout_with_fr(anabaptist_ig) # Plot graph plot(anabaptist_ig, layout = layout_fr, vertex.label.cex = 0.4, vertex.label.color = &quot;black&quot;, vertex.size = 12, vertex.color = attributes[[&quot;anabmunst&quot;]]) Blue nodes are those who participated in Munster Rebellion, orange nodes are Anabaptists who didnt, and white nodes are non-Anabaptists. Now, lets give the network a bit more color. # Declare which values correspond to which color recode &lt;- c(`0` = &quot;yellow&quot;, `1` = &quot;lightblue&quot;, `2` = &quot;red&quot;) # Add a new variable to our attributes data.frame for color attributes[[&quot;color&quot;]] &lt;- recode[as.character(attributes[[&quot;anabmunst&quot;]])] # Plot plot(anabaptist_ig, layout = layout_fr, vertex.label.cex = 0.4, vertex.label.color = &quot;black&quot;, vertex.size = 12, vertex.color = attributes[[&quot;color&quot;]]) 5.4.3 Shrinking (Collapsing, Contracting) Networks Shrink (collapse) the Anabaptist Leadership network by Melchiorite, Non-Melchiorite, and Non-Anabaptist (Other). First, we need to change the numbering scheme of anabmelch.vec from 0, 1, 2 to 1, 2, 3 (igraph doesnt like ids with 0s). # First, take a look at the values you will be recoding table(attributes[, &quot;anabmelch&quot;]) 0 1 2 12 39 16 A 0 represents Other, 1 Non-Melchiorite, and 2 Melchiorite. With these categories, lets contract all vertices from a given category into a single vertex. To do so, we will use the contract() function, which creates a new graph by merging nodes. Note that we will need to provide a numeric vector that specifies the group of each vertex. Note that the vector must be numeric and igraph does not like zeros. As such, we can add a 1 to the values, changing the numbering scheme of anabmelch from 0, 1, 2 to 1, 2, 3. # Contract anagroup_ig &lt;- contract(anabaptist_ig, attributes[[&quot;anabmelch&quot;]] + 1) # Look at output anagroup_ig IGRAPH 16fc71a UNW- 3 183 -- + attr: name (v/x), weight (e/n) + edges from 16fc71a (vertex names): [1] Martin Luther , John Calvin , Ulrich Zwingli , Joachim Vadian , Leo Jud , Henry Bullinger , Thomas Muntzer , Andreas Carlstadt , Caspar Schwenckfeld, Philipp Melanchthon, Wolfgang Capito , Martin Bucer --Martin Luther , John Calvin , Ulrich Zwingli , Joachim Vadian , Leo Jud , Henry Bullinger , Thomas Muntzer , Andreas Carlstadt , Caspar Schwenckfeld, Philipp Melanchthon, Wolfgang Capito , Martin Bucer + ... omitted several edges After shrinking, we can relabel the categories from 1, 2 and 3 to the string names. anagroup_ig &lt;- set.vertex.attribute(anagroup_ig, name = &quot;label&quot;, value = c(&quot;Other&quot;, &quot;Non-Melchiorite&quot;, &quot;Melchiorite&quot;)) Now plot resulting graph. Note that here we first estimate and then store the layout coordinates as an attribute of the graph, rather than as separate coordinates. par(mfrow = c(1, 2)) # Set coordinates anabaptist_ig &lt;- set.graph.attribute(anabaptist_ig, name = &quot;layout&quot;, value = layout_with_fr(anabaptist_ig)) anagroup_ig &lt;- set.graph.attribute(anagroup_ig, name = &quot;layout&quot;, value = layout_with_fr(anagroup_ig)) # Now plot plot(anabaptist_ig, vertex.color = attributes[[&quot;anabmelch&quot;]], vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) plot(anagroup_ig, vertex.color = &quot;lightblue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.mode = 0) Note that there are multiple edges; lets get rid of those using simplify() and then replot. par(mfrow = c(1, 2)) anagroup_ig &lt;- simplify(anagroup_ig) plot(anabaptist_ig, vertex.color = attributes[[&quot;anabmelch&quot;]], vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) plot(anagroup_ig, vertex.color=&quot;Sky Blue&quot;, vertex.label.cex=.6, vertex.label.color=&quot;black&quot;, edge.arrow.mode=0) The graph isnt too exciting although it does highlight how the Melchiorites (at least at the leadership level) were only connected to other Anabaptists through non-Anabaptists like Martin Luther. Now, lets collapse it by religious tradition and then assign labels to the collapsed groups. anatrad_ig &lt;- contract(anabaptist_ig, attributes[[&quot;Tradition&quot;]]) anatrad_ig &lt;- set.vertex.attribute(anatrad_ig, &quot;label&quot;, value = c(&quot;Melchiorite&quot;, &quot;Swiss Brethren&quot;, &quot;Denck&quot;, &quot;Hut&quot;, &quot;Hutterite&quot;, &quot;Other Anabaptist&quot;, &quot;Lutheran&quot;, &quot;Reformed&quot;, &quot;Other Protestant&quot;)) Simplify the network, this time weighting edges by sum of the multiple lines. anatrad_ig &lt;- simplify(anatrad_ig, edge.attr.comb = sum) Now plot the Anabaptist graph colored by Tradition side-by-side with the collapsed graph. par(mfrow = c(1, 2)) anatrad_ig &lt;- set.graph.attribute(anatrad_ig, name = &quot;layout&quot;, layout.kamada.kawai(anatrad_ig)) plot(anabaptist_ig, vertex.color = attributes[[&quot;Tradition&quot;]], vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) plot(anatrad_ig, vertex.color = &quot;skyblue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.mode = 0, edge.width = E(anatrad_ig)$weight/5) 5.4.4 Extracting Subnetworks To extract subnetworks in igraph, we use the induced_subgraph() command. Here, we extract just the Anabaptists and plot the resulting network. First, though, we need to assign the Anabaptist attribute to the network. Using the table() command, we can see that there are 55 Anabaptists, which means that the extracted subnetwork should have 55 actors. That is what we end up with. table(attributes[[&quot;Anabaptist&quot;]]) 0 1 12 55 Now assign this vector to a vertex attribute using the V() function. V(anabaptist_ig)$anabaptist &lt;- attributes[[&quot;Anabaptist&quot;]] Now extract the subgraph by indicating which vertices should be included. To do so, use the induced_subgraph() function, specifying the vertices with a value of 1 for the attribute anabaptist. anabaptist2_ig &lt;- induced_subgraph(anabaptist_ig, vids = which(V(anabaptist_ig)$anabaptist == 1)) anabaptist2_ig IGRAPH 17aef93 UNW- 55 131 -- + attr: layout (g/n), id (v/c), name (v/c), x (v/n), y (v/n), z (v/n), | anabaptist (v/n), weight (e/n) + edges from 17aef93 (vertex names): [1] Conrad Grebel--Felix Manz [2] Conrad Grebel--George Blaurock [3] Conrad Grebel--Wilhelm Reublin [4] Conrad Grebel--Johannes Brotli [5] Conrad Grebel--Louis Haetzer [6] Conrad Grebel--Wolfgang Ulimann [7] Conrad Grebel--Andrew Castelberger + ... omitted several edges Now, lets plot the subnetwork. Note that it is disconnected, something which we saw above when we collapsed the network by Melchiorite, Non-Melchiorite, and Non-Anabaptist (Other). In other words, it is held together by non-Anabaptists. par(mfrow = c(1, 2)) plot(anabaptist_ig, vertex.color = attributes[[&quot;Anabaptist&quot;]], vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;) plot(anabaptist2_ig, layout = layout_with_kk, vertex.color = &quot;light blue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = 12) 5.5 Multiple (Stacked) Networks in igraph: Sampson Monastery The data we will use in this exercise are the Sampson Monastery network data collected by Samuel Sampson. Sampson observed and recorded the social interactions among a group of novices (men who were preparing to join a monastic order). He recorded four types of ties: esteem (SAMPES) and disesteem (SAMPDES); liking (SAMPLK - three different time periods recorded) and disliking (SAMPDLK - one-time period recorded); positive influence (SAMPIN) and negative influence (SAMPNIN); praise (SAMPPR) and blame (SAMPNPR). Each novice only ranked his top three choices for each type of tie where a 3 indicates their first choice, a 2 their second, and a 1 their third (some subjects offered tied ranks for their top four choices). During Sampsons period of observation, a crisis in the cloister occurred in response to some of the changes proposed by the Second Vatican Council (Vatican II). This led to the expulsion of four novices and the voluntary departure of several others. Based on his observations, Sampson partitioned (i.e., sorted, divided) the novices into four groups: (1) the young turks, (2) the loyal opposition, (3) the outcasts, and (4) the neutrals. The young turks arrived later and questioned some of the monasterys practices, which the loyal opposition defended. The outcasts were novices that were not accepted by the larger group, and the neutrals were those who did not take sides in the debate. Most of the loyal opposition had attended a seminary, Cloisterville, prior to their arrival at the monastery. For igraph, we will read in the data as edge lists and store them as data frames. Typically, edge lists do not contain zeros; they only include edges (ties) that actually exist (i.e., of tie strength 1 or higher). However, when stacking edge lists, we need to include edges of strength 0 so that all possible pairs of actors are included. Otherwise, its impossible to merge them together. 5.5.1 Importing Edge List Network Data Well bring in the four positive tie edge If you are interested in what the edge lists look like, inspect them after importing. liking3 &lt;- read.csv(&quot;data/Liking 3.csv&quot;, header = TRUE) esteem &lt;- read.csv(&quot;data/High esteem.csv&quot;, header = TRUE) influence &lt;- read.csv(&quot;data/Positive Influence.csv&quot;, header = TRUE) praise &lt;- read.csv(&quot;data/Praise.csv&quot;, header = TRUE) You can and should inspect all imported objects. Do they share column names? head(liking3) head(esteem) head(influence) head(praise) Since each data.frame shares the same column headers, we can go ahead and row bind them together using the rbind() function. samp_pos &lt;- rbind(liking3, esteem, influence, praise) # Look at the data str(samp_pos) &#39;data.frame&#39;: 202 obs. of 4 variables: $ ï..Source: chr &quot;ROMUALD&quot; &quot;ROMUALD&quot; &quot;ROMUALD&quot; &quot;ROMUALD&quot; ... $ Target : chr &quot;BONAVENTURE&quot; &quot;AMBROSE&quot; &quot;PETER&quot; &quot;AMAND&quot; ... $ Weight : int 1 1 3 2 1 3 2 1 3 2 ... $ Relation : chr &quot;Liking 3&quot; &quot;Liking 3&quot; &quot;Liking 3&quot; &quot;Liking 3&quot; ... Now, convert the data frame to an igraph object. samp_pos_ig &lt;- graph_from_data_frame(d = samp_pos, directed = TRUE) samp_pos_ig IGRAPH 17fc2f0 DN-- 18 202 -- + attr: name (v/c), Weight (e/n), Relation (e/c) + edges from 17fc2f0 (vertex names): [1] ROMUALD -&gt;BONAVENTURE ROMUALD -&gt;AMBROSE ROMUALD -&gt;PETER [4] ROMUALD -&gt;AMAND BONAVENTURE-&gt;AMBROSE BONAVENTURE-&gt;PETER [7] BONAVENTURE-&gt;LOUIS AMBROSE -&gt;BONAVENTURE AMBROSE -&gt;VICTOR [10] AMBROSE -&gt;WINFRID BERTHOLD -&gt;BONAVENTURE BERTHOLD -&gt;AMBROSE [13] BERTHOLD -&gt;PETER PETER -&gt;BONAVENTURE PETER -&gt;BERTHOLD [16] PETER -&gt;LOUIS LOUIS -&gt;BONAVENTURE LOUIS -&gt;VICTOR [19] LOUIS -&gt;HUGH VICTOR -&gt;AMBROSE VICTOR -&gt;BERTHOLD [22] VICTOR -&gt;PETER WINFRID -&gt;JOHN_BOSCO WINFRID -&gt;GREGORY + ... omitted several edges 5.5.2 Plotting Stacked Networks Lets plot the graph, saving the coordinates into an object so that we can use them later. samp_pos_layout &lt;- layout_with_fr(samp_pos_ig) plot(samp_pos_ig, layout = samp_pos_layout, vertex.color = &quot;lightblue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.size = 0.25) Lets set the color of the edges so that theyll be different for each type of tie. Well set the colors to red, blue, yellow, and green. # Create a named vector used for recoding recode &lt;- c(&quot;Liking 3&quot; = &quot;red&quot;, &quot;High Esteem&quot; = &quot;blue&quot;, &quot;Positive Influence&quot; = &quot;yellow&quot;, &quot;Praise&quot; = &quot;green&quot;) # We will create a color edge attribute by recoding based on the Relation # edge attribute that we imported into the graph from the edge list E(samp_pos_ig)$color &lt;- recode[E(samp_pos_ig)$Relation] plot(samp_pos_ig, layout = samp_pos_layout, vertex.color = &quot;lightblue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.size = 0.25) 5.5.3 Extracting and Plotting Individual Networks Heres how to extract the Liking 3 network from the larger stacked network and then plot it. samp_liking3_ig &lt;- subgraph.edges(samp_pos_ig, eids = which( E(samp_pos_ig)$Relation == &quot;Liking 3&quot;), delete.vertices = FALSE) Heres a network plot based solely on the ties of the Liking 3 network. plot(samp_liking3_ig, layout = samp_pos_layout, vertex.color = &quot;lightblue&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.size = 0.5, edge.color = &quot;darkgrey&quot;, edge.curved = 0.5) Thats all for igraph for now. References "],["manipulating-and-simplifying-social-network-data-in-statnet.html", "6 Manipulating and Simplifying Social Network Data in statnet 6.1 Setup 6.2 Load Libraries 6.3 Extracting the Backbones of Two-Mode Network Projections 6.4 Simplifying Networks in statnet: Anabaptist Network 6.5 Multiple (Stacked) Networks in statnet: Sampson Monastery", " 6 Manipulating and Simplifying Social Network Data in statnet In this lab well explore a variety of methods for manipulating and simplifying social network data. As with the previous lab, well use a variety of social networks, some of which youll recognize from other classes. 6.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ####################################################################### # What: Manipulating and Simplifying Social Network Data # File: lab2_statnet.R # Created: 02.28.14 # Revised: 12.23.21 ####################################################################### If you have not set up your RStudio Project to clear the workspace on exit, your environment contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (Anabaptists.csv, Anabaptists.net, High esteem.csv, Liking3.csv, Positive Influence.csv, Praise.csv, S114 attributes.csv, and S114.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. 6.2 Load Libraries We need to load the libraries we plan to use. Here we will use statnet. library(statnet) In addition to statnet, we will be introducing and using backbone. Since this may be the first time you are using this tool, please ensure you install it prior to loading it. # If you haven&#39;t done so, install backbone # install.packages(&quot;backbone&quot;) library(backbone) 6.3 Extracting the Backbones of Two-Mode Network Projections In this section, we illustrate how to extract the backbone of a projected two-mode network, a concept that we discussed in class and will not discuss in too much detail here. The basic idea is to use statistical tests to compare an edges observed weight in the two-mode projection to the distribution of its expected weight under a null model, controlling for different factors. An edges observed weight is considered to be statistically significant (and thus an actual tie) if it is in the upper or lower tail of the distribution of possible edge weights. Positive edges (ties) are found in the upper tail, while negative edges are found in the lower tail. For this, we will use the backbone package (Domagalski, Neal, and Sagan 2021). Because we need a much larger network to stochastically (statistically) project the backbone, here we will use bill co-sponsorship in the 114th US Senate (S114.csv) to illustrate this process. The code found in this section is adapted from code written by Zachary Neal, Rachel Domagalski, and Bruce Sagan for the Backbone Workshop delivered on 13 July 2020, which can be viewed at https://youtu.be/qLrUMZp93D0. For complete workshop materials and more details about backbone, visit http://www.zacharyneal.com/backbone or contact Zachary Neal at zpneal@msu.edu. Specify the seeds before starting the analysis. set.seed(19) 6.3.1 Load Data Load and examine the network data. We will begin working with matrix data. As you can see, there are 100 Senators who sponsored a total of 3,549 bills. senate_mat &lt;- as.matrix( read.csv(&quot;data/S114.csv&quot;, row.names = 1, header = TRUE, check.names = FALSE) ) # Retrieve the dimension of a matrix. dim(senate_mat) [1] 100 3589 # Print out the first five rows and columns. senate_mat[1:5, 1:5] sj9 sj8 sj7 sj6 sj5 Alexander, L. (TN-R) 0 1 0 1 0 Boxer, B. (CA-D) 0 0 0 0 1 Cantwell, M. (WA-D) 0 0 0 0 1 Carper, T. (DE-D) 0 0 0 0 1 Cochran, T. (MS-R) 0 1 0 1 0 S114attributes_mat &lt;- as.matrix( read.csv(&quot;data/S114 attributes.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) ) Take a quick look. # Get first five rows. S114attributes_mat[1:5, ] Alexander, L. (TN-R) Boxer, B. (CA-D) Cantwell, M. (WA-D) 1 2 2 Carper, T. (DE-D) Cochran, T. (MS-R) 2 1 6.3.2 Non-Statistical Projections of the Co-Sponsorhip Data 6.3.2.1 Standard Projection Project the one-mode network the by multiplying the matrix times its transpose. senators_mat &lt;- senate_mat %*% t(senate_mat) As the result from the dim() function indicates, the one-mode network is a 100 x 100 network (100 senators x 100 senators). dim(senators_mat) [1] 100 100 Turn the matrix into an network object senators_net &lt;- as.network(senators_mat, directed = FALSE, ignore.eval = FALSE) senators_net Network attributes: vertices = 100 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 4949 missing edges= 0 non-missing edges= 4949 Vertex attribute names: vertex.names Edge attribute names not shown Now, plot the network, but first set the colors to blue (Democrats), red (Republicans), and white (Independents). # Create recoding named vector recode &lt;- c(`1` = &quot;red&quot;, `2` = &quot;blue&quot;, `3` = &quot;white&quot;) # Recode and assign strings to vertex attribute senators_net %v% &quot;color&quot; &lt;- recode[as.character(S114attributes_mat[, 1])] # Plot gplot(dat = senators_net, gmode = &quot;onemode&quot;, mode = &quot;fruchtermanreingold&quot;, vertex.col = senators_net %v% &quot;color&quot;, vertex.cex = 2, usearrows = FALSE) How densely interconnected are actors? gden(senators_net) [1] 0.999798 The network plot is unhelpful because its so dense. Its almost 100% (0.999798), which suggests that were picking up ties between senators that really shouldnt be there. 6.3.2.2 Universal Threshold Projection A common approach to reducing the number of ties in a projected one-mode network is to choose a threshold where cells greater than the threshold are set to 1 while all others are set to 0. Here, we will use mean and median edge weights as thresholds. First, we need to get the mean and median edge weight. Because the edge attributes werent displayed above, we need to see what the edge weight attribute is with the list.edge.attributes() command. list.edge.attributes(senators_net) [1] &quot;1&quot; &quot;na&quot; Lets see if 1 is the edge weight. Here we pull the first twenty edge weights. get.edge.attribute(senators_net, &quot;1&quot;)[1:20] [1] 10 15 12 40 48 48 42 16 52 18 37 32 52 48 9 40 30 11 46 27 It appears that it is. Now, we can get the mean and median edge weight of the entire graph. mean(get.edge.attribute(senators_net, &quot;1&quot;)) [1] 44.17054 median(get.edge.attribute(senators_net, &quot;1&quot;)) [1] 38 Create a projection using mean edge weight. threshold_mean_bb &lt;- global(senate_mat, # Set upper threshold value upper = mean(get.edge.attribute(senators_net, &quot;1&quot;))) # Take a look at the matrix: threshold_mean_bb[1:5, 1:5] Alexander, L. (TN-R) Boxer, B. (CA-D) Cantwell, M. (WA-D) Alexander, L. (TN-R) 0 0 0 Boxer, B. (CA-D) 0 0 1 Cantwell, M. (WA-D) 0 1 0 Carper, T. (DE-D) 0 1 0 Cochran, T. (MS-R) 0 0 0 Carper, T. (DE-D) Cochran, T. (MS-R) Alexander, L. (TN-R) 0 0 Boxer, B. (CA-D) 1 0 Cantwell, M. (WA-D) 0 0 Carper, T. (DE-D) 0 0 Cochran, T. (MS-R) 0 0 Now create an network object from this matrix. threshold_mean_net &lt;- as.network(threshold_mean_bb, directed = FALSE, ignore.eval = FALSE) Create a projection using median edge weight. threshold_med_bb &lt;- global(senate_mat, # Set upper threshold value upper = median(get.edge.attribute(senators_net, &quot;1&quot;)) ) # Create an igraph graph with this object threshold_med_net &lt;- as.network(threshold_med_bb, directed = FALSE, ignore.eval = FALSE) Now, visualize the two projected networks. # Set graph parameters to 1 row and 2 columns par(mfrow = c(1, 2)) # Recode and assign strings to vertex attribute threshold_mean_net %v% &quot;color&quot; &lt;- recode[as.character(S114attributes_mat[, 1])] threshold_med_net %v% &quot;color&quot; &lt;- recode[as.character(S114attributes_mat[, 1])] # Save coordinates coords &lt;- gplot.layout.fruchtermanreingold(threshold_med_net, layout.par = NULL) # Plot them gplot(dat = threshold_mean_net, gmode = &quot;onemode&quot;, coord = coords, vertex.col = threshold_mean_net %v% &quot;color&quot;, vertex.cex = 2, usearrows = FALSE) gplot(dat = threshold_med_net, gmode = &quot;onemode&quot;, coord = coords, vertex.col = threshold_mean_net %v% &quot;color&quot;, vertex.cex = 2, usearrows = FALSE) Once again, take a look at each graphs edge density. gden(threshold_mean_net) [1] 0.4226263 gden(threshold_med_net) [1] 0.4987879 These are definitely less dense (0.422 and 0.499) than the standard projection and Democrats and Republicans are clearly sorted into separate clusters. Still, these projections arent statistically derived although at least the thresholds arent entirely arbitrary. The next section demonstrates three statistical methods for extracting backbones. 6.3.3 Extracting Backbones As noted above, the backbone package uses statistical tests to compare an edges observed weight in the bipartite (two-mode) projection to the distribution of its weights expected under a null model. And an edges observed weight is statistically significant if it is in the upper or lower tail of the distribution. We will only focus on positive ties, so well only extract backbones based on the upper tail of the distribution. With the co-sponsorship data, edge weights depend on how many bills each senator sponsors (row sums), and how many sponsors each bill has (column sums). (Note: If the rows were terrorists and the columns represented groups with which they were affiliated, then the edge weights would depend on how many groups to which the terrorists belonged and how many terrorists each group has.) 6.3.3.1 Hypergeometric Backbone The Hypergeometric Model (formerly hyperg(), currently fixedrow()) controls exactly for row sums and is the fastest of the three backbone functions. First, we need to compute the probabilities and extract the backbone, saving it as an network object. Youll notice that there are numerous arguments for the fixedrow() function: The signed option returns a signed network if set to TRUE (default is FALSE); The alpha option sets the significance test, the fwer indicates whether to apply at familywise error rate correction (default is \"none\"); The class indicates what type of object to return (it can return both igraph and network\\statnet objects). # Compute the probabilities and create a backbone object hyperg_probs &lt;- fixedrow(senate_mat) This matrix object is being treated as an unweighted bipartite network of 100 agents and 3589 artifacts. # Extract a backbone network from a backbone object hyperg_net &lt;- backbone.extract(hyperg_probs, signed = FALSE, alpha = .01, fwer = &quot;none&quot;, class = &quot;network&quot;) Now, plot the projected network. The network is clearly sorted into two distinct clusters, and the density (0.704) is much lower than the standard projection. hyperg_net %v% &quot;color&quot; &lt;- recode[as.character(S114attributes_mat[, 1])] gplot(dat = hyperg_net, gmode = &quot;onemode&quot;, mode = &quot;fruchtermanreingold&quot;, vertex.col = hyperg_net %v% &quot;color&quot;, vertex.cex = 2, displaylabels = TRUE, label.cex = 0.4, label.pos = 5, usearrows = FALSE) gden(hyperg_net) [1] 0.7038384 6.3.3.2 Stochastic Degree Sequence Model (SDSM) Backbone The Stochastic Degree Sequence Model (sdsm()) approximately controls for both row and column sums and is slower than the Hypergeometric Model (but probably more accurate). # Compute the probabilities sdsm_probs &lt;- sdsm(senate_mat, # If TRUE the suggested text and citation will be # displayed, which you can use for your write up. narrative = TRUE) This matrix object is being treated as an unweighted bipartite network of 100 agents and 3589 artifacts. # Extract a backbone network from a backbone object sdsm_net &lt;- backbone.extract(sdsm_probs, signed = FALSE, alpha = .01, fwer = &quot;none&quot;, class = &quot;network&quot;) Now, plot the network backbone. sdsm_net %v% &quot;color&quot; &lt;- recode[as.character(S114attributes_mat[, 1])] gplot(dat = sdsm_net, gmode = &quot;onemode&quot;, mode = &quot;fruchtermanreingold&quot;, vertex.col = sdsm_net %v% &quot;color&quot;, vertex.cex = 2, displaylabels = TRUE, label.cex = 0.4, label.pos = 5, usearrows = FALSE) gden(sdsm_net) [1] 0.3084848 This network plot clearly differs from what weve seen so far. The density is much lower (0.308) and now the two parties are more distinct. Moreover, there are a handful of senators who appear to lie in between the two clusters and are in positions of brokerage. 6.3.3.3 Fixed Degree Sequence Model (FDSM) Backbone The Fixed Degree Sequence Model (fdsm()) exactly controls for both row and column sums and is the slowest of the three models (but probably the most accurate). We extract the FDSM backbone like we did the previous two. # Compute the probabilities fdsm_probs &lt;- fdsm(senate_mat, narrative = TRUE) This matrix object is being treated as an unweighted bipartite network of 100 agents and 3589 artifacts. Constructing empirical edgewise p-values - # Extract a backbone network from a backbone object fdsm_net &lt;- backbone.extract(fdsm_probs, signed = FALSE, alpha = .01, fwer = &quot;none&quot;, class = &quot;network&quot;) Now, lets plot this backbone and calculate the edge density. fdsm_net %v% &quot;color&quot; &lt;- recode[as.character(S114attributes_mat[, 1])] gplot(dat = fdsm_net, gmode = &quot;onemode&quot;, mode = &quot;fruchtermanreingold&quot;, vertex.col = fdsm_net %v% &quot;color&quot;, vertex.cex = 2, displaylabels = TRUE, label.cex = 0.4, label.pos = 5, usearrows = FALSE) gden(fdsm_net) [1] 0.3606061 This network is a bit denser (0.362) than the previous network, but it looks quite similar to it. Once again, there are a handful of senators who appear to lie in between the two clusters and are in positions of brokerage. 6.4 Simplifying Networks in statnet: Anabaptist Network For this exercise, we will use the Anabaptist Leadership network and its related attribute data. The dataset includes 67 actors, 55 who were sixteenth century Anabaptist leaders and 12 who were prominent Protestant Reformation leaders (e.g., Martin Luther, John Calvin, Ulrich Zwingli, Martin Bucer, and Philip Melanchthon) that had contact with and influenced some of the Anabaptist leaders included in this dataset. These data build on a smaller dataset (Matthews et al. 2013) that did not include some leading Anabaptist leaders, such as Menno Simons, who is generally seen as the founder of the Amish and Mennonites. 6.4.1 Importing Network and Attribute Data Import the leadership network and modify the graph as undirected. anabaptist_net &lt;- read.paj(&quot;data/Anabaptist Leaders.net&quot;) anabaptist_net &lt;- set.network.attribute(anabaptist_net, &quot;directed&quot;, FALSE) anabaptist_net &lt;- set.network.attribute(anabaptist_net, &quot;multiple&quot;, FALSE) # Now inspect the object anabaptist_net Network attributes: vertices = 67 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE title = Anabaptist Leaders total edges= 366 missing edges= 0 non-missing edges= 366 Vertex attribute names: vertex.names x y z Edge attribute names: Anabaptist Leaders Now, lets bring in the Anabaptist Attributes.csv data, which includes the node attributes. The first six identify whether someone (1) embraced believers baptism, (2) supported violence, (3) participated in the Münster Rebellion, (4) held apocalyptic beliefs, (5) was an Anabaptist, and/or (6) was a follower of Melchior Hoffman (i.e., a Melchiorite). The last two combine other vectors in order to create a new set of attributes. The first creates vector that distinguishes between Anabaptists who didnt participate in the Münster Rebellion, Anabaptists who did, and non-Anabaptists (e.g, Martin Luther, John Calvin). The values of the resulting vector equal 0 for non-Anabaptists, 1 for Anabaptists who didnt participate in the rebellion, and 2 for those who did. The second creates a vector that distinguishes between Melchiorite Anabaptists, Non-Melchiorite Anabaptists, and non-Anabaptists. The values of the resulting vector equal 0 for non-Anabaptists, 1 for non-Melchiorite Anabaptists, and 2 for Melchiorite Anabaptists. attributes &lt;- read.csv(&quot;data/Anabaptist Attributes.csv&quot;, header = TRUE) Before moving forward to the next step, lets briefly inspect the imported attribute variables in attributes. Begining with looking at the column names. colnames(attributes) [1] &quot;ï..Names&quot; &quot;Believers.Baptism&quot; &quot;Violence&quot; [4] &quot;Munster.Rebellion&quot; &quot;Apocalyptic&quot; &quot;Anabaptist&quot; [7] &quot;Melchiorite&quot; &quot;Swiss.Brethren&quot; &quot;Denck&quot; [10] &quot;Hut&quot; &quot;Hutterite&quot; &quot;Other.Anabaptist&quot; [13] &quot;Lutheran&quot; &quot;Reformed&quot; &quot;Other.Protestant&quot; [16] &quot;Tradition&quot; &quot;Origin..&quot; &quot;Operate..&quot; Each column in the data frame represents a vector of values that correspond with the nodes in our graph. We can subset each or multiple columns using the [ operator. If you are unfamiliar with this form of subsetting, keep in mind that you may provide two sets of values separated by a comma to access either rows or columns. The former will subset rows (e.g., attributes[1:3, ]), the latter will return columns (e.g., attributes[, 12]). However, since we have named columns, we can also supply the name of the column in order to subset (e.g., attributes[, \"Violence\"]). For instance, we can select the first five rows of the Anabaptist column. attributes[1:5, &quot;Anabaptist&quot;] [1] 0 0 0 0 1 What this tells us is that of the first five actors listed in the data, only Conrad Grebel was an Anabaptist. We can also get the same information this way. attributes[1:5, # Rows 1 through 5 c(1, 6)] # Columns 1 and 6 ï..Names Anabaptist 1 Martin Luther 0 2 John Calvin 0 3 Ulrich Zwingli 0 4 Joachim Vadian 0 5 Conrad Grebel 1 The following commands create and add a series of vectors to the attributes data.frame that we can later use with our network. attributes[[&quot;anabmunst&quot;]] &lt;- attributes[, &quot;Munster.Rebellion&quot;] + attributes[, &quot;Anabaptist&quot;] attributes[[&quot;anabmelch&quot;]] &lt;- attributes[, &quot;Melchiorite&quot;] + attributes[, &quot;Anabaptist&quot;] 6.4.2 Plotting the Anabaptist Network Plot the network (and save the coordinates) where color indicates whether the actor is an Anabaptist who didnt participate in the Munster Rebellion, an Anabaptist who did, or a non-Anabaptist (anabmunst variable). # Save coordinates layout_fr &lt;- gplot.layout.fruchtermanreingold(anabaptist_net, layout.par = NULL) # Plot graph gplot(dat = anabaptist_net, gmode = &quot;onemode&quot;, coord = layout_fr, vertex.col = attributes[[&quot;anabmunst&quot;]], vertex.cex = 2, displaylabels = TRUE, label.cex = .6, label.pos = 5, usearrows = FALSE) Black vertices are Anabaptists, red are Anabaptists who participated in the Munster Rebellion, and White are non-Anabaptists. Now, lets give the network some more colorful colors. # Declare which values correspond to which color recode &lt;- c(`0` = &quot;yellow&quot;, `1` = &quot;lightblue&quot;, `2` = &quot;red&quot;) # Add a new variable to our attributes data.frame for color attributes[[&quot;color&quot;]] &lt;- recode[as.character(attributes[[&quot;anabmunst&quot;]])] gplot(dat = anabaptist_net, gmode = &quot;onemode&quot;, coord = layout_fr, vertex.col = attributes[[&quot;color&quot;]], vertex.cex = 2, displaylabels = TRUE, label.cex = .6, label.pos = 5, usearrows = FALSE) 6.4.3 Shrinking (Collapsing, Contracting) Networks Currently, statnet does not have a function for collapsing, contracting, shrinking networks; igraph does. You could use intergraph to transition neatly from statnet to igraph and back. 6.4.4 Extracting Subnetworks To extract subnetworks in statnet, we use the get.inducedSubgraph() command. Here, we extract just the Anabaptists and plot the resulting network. First, though, we need to assign the Anabaptist attribute to the network. Using the table() command, we can see that there are 55 Anabaptists, which means that the extracted subnetwork should have 55 actors. That is what we end up with. table(attributes[[&quot;Anabaptist&quot;]]) 0 1 12 55 Now assign this vector to a vertex attribute using the %v% operator. anabaptist_net %v% &quot;anabaptist&quot; &lt;- attributes[[&quot;Anabaptist&quot;]] Now, extract the subgraph by indicating which vertices should be included. To do so, use the get.inducedSubgraph() function, specifying the vertices with a value of 1 for the attribute anabaptist. # Extract just the Anabaptists anabaptists_net &lt;- get.inducedSubgraph(anabaptist_net, v = which( anabaptist_net %v% &quot;anabaptist&quot; == 1)) Now, plot the extracted network. Note that it is disconnected. That is, it is held together by non-Anabaptists. gplot(dat = anabaptists_net, gmode = &quot;onemode&quot;, mode = &quot;fruchtermanreingold&quot;, vertex.col = &quot;Light Blue&quot;, vertex.cex = 2, displaylabels = TRUE, label.cex = .6, label.pos = 5, usearrows = FALSE) 6.5 Multiple (Stacked) Networks in statnet: Sampson Monastery The data we will use in this exercise are the Sampson Monastery network data collected by Samuel Sampson. Sampson observed and recorded the social interactions among a group of novices (men who were preparing to join a monastic order). He recorded four types of ties: esteem (SAMPES) and disesteem (SAMPDES); liking (SAMPLK - three different time periods recorded) and disliking (SAMPDLK - one-time period recorded); positive influence (SAMPIN) and negative influence (SAMPNIN); praise (SAMPPR) and blame (SAMPNPR). Each novice only ranked his top three choices for each type of tie where a 3 indicates their first choice, a 2 their second, and a 1 their third (some subjects offered tied ranks for their top four choices). During Sampsons period of observation, a crisis in the cloister occurred in response to some of the changes proposed by the Second Vatican Council (Vatican II). This led to the expulsion of four novices and the voluntary departure of several others. Based on his observations, Sampson partitioned (i.e., sorted, divided) the novices into four groups: (1) the young turks, (2) the loyal opposition, (3) the outcasts, and (4) the neutrals. The young turks arrived later and questioned some of the monasterys practices, which the loyal opposition defended. The outcasts were novices that were not accepted by the larger group, and the neutrals were those who did not take sides in the debate. Most of the loyal opposition had attended a seminary, Cloisterville, prior to their arrival at the monastery. 6.5.1 Importing Edge List Network Data Well bring in the four positive tie edge If you are interested in what the edge lists look like, inspect them after importing. liking3 &lt;- read.csv(&quot;data/Liking 3.csv&quot;, header = TRUE) esteem &lt;- read.csv(&quot;data/High esteem.csv&quot;, header = TRUE) influence &lt;- read.csv(&quot;data/Positive Influence.csv&quot;, header = TRUE) praise &lt;- read.csv(&quot;data/Praise.csv&quot;, header = TRUE) You can and should inspect all imported objects. Do they share column names? head(liking3) head(esteem) head(influence) head(praise) Now, lets convert each edge list into network objects that statnet will recognize. liking3_net &lt;- network(x = liking3, matrix.type = &quot;edgelist&quot;, directed = TRUE) esteem_net &lt;- network(x = esteem, matrix.type = &quot;edgelist&quot;, directed = TRUE) influence_net &lt;- network(x = influence, matrix.type = &quot;edgelist&quot;, directed = TRUE) praise_net &lt;- network(x = praise, matrix.type = &quot;edgelist&quot;, directed = TRUE) We can use the flexibility of the list() function to create an ordered container for the network objects. samppos &lt;- list(&quot;liking3&quot; = liking3_net, &quot;esteem&quot; = esteem_net, &quot;influence&quot; = influence_net, &quot;praise&quot; = praise_net) Next, we will turn each network object in samppos into a sociomatrix (matrix class) in a new list. We are doing this, to demonstrate a feature of statnet for working with network and matrix stacks. samp_pos_smats &lt;- lapply(samppos, as.sociomatrix) Lets examine the output samp_pos_smats and compare it with the list input sampos. summary(samppos) Length Class Mode liking3 5 network list esteem 5 network list influence 5 network list praise 5 network list summary(samp_pos_smats) Length Class Mode liking3 324 -none- numeric esteem 324 -none- numeric influence 324 -none- numeric praise 324 -none- numeric As you can see, both contain 4 objects. samppos contains four networks while samp_pos_smats includes only 18x18 sociomatrices. Both objects are stacks of objects. We can run the stackcount() function to see how many networks or networks are stacked together. stackcount(samppos) [1] 4 stackcount(samp_pos_smats) [1] 4 6.5.2 Plotting Stacked Networks par(mfrow = c(1, 2)) # Save coordinates coordskk &lt;- gplot.layout.kamadakawai(samppos, layout.par = NULL) gplot(dat = samp_pos_smats[[&quot;liking3&quot;]]| samp_pos_smats[[&quot;esteem&quot;]]| samp_pos_smats[[&quot;influence&quot;]]| samp_pos_smats[[&quot;praise&quot;]], gmode = &quot;digraph&quot;, coord = coordskk, label = network.vertex.names(samppos[[&quot;liking3&quot;]]), vertex.col = &quot;Light Blue&quot;, label.col = &quot;black&quot;, label.cex = 0.4, label.pos = 5, edge.col = &quot;gray&quot;, edge.curve = 0.05, usecurve = TRUE, usearrows = TRUE ) gplot(dat = samppos[[&quot;liking3&quot;]]| samppos[[&quot;esteem&quot;]]| samppos[[&quot;influence&quot;]]| samppos[[&quot;praise&quot;]], gmode = &quot;digraph&quot;, coord = coordskk, label = network.vertex.names(samppos[[&quot;liking3&quot;]]), vertex.col = &quot;Light Blue&quot;, label.col = &quot;black&quot;, label.cex = 0.4, label.pos = 5, edge.col = &quot;gray&quot;, edge.curve = 0.05, usecurve = TRUE, usearrows = TRUE) Now, lets plot the networks where the ties between the actors vary in color based on the type of tie. First, lets visualize the stacked list of network in samppos. gplot(dat = samppos[[&quot;liking3&quot;]]| samppos[[&quot;esteem&quot;]]| samppos[[&quot;influence&quot;]]| samppos[[&quot;praise&quot;]], gmode = &quot;digraph&quot;, coord = coordskk, label = network.vertex.names(samppos[[&quot;liking3&quot;]]), vertex.col = &quot;Light Blue&quot;, label.col = &quot;black&quot;, label.cex = 0.4, label.pos = 5, edge.col = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;yellow&quot;), edge.curve = 0.05, usecurve = TRUE, usearrows = TRUE) 6.5.3 Extracting and Plotting Individual Networks We can also extract individual networks and sociomatrices from the containers and plot them separately. For instance, lets extract liking3. par(mfrow = c(1, 2)) # Plot individual layers: gplot(dat = samppos[[&quot;liking3&quot;]], gmode = &quot;digraph&quot;, coord = coordskk, # Note that the function to extract names expects a network object: label = network.vertex.names(samppos[[&quot;liking3&quot;]]), vertex.col = &quot;Light Blue&quot;, label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, usearrows = TRUE) gplot(dat = samp_pos_smats[[&quot;liking3&quot;]], gmode = &quot;digraph&quot;, coord = coordskk, # Note that the function to extract names expects a matrix object: label = rownames(samp_pos_smats[[&quot;liking3&quot;]]), vertex.col = &quot;Light Blue&quot;, label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, usearrows = TRUE) Thats all for statnet for now. References "],["network-topography-in-igraph.html", "7 Network Topography in igraph 7.1 Setup 7.2 Load Libraries 7.3 Import Data 7.4 Network Size and Interconnectedness 7.5 Centralization and Related Measures of Spread 7.6 Calculating the E-I Index with isnar 7.7 Network Level Measures Table", " 7 Network Topography in igraph 7.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ################################################# # What: Network Topography in R # Created: 02.28.14 # Revised: 01.18.22 ################################################# If you have not set up your RStudio Project to clear the workspace on exit, your environment contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (Anabaptists Leaders.csv, and Anabaptists Attributes.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. For this exercise, well use the Anabaptist Leadership network and its related attribute data, both of which can be found in the file we shared with you. The data set includes 67 actors, 55 who were sixteenth century Anabaptist leaders and 12 who were prominent Protestant Reformation leaders who had contact with and influenced some of the Anabaptist leaders included in this data set. These network data build upon a smaller dataset (Matthews et al. 2013) that did not include some leading Anabaptist leaders, such as Menno Simons, who is generally seen as the founder of the Amish and Mennonites. We will add a measure here not implemented in the statnet version of this lab, namely the clustering coefficient. A few versions exist but it is conceptually similar to other measures of interconnectedness. 7.2 Load Libraries Load igraph library. library(igraph) It is not currently possible to calculate the E-I index in statnet and igraph, but a package, isnar, has been developed to do just that. Its functionality is demonstrated at the end of this lab. Weve included the scripts in both statnet and igraph versions of this lab, but you need to do this section only once. In addition to igraph, we will be introducing and using isnar. Since this may be the first time you are using this tool, please ensure you install it prior to loading it. You will need to install remotes in order to use the function install_github() to download and set up isnar as it is not published on the CRAN. install.packages(&quot;remotes&quot;) Now install isnar. remotes::install_github(&quot;mbojan/isnar&quot;) Before moving forward, lets load the isnar package: library(isnar) Note: igraph imports the %&gt;% operator on load (library(igraph)). This series of exercises leverages the operator because we find it very useful in chaining functions. We occasionally show how to carry out a series of commands with and without piping. 7.3 Import Data Lets import the data using the read.csv() function. Remember, igraphs graph_adjacency() function requires a matrix. anabaptist_matrix &lt;- read.csv(&quot;data/Anabaptist Leaders.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) %&gt;% as.matrix() Now transform the matrix to an igraph object. anabaptist_ig &lt;- graph.adjacency(anabaptist_matrix, mode = &quot;undirected&quot;) anabaptist_ig IGRAPH 2ed28e0 UN-- 67 183 -- + attr: name (v/c) + edges from 2ed28e0 (vertex names): [1] Martin Luther --Ulrich Zwingli Martin Luther --Thomas Muntzer [3] Martin Luther --Andreas Carlstadt Martin Luther --Caspar Schwenckfeld [5] Martin Luther --Melchior Hofmann Martin Luther --Philipp Melanchthon [7] Martin Luther --Martin Bucer John Calvin --Wolfgang Capito [9] John Calvin --Martin Bucer Ulrich Zwingli--Joachim Vadian [11] Ulrich Zwingli--Conrad Grebel Ulrich Zwingli--Felix Manz [13] Ulrich Zwingli--George Blaurock Ulrich Zwingli--Wilhelm Reublin [15] Ulrich Zwingli--Johannes Brotli Ulrich Zwingli--Louis Haetzer + ... omitted several edges To correctly calculate a number of topographical metrics in igraph (e.g., centralization), we need to make sure that the network is a simple graph/network, that is, a network without multiple lines or loops (diagonal). We can check whether the Anabaptist network is a simple graph with the following command: is_simple(anabaptist_ig) [1] TRUE We can see that it is already a simple graph, so we dont have to simplify it. However, if we need to, we would issue the following command: simplify(anabaptist_ig, remove.multiple = TRUE, remove.loops = TRUE, ) IGRAPH 2edfd48 UN-- 67 183 -- + attr: name (v/c) + edges from 2edfd48 (vertex names): [1] Martin Luther --Ulrich Zwingli Martin Luther --Thomas Muntzer [3] Martin Luther --Andreas Carlstadt Martin Luther --Caspar Schwenckfeld [5] Martin Luther --Melchior Hofmann Martin Luther --Philipp Melanchthon [7] Martin Luther --Martin Bucer John Calvin --Wolfgang Capito [9] John Calvin --Martin Bucer Ulrich Zwingli--Joachim Vadian [11] Ulrich Zwingli--Conrad Grebel Ulrich Zwingli--Felix Manz [13] Ulrich Zwingli--George Blaurock Ulrich Zwingli--Wilhelm Reublin [15] Ulrich Zwingli--Johannes Brotli Ulrich Zwingli--Louis Haetzer + ... omitted several edges Note that the defaults for the remove.multiple and remove.loops options are TRUE, so we didnt really need to include them in the previous command. 7.4 Network Size and Interconnectedness 7.4.1 Network Size Network size is a basic descriptive statistic that is important to know because many of the subsequent measures are sensitive to it. Network size is easy to get with the vcount() function. As you may have noticed, you get the network size as well when you call the igraph object anabaptist_ig, which is what we just did in the previous step. vcount(anabaptist_ig) [1] 67 7.4.2 Density and Average Degree Network density equals actual ties divided by all possible ties. However, density tends to decrease as social networks get larger because the number of possible ties increases exponentially, whereas the number of ties that each actor can maintain tends to be limited. Consequently, we can only use it to compare networks of the same size. An alternative to network density is average degree centrality, which is not sensitive to network size and thus can be used to compare different sized networks. First, calculate density using density using the edge_density() function. edge_density(anabaptist_ig) [1] 0.08276798 In order to calculate the average degree centrality, you will have to calculate vertex degree and proceed taking the average of this vector of scores. degree(anabaptist_ig) %&gt;% mean() [1] 5.462687 You can also do this not using pipes: mean(degree(anabaptist_ig)) [1] 5.462687 Keep in mind that you may continue refining the output by rounding the value: edge_density(anabaptist_ig) %&gt;% round(digits = 3) [1] 0.083 degree(anabaptist_ig) %&gt;% mean() %&gt;% round(digits = 3) [1] 5.463 7.4.3 Clustering Coefficient (Global and Local) To calculate this measure, use the transitivity() function. This measure can be calculated for each vertex (type = \"local\") or as a ratio of triangles and the connected triples in the graph (type = \"global\"). # Traditional transitive measure: transitivity(anabaptist_ig, type = &quot;global&quot;) [1] 0.3557214 # Local transitive scores (for each ego): transitivity(anabaptist_ig, type = &quot;local&quot;) [1] 0.2857143 1.0000000 0.1923077 0.3333333 0.3818182 0.6666667 0.2380952 [8] 0.2527473 0.6666667 0.4000000 1.0000000 1.0000000 1.0000000 1.0000000 [15] 0.2222222 0.3333333 0.5000000 0.2545455 0.2166667 0.5000000 0.2142857 [22] 0.7000000 0.3888889 0.3333333 0.2500000 0.3333333 1.0000000 0.3214286 [29] 0.4285714 0.4285714 0.3928571 0.5000000 0.7619048 0.8000000 0.8666667 [36] 0.3571429 NaN 0.0000000 0.0000000 NaN 0.8333333 0.5000000 [43] NaN 0.4000000 1.0000000 0.0000000 0.1666667 0.4444444 0.4444444 [50] 1.0000000 NaN 0.4722222 0.4444444 0.5000000 0.2380952 0.5000000 [57] 1.0000000 0.3333333 0.5000000 0.0000000 0.3000000 0.0000000 0.3333333 [64] 0.3111111 0.3111111 NaN 0.0000000 Notice the NaN values (not a number). We can take the average of local clustering coefficients and ignore these missing values by combining this function with the mean() function. mean( transitivity(anabaptist_ig, type = &quot;local&quot;), na.rm = TRUE ) [1] 0.4605426 Alternatively, rather than removing the NaN we could zero them out and include them in the calculation of an average clustering coefficient. This is how ORA calculates the measure. trans &lt;- transitivity(anabaptist_ig, type = &quot;local&quot;) # Calculate the mean: mean( # Recode trans vector, if NaN assing 0, otherwise return value sapply(trans, function(s) ifelse(is.nan(s), 0, s)) ) [1] 0.4261737 7.4.4 Cohesion and Fragmentation Now we turn to some additional measures related to the concept of interconnectedness. cohesion(anabaptist_ig) [1] 1 Because the network is not disconnected, cohesion is 1.00 and fragmentation is 0.00. However, with a little manipulation, we can also compute distance weighted cohesion and fragmentation, what is often called compactness and breadth. First, calculate length of all shortest paths from or to the vertices in the graph. anabaptist_dist &lt;- distance_table(anabaptist_ig, directed = FALSE) The distance_table() function returns a named list with two objects. The first, res, is a numeric vector of distances. The second, unconnected, the number of unconnected pairs. The sum of the two is always n(n-1) for directed graphs and n(n-1)/2 for undirected graphs, which is the number of potential pairs in a network. Cohesion can be calculated by adding the number of connected pairs divided by the total number of possible pairs in the network. # Calculate cohesion sum(anabaptist_dist$res) / (sum(anabaptist_dist$res) + anabaptist_dist$unconnected) [1] 1 Calculating the fragmentation is as simple as removing the cohesion score from 1. # Calculate fragmentation 1 - sum(anabaptist_dist$res) / (sum(anabaptist_dist$res) + anabaptist_dist$unconnected) [1] 0 7.4.5 Compactness and Breadth igraph has no direct way to calculate compactness. However, here is how to compute compactness and breadth using the available tools from igraph. First, calculate the length of all the shortest paths for all vertices in the network. distance &lt;- distances(anabaptist_ig) Take a look at the matrix of distances, here only the first four rows and columns: distance[1:4, 1:4] Martin Luther John Calvin Ulrich Zwingli Joachim Vadian Martin Luther 0 2 1 2 John Calvin 2 0 2 3 Ulrich Zwingli 1 2 0 1 Joachim Vadian 2 3 1 0 We can read these distances as steps between nodes. So Martin Luther is two steps away from John Calvin. Calculating compactness requires calculating the reciprocal distance by taking the inverse of the distances in the matrix, removing the diagonal containing self distance scores and replacing infinite distances (disconnected nodes listed as Inf) with a zero. Then taking the mean of all reciprocal distances in the matrix. # Calculate reciprocal distances reciprocal_distances &lt;- 1/distance # Modify the reciprocal_distances matrix diag(reciprocal_distances) &lt;- NA reciprocal_distances[reciprocal_distances == Inf] &lt;- 0 # Calculate compactness compactness &lt;- mean(reciprocal_distances, na.rm = TRUE) compactness [1] 0.3800372 For breadth, we could, of course, just take the additive inverse of compactness. breadth &lt;- 1 - compactness breadth [1] 0.6199628 7.5 Centralization and Related Measures of Spread Network centralization, variance, and standard deviation are measures that can capture the hierarchical dimension of a networks topography. Centralization uses the variation in actor centrality (as compared to the highest centrality score) within the network to measure the level of centralization. More variation yields higher network centralization scores, while less yields lower scores. In general, the larger a centralization index is, the more likely it is that a single actor is very central while the other actors are not. Thus, the index can be seen as measuring how unequal the distribution of individual actor scores are. Because we can calculate centralization using different measures of centrality (e.g., degree, betweenness, closeness, and eigenvector), we need to interpret the results in light of the type of centrality used. Centralization scores range from 0.00  1.00 (or 0  100%) when analyzing dichotomized data. If you are analyzing valued data, centralization scores will sometimes be larger than 1.00; thus, its generally a good idea to dichotomize your data before estimating network centralization. 7.5.1 Centralization Heres how to get centralization scores for the four primary measures of centrality that weve discussed in previous classes. Lets begin taking a look at how to calculate degree centralization, which is accomplished in igraph through the centralization.degree() function. It takes an igraph object as input and return a named list with three components: res: a numeric vector containing the node-level degree centrality score for all vertices in a graph centralization: a graph level centrality index theoretical_max: The theoretical maximum graph level centralization for a graph with the given number of nodes Since we are looking for topographical or network level measures, the focus here is on extracting the centralization component from the output. # First calculate the centralization anabaptist_deg_cent &lt;- centralization.degree(anabaptist_ig, loops = FALSE) # Now return the named component of interest anabaptist_deg_cent$centralization [1] 0.1645688 You could assign the centralization score to an object, or bypass this step and just call it by attaching a $ accessor and the named component to the function call. # Calculate betweenness centralization centralization.betweenness(anabaptist_ig)$centralization [1] 0.1974781 Here is the last two remaining centralization functions. # Calculate closensess centralization centralization.closeness(anabaptist_ig)$centralization [1] 0.2199767 # Calculate eigenvector centralization centralization.evcent(anabaptist_ig, scale = FALSE)$centralization [1] 0.3067772 7.5.2 Variance and Standard Deviation Variance and standard deviation are similar to centralization. They differ from centralization in that rather comparing individual scores to the highest centrality score, they compare individual scores to the average centrality score. Because standard deviation is the square root of the variance, it is probably preferable to variance because it returns to the original unit of measure. Heres how to get the standard deviation of the network. To do so, you will have to provide the sd() function with a numeric vector, which will represent the node level measures (e.g., degree centrality (degree()), closeness (closeness()), etc.). Lets begin by setting up the code to calculate the standard deviation for the anabaptist_ig graph based on degree centrality. # Calculate standard deviation sd( # Provide the numeric vector of degree scores degree(anabaptist_ig, # Ignore loop edges loops = FALSE) ) [1] 3.434797 Now calculate the standard deviation for closeness, betweenness, and eigenvector centrality. sd( closeness(anabaptist_ig, normalized = TRUE) ) [1] 0.05769685 sd( betweenness(anabaptist_ig) ) [1] 110.4204 sd( # Returns a named list, with the centrality scores in the vector component evcent(anabaptist_ig, scale = FALSE)$vector ) [1] 0.0801868 A drawback of standard deviation sd.deg &lt;- sd(degree(anabaptist_ig)) sd.clo &lt;- sd(closeness(anabaptist_ig, normalized = TRUE)) sd.bet &lt;- sd(betweenness(anabaptist_ig)) sd.eig &lt;- sd(evcent(anabaptist_ig, scale = TRUE)$vector) # Create a star graph with the same number of actors star.ig &lt;- make_star(vcount(anabaptist_ig), mode = &quot;undirected&quot;) plot(star.ig) # Standard deviation of star graphs starsd.deg &lt;- sd(degree(star.ig)) starsd.clo &lt;- sd(closeness(star.ig, normalized = TRUE)) starsd.bet &lt;- sd(betweenness(star.ig)) starsd.eig &lt;- sd(evcent(star.ig, scale = TRUE)$vector) # Divide the first by the second sd.deg/starsd.deg [1] 0.4325388 sd.clo/starsd.clo [1] 0.9518038 sd.bet/starsd.bet [1] 0.421366 sd.eig/starsd.eig [1] 2.469133 7.5.3 Diameter and Average Path Distance Heres how to get geodesic information on a network and then use it to calculate average distance and diameter. The diameter is the longest of all shortest paths that traverse the network. It is calculated in igraph using the diameter() function. diameter(anabaptist_ig, directed = FALSE, unconnected = FALSE) [1] 9 The average path length is the shortest paths between all actors in the network. It is calculated in igraph using the average.path.lenght() function. average.path.length(anabaptist_ig) [1] 3.354138 7.6 Calculating the E-I Index with isnar This section is in both statnet and igraph versions of this lab. You only need to do this section one time. E-I Index indicate the ration of ties a group has to nongroup members. The index equals 1.0 for groups that have all external ties, while a group with -1.0 score has all internal ties. If the internal and external ties are equal, the index equals 0.0. The E-I Index is not common to many R packages, and it is not as simple as one would think it would be to program. However, there is a package called isnar that does calculate it (Bojanowski 2021). It is written and maintained by Michal Bojanowski (m.bojanowski@icm.edu.pl) as a supplement to igraph. The only thing is that isnar is only available through GitHub. GitHub is a repository for open-source software, like R packages in development. To estimate the E-I index, we require an attribute vector. Here, well use the Melchiorite attribute included in the attribute file. attributes &lt;- read.csv(&quot;data/Anabaptist Attributes.csv&quot;, header = TRUE) Take a look at the vector names. names(attributes) [1] &quot;ï..Names&quot; &quot;Believers.Baptism&quot; &quot;Violence&quot; [4] &quot;Munster.Rebellion&quot; &quot;Apocalyptic&quot; &quot;Anabaptist&quot; [7] &quot;Melchiorite&quot; &quot;Swiss.Brethren&quot; &quot;Denck&quot; [10] &quot;Hut&quot; &quot;Hutterite&quot; &quot;Other.Anabaptist&quot; [13] &quot;Lutheran&quot; &quot;Reformed&quot; &quot;Other.Protestant&quot; [16] &quot;Tradition&quot; &quot;Origin..&quot; &quot;Operate..&quot; The Melchiorite vector can be accessed using the [[ accessor. Now, use the ei() function to get the E-I index. ei(anabaptist_ig, attributes[[&quot;Melchiorite&quot;]], loops = FALSE, directed = FALSE) [1] -0.9344262 7.7 Network Level Measures Table You may want to export out these measures as a table for your report. Luckily, we can use a data.frame to capture the data in a tabular format, then export it out as a CSV. # First, create a data.frame of outputs net_topography &lt;- data.frame( `size` = vcount(anabaptist_ig), `average distance` = average.path.length(anabaptist_ig), `diameter` = diameter(anabaptist_ig), `degree centralization` = centralization.degree(anabaptist_ig)$centralization, `standard deviation` = sd(degree(anabaptist_ig)), `density` = edge_density(anabaptist_ig), `average degree` = mean(degree(anabaptist_ig)), `global clustering coefficient` = transitivity(anabaptist_ig, type = &quot;global&quot;) ) Take a look at the output: str(net_topography) &#39;data.frame&#39;: 1 obs. of 8 variables: $ size : int 67 $ average.distance : num 3.35 $ diameter : num 9 $ degree.centralization : num 0.16 $ standard.deviation : num 3.43 $ density : num 0.0828 $ average.degree : num 5.46 $ global.clustering.coefficient: num 0.356 Export it out. write.csv(net_topography, file = &quot;network_topography.csv&quot;, row.names = FALSE) Thats all for igraph now. References "],["network-topography-in-statnet.html", "8 Network Topography in statnet 8.1 Setup 8.2 Load Libraries 8.3 Import Data 8.4 Network Size and Interconnectedness 8.5 Centralization and Related Measures of Spread 8.6 Calculating the E-I Index with isnar", " 8 Network Topography in statnet 8.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ################################################# # What: Network Topography in R # Created: 02.28.14 # Revised: 01.18.22 ################################################# If you have not set up your RStudio Project to clear the workspace on exit, your environment contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (Anabaptists Leaders.csv, and Anabaptists Attributes.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. For this exercise, well use the Anabaptist Leadership network and its related attribute data, both of which can be found in the file we shared with you. The data set includes 67 actors, 55 who were sixteenth century Anabaptist leaders and 12 who were prominent Protestant Reformation leaders who had contact with and influenced some of the Anabaptist leaders included in this data set. These network data build upon a smaller dataset (Matthews et al. 2013) that did not include some leading Anabaptist leaders, such as Menno Simons, who is generally seen as the founder of the Amish and Mennonites. 8.2 Load Libraries Load statnet library. library(statnet) It is not currently possible to calculate the E-I index in statnet and igraph, but a package, isnar, has been developed to do just that. Its functionality is demonstrated at the end of this lab. Weve included the scripts in both statnet and igraph versions of this lab, but you need to do this section only once. In addition to statnet, we will be introducing and using isnar. Since this may be the first time you are using this tool, please ensure you install it prior to loading it. You will need to install remotes in order to use the function install_github() to download and set up isnar as it is not published on the CRAN. install.packages(&quot;remotes&quot;) Now install isnar. remotes::install_github(&quot;mbojan/isnar&quot;) Before moving forward, lets load the isnar package: library(isnar) 8.3 Import Data Lets import the data, which weve stored as a matrix, using the read.csv() function nested within as.matrix() in order to return a matrix class object, which is one format required by the as.network() function to generate a network object. # First, read it the matrix of relations anabaptist_mat &lt;- as.matrix( read.csv(&quot;data/Anabaptist Leaders.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) ) Now transform the matrix to a network object. anabaptist_net &lt;- as.network(anabaptist_mat) Take a look at the newly created object. anabaptist_net Network attributes: vertices = 67 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 366 missing edges= 0 non-missing edges= 366 Vertex attribute names: vertex.names No edge attributes 8.4 Network Size and Interconnectedness 8.4.1 Network Size Network size is a basic descriptive statistic that is important to know because many of the subsequent measures are sensitive to it. Network size is easy to get with the network.size() function. network.size(anabaptist_net) [1] 67 8.4.2 Density and Average Degree Network density equals actual ties divided by all possible ties. However, density tends to decrease as social networks get larger because the number of possible ties increases exponentially, whereas the number of ties that each actor can maintain tends to be limited. Consequently, we can only use it to compare networks of the same size. An alternative to network density is average degree centrality, which is not sensitive to network size and thus can be used to compare different sized networks. Lets see how we can get these two measures in statnet. First, calculate density using the gden() function. gden(anabaptist_net) [1] 0.08276798 In order to calculate the average degree centrality, you will have to calculate vertex degree and proceed taking the average of this vector of scores. mean( degree(anabaptist_net, # Indicate the type of graph evaluated as undirected gmode = &quot;graph&quot;) ) [1] 5.462687 8.4.3 Cohesion and Fragmentation In statnet the connectedness() function takes a graph and returns the Krackhardt connectedness score (Krackhardt 1994), which other programs, such as UCINET, call cohesion. Fragmentation is simply the additive inverse of cohesion First take a look at how to calculate connectedness. connectedness(anabaptist_net) [1] 1 Now calculate fragmentation. 1 - connectedness(anabaptist_net) [1] 0 8.4.4 Compactness and Breadth Because the network is not disconnected, cohesion is 1.00 and fragmentation is 0.00. However, with a little manipulation, we can also compute distance weighted cohesion and fragmentation, what other programs, such as UCINET, calls compactness and breadth. Calculating compactness requires calculating the geodesic distances between all nodes in the network. Then take the inverse of these scores, which are the reciprocal geodesic distance. Remove self loops. Finally, replace the infinity scores, which occur in disconnected graphs, with 0. First, lets begin by calculating the distances: distance &lt;- geodist(anabaptist_net, # Replace the Inf values with 0s inf.replace = 0) Take a look at the matrix of distances, here only the first four rows and columns: distance$gdist[1:4, 1:4] [,1] [,2] [,3] [,4] [1,] 0 2 1 2 [2,] 2 0 2 3 [3,] 1 2 0 1 [4,] 2 3 1 0 We can read these distances as steps between nodes. So node one is two steps away from node two. Proceed with the remaining steps outlined above to calculate the desired measure. # Calculate reciprocal distances reciprocal_distances &lt;- 1/distance$gdist # Modify the reciprocal_distances matrix diag(reciprocal_distances) &lt;- NA reciprocal_distances[reciprocal_distances == Inf] &lt;- 0 # Calculate compactness compactness &lt;- mean(reciprocal_distances, na.rm = TRUE) compactness [1] 0.3800372 For breadth, we could, of course, just take the additive inverse of compactness. breadth &lt;- 1 - compactness breadth [1] 0.6199628 We can automate the process of calculating compactness by turning the process into a function. my_compactness &lt;- function(dat, na_rm = TRUE) { stopifnot(!is.network(dat) == &quot;dat must be network object.&quot;) stopifnot(!is.logical(na_rm) == &quot;na_rm must be a logical.&quot;) # Get reciprocal distances: reciprocal_distances &lt;- 1/geodist(anabaptist_net, inf.replace = 0)$gdist # Clean up the matrix: diag(reciprocal_distances) &lt;- NA reciprocal_distances[reciprocal_distances == Inf] &lt;- 0 # Calculate compacteness mean(reciprocal_distances, na.rm = na_rm) } Run the function. my_compactness(anabaptist_net) [1] 0.3800372 8.4.5 Table of Interconnectedness Scores We can create a table of interconnectedness scores and save them to a csv file. You can check your working directory to see the results in the interconnectedness.csv file. # Create a data.frame with the desired measures interconnectedness &lt;- data.frame( &quot;Size&quot; = network.size(anabaptist_net), &quot;Density&quot; = gden(anabaptist_net), &quot;Average Degree&quot; = mean(degree(anabaptist_net, gmode = &quot;graph&quot;)), &quot;Cohesion&quot; = connectedness(anabaptist_net), &quot;Fragmentation&quot; = 1 - connectedness(anabaptist_net), &quot;Compactness&quot; = my_compactness(anabaptist_net), &quot;Breadth&quot; = 1 - my_compactness(anabaptist_net) ) # Take a look str(interconnectedness) &#39;data.frame&#39;: 1 obs. of 7 variables: $ Size : num 67 $ Density : num 0.0828 $ Average.Degree: num 5.46 $ Cohesion : num 1 $ Fragmentation : num 0 $ Compactness : num 0.38 $ Breadth : num 0.62 Now write it to a CSV: write.csv(interconnectedness, file = &quot;interconnectedness.csv&quot;, row.names = FALSE) 8.5 Centralization and Related Measures of Spread Network centralization, variance, and standard deviation are measures that can capture the hierarchical dimension of a networks topography. Centralization uses the variation in actor centrality (as compared to the highest centrality score) within the network to measure the level of centralization. More variation yields higher network centralization scores, while less yields lower scores. In general, the larger a centralization index is, the more likely it is that a single actor is very central while the other actors are not. Thus, the index can be seen as measuring how unequal the distribution of individual actor scores are. Because we can calculate centralization using different measures of centrality (e.g., degree, betweenness, closeness, and eigenvector), we need to interpret the results in light of the type of centrality used. Centralization scores range from 0.00  1.00 (or 0  100%) when analyzing dichotomized data. If you are analyzing valued data, centralization scores will sometimes be larger than 1.00; thus, its generally a good idea to dichotomize your data before estimating network centralization. 8.5.1 Centralization Heres how to get centralization scores for the four primary measures of centrality. # Degree centralization centralization(dat = anabaptist_net, # Function to return nodal centrality scores, here degree FUN = degree, # Indicate the type of graph being evaluated as undirected mode = &quot;graph&quot;) [1] 0.1645688 # Betweenness centralization centralization(dat = anabaptist_net, # Function to return nodal centrality scores, here betweenness FUN = betweenness, # Indicate the type of graph being evaluated as undirected mode = &quot;graph&quot;) [1] 0.1974781 # Closeness centralization centralization(dat = anabaptist_net, # Function to return nodal centrality scores, here closeness FUN = closeness, # Indicate the type of graph being evaluated as undirected mode = &quot;graph&quot;) [1] 0.2199767 # Eigenvector centralization centralization(dat = anabaptist_net, # Function to return nodal centrality scores, here evcent FUN = evcent, # Indicate the type of graph being evaluated as undirected mode = &quot;graph&quot;) [1] 0.3067772 To calculate ARD (average reciprocal distance) closeness, which is what we want to use when were analyzing a disconnected network, you will have to pass along an additional argument (cmode = \"suminvundir\") to specify the type of closeness being computed. # ARD Closeness centralization centralization(dat = anabaptist_net, # Function to return nodal centrality scores, here closeness FUN = closeness, # Indicate the type of closeness being computed cmode = &quot;suminvundir&quot;, # Indicate the type of graph being evaluated as undirected mode = &quot;graph&quot;) [1] 0.3061346 8.5.2 Variance and Standard Deviation Variance and standard deviation are similar to centralization. They differ from centralization in that rather comparing individual scores to the highest centrality score, they compare individual scores to the average centrality score. Because standard deviation is the square root of the variance, it is probably preferable to variance because it returns to the original unit of measure. Lets begin by setting up the code to calculate the standard deviation for the anabaptist_ig graph based on degree centrality. # Calculate standard deviation sd( # Provide the numeric vector of degree scores degree(anabaptist_net, gmode = &quot;graph&quot;) ) [1] 3.434797 Now calculate the standard deviation for closeness, betweenness, and eigenvector centrality. sd( closeness(anabaptist_net, gmode = &quot;graph&quot;) ) [1] 0.05769685 sd( betweenness(anabaptist_net, gmode = &quot;graph&quot;) ) [1] 110.4204 sd( evcent(anabaptist_net, gmode = &quot;graph&quot;) ) [1] 0.0801868 8.5.3 Table of Centralization Scores Lets create a table of centralization scores and save them to a CSV file, which you should see in your working directory after running the following lines of code. centralization &lt;- data.frame( &quot;Type&quot; = c(&quot;Degree&quot;, &quot;Betweenness&quot;, &quot;Closeness&quot;, &quot;ARD Closeness&quot;, &quot;Eigenvector&quot;), &quot;Centralization&quot; = c( centralization(anabaptist_net, FUN = degree, mode = &quot;graph&quot;), centralization(anabaptist_net, FUN = betweenness, mode = &quot;graph&quot;), centralization(anabaptist_net, FUN = closeness, mode = &quot;graph&quot;), centralization(anabaptist_net, FUN = closeness, mode = &quot;graph&quot;, cmode = &quot;suminvundir&quot;), centralization(anabaptist_net, FUN = evcent, mode = &quot;graph&quot;)), &quot;Standard Deviation&quot; = c( sd(degree(anabaptist_net, gmode = &quot;graph&quot;)), sd(betweenness(anabaptist_net, gmode = &quot;graph&quot;)), sd(closeness(anabaptist_net, gmode = &quot;graph&quot;)), sd(closeness(anabaptist_net, gmode = &quot;graph&quot;, cmode = &quot;suminvundir&quot;)), sd(evcent(anabaptist_net, gmode = &quot;graph&quot;))) ) centralization Type Centralization Standard.Deviation 1 Degree 0.1645688 3.43479680 2 Betweenness 0.1974781 110.42040498 3 Closeness 0.2199767 0.05769685 4 ARD Closeness 0.3061346 0.07458466 5 Eigenvector 0.3067772 0.08018680 Now save it: write.csv(centralization, file= &quot;centralization.csv&quot;, row.names = FALSE) 8.5.4 Diameter and Average Path Distance Heres how to get geodesic information on a network and then use it to calculate average distance and diameter. The diameter is the longest of all shortest paths that traverse the network. It is calculated in statnet using the geodist() function. Because the function returns a named list, we need to specifically extract the geodesic distances that were calculated by command using $gdist. We also need to set the diagonal of each the geodistance network to NA rather than 0, otherwise our scores take into account the diagonal. # Calculate distances distances &lt;- geodist(anabaptist_net, inf.replace = NA)$gdist # Clean up diagonal diag(distances) &lt;- NA After all that is done, then for average distance we simply get the mean of the geodesic distances, and for the diameter, we get the maximum. # Mean distance is the average distance mean(distances, na.rm = TRUE) [1] 3.354138 # Max distance is the diameter max(distances, na.rm = TRUE) [1] 9 8.6 Calculating the E-I Index with isnar This section is in both statnet and igraph versions of this lab. You only need to do this section one time. E-I Index indicate the ration of ties a group has to nongroup members. The index equals 1.0 for groups that have all external ties, while a group with -1.0 score has all internal ties. If the internal and external ties are equal, the index equals 0.0. The E-I Index is not common to many R packages, and it is not as simple as one would think it would be to program. However, there is a package called isnar that does calculate it (Bojanowski 2021). It is written and maintained by Michal Bojanowski (m.bojanowski@icm.edu.pl) as a supplement to igraph. The only thing is that isnar is only available through GitHub. GitHub is a repository for open-source software, like R packages in development. To estimate the E-I index, we need an attribute vector. Here, well use the Melchiorite attribute included in the attribute file. attributes &lt;- read.csv(&quot;data/Anabaptist Attributes.csv&quot;, header = TRUE) Take a look at the vector names. names(attributes) [1] &quot;ï..Names&quot; &quot;Believers.Baptism&quot; &quot;Violence&quot; [4] &quot;Munster.Rebellion&quot; &quot;Apocalyptic&quot; &quot;Anabaptist&quot; [7] &quot;Melchiorite&quot; &quot;Swiss.Brethren&quot; &quot;Denck&quot; [10] &quot;Hut&quot; &quot;Hutterite&quot; &quot;Other.Anabaptist&quot; [13] &quot;Lutheran&quot; &quot;Reformed&quot; &quot;Other.Protestant&quot; [16] &quot;Tradition&quot; &quot;Origin..&quot; &quot;Operate..&quot; The Melchiorite vector can be accessed using the [[ accessor. Now, use the ei() function to get the E-I index. Weve found that calculating the E-I index works best with igraph objects. If you start with statnet and you would like to run the E-I index, then we recommend using intergraph to convert your network object into an igraph object using asIgraph(). anabaptist_ig &lt;- intergraph::asIgraph(anabaptist_net) ei(anabaptist_ig, attributes[[&quot;Melchiorite&quot;]], loops = FALSE, directed = FALSE) [1] -0.9344262 Thats all for statnet now. References "],["detecting-subgroups-in-igraph.html", "9 Detecting Subgroups in igraph 9.1 Setup 9.2 Load Library 9.3 Import and Plot Network 9.4 Components in igraph 9.5 Cliques in igraph 9.6 K-Cores in igraph 9.7 Community Detection in igraph", " 9 Detecting Subgroups in igraph 9.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ################################################# # What: Detecting Subgroups in R with igraph # Created: 02.28.14 # Revised: 01.24.22 ################################################# If you have not set up your RStudio Project to clear the workspace on exit, your environment may contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (Troll_EL.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. Social scientists generally assume that social interaction is the basis for solidarity, shared norms, identity, and collective behavior, so people who interact intensively are likely to consider themselves a social group. Thus, a major focus of social network analysis is to identify dense clusters of actors among whom there are relatively strong, direct, intense, and/or positive ties. These are typically referred to as cohesive subgroups, subnetworks, or sub-clusters. One way to cluster actors is based on shared attributes (e.g., race, gender, etc.). Another is to use the pattern of ties among actors. That is what this lab demonstrates. In an ideal world, there would be a single algorithm for identifying cohesive subgroups, but this is not an ideal world, so social network analysts have developed a variety of algorithms for identifying subnetworks. For this exercise, well use a portion of the Twitter Russian Troll network that FiveThirtyEight released in mid-2018.1 This subset is comprised of retweet ties among approximately 1,700 Twitter handles connected to the Internet Research Agency (a Russian troll factory) that were active from at least May 2015 and up through the 2016 election. From the original data set, weve created an edge list for this subset that contains directed ties among the accounts. 9.2 Load Library Load the igraph library. library(igraph) Note: igraph imports the %&gt;% operator on load (library(igraph)). This series of exercises leverages the operator because we find it very useful in chaining functions. 9.3 Import and Plot Network Lets read the data using the read.csv() function. read.csv(&quot;data/Troll_EL.csv&quot;, header = TRUE) %&gt;% head() Source Target Type Rel 1 1d_nicole_ ina_malone Directed Retweet 2 1d_nicole_ willie_bign Directed Retweet 3 4ever1937 bydrbre_ Directed Retweet 4 4ever1937 novostidamask Directed Retweet 5 4ever1937 novostinn Directed Retweet 6 4mysquad crystal1johnson Directed Retweet Next, convert the file to an igraph object using the graph_from_data_frame()function. The directed = TRUE argument is a bit redundant here but it wont hurt to ensure our ties are read as directed. troll_g &lt;- read.csv(&quot;data/Troll_EL.csv&quot;, header = TRUE) %&gt;% graph_from_data_frame(directed = TRUE) You can use the is.directed() function to double-check if you have a directed network or not (the answer is TRUE in this case). is.directed(troll_g) [1] TRUE Knowing the size of the network will be useful as we start looking at the distribution of nodes across various subgroups we detect throughout this lab. vcount(troll_g) [1] 1704 Lets plot the network to see what it looks like. Since this is a larger network, it will take a little longer to plot it. Our recommendation is to be patient and avoid clicking on anything until the plot appears. plot(troll_g, layout = layout_with_graphopt, main = &quot;Troll Network&quot;, sub = paste0(&quot;Network size: &quot;, vcount(troll_g)), vertex.color = &quot;lightblue&quot;, vertex.label = NA, vertex.size = 5, edge.arrow.size = 0.1) 9.4 Components in igraph Identifying weakly and strongly connected components in igraph is relatively simple, but we will use different functions for each of these, which is not ideal. For weakly connected components, well use decompose() function, which as its name implies, decomposes the network into a list of subnetworks that represent a networks components. It is not an intuitive approach for identifying strongly connected components, however. For strong components, well use the clusters function. Note you can use the clusters function to identify weakly connected components but this approach is a bit more tedious for extracting and manipulating your data later. w_comp &lt;- decompose(troll_g, mode = &quot;weak&quot;) You can see that we have two weak components here. length(w_comp) [1] 2 w_comp [[1]] IGRAPH b68efa8 DN-- 1702 9824 -- + attr: name (v/c), Type (e/c), Rel (e/c) + edges from b68efa8 (vertex names): [1] 1d_nicole_ -&gt;ina_malone 1d_nicole_ -&gt;willie_bign [3] 4ever1937 -&gt;bydrbre_ 4ever1937 -&gt;novostidamask [5] 4ever1937 -&gt;novostinn 4mysquad -&gt;crystal1johnson [7] jusslilstoner -&gt;4mysquad acejinev -&gt;4mysquad [9] adrgreerr -&gt;4mysquad bigboyjasiah -&gt;4mysquad [11] blacknewsoutlet-&gt;4mysquad camosaseko -&gt;4mysquad [13] cannonsher -&gt;4mysquad cliftonhughes_ -&gt;4mysquad [15] cornellburchet -&gt;4mysquad crystal1johnson-&gt;4mysquad + ... omitted several edges [[2]] IGRAPH b68efa8 DN-- 2 1 -- + attr: name (v/c), Type (e/c), Rel (e/c) + edge from b68efa8 (vertex names): [1] sto_christi-&gt;ingrkoch_koch We dont have any isolates in this example but if you dont want to consider isolates as components in other data sets, you can modify the command using the min.vertices = 2 argument. decompose(troll_g, mode = &quot;weak&quot;, # The minimum number of nodes a component should contain min.vertices = 2) [[1]] IGRAPH b6a1712 DN-- 1702 9824 -- + attr: name (v/c), Type (e/c), Rel (e/c) + edges from b6a1712 (vertex names): [1] 1d_nicole_ -&gt;ina_malone 1d_nicole_ -&gt;willie_bign [3] 4ever1937 -&gt;bydrbre_ 4ever1937 -&gt;novostidamask [5] 4ever1937 -&gt;novostinn 4mysquad -&gt;crystal1johnson [7] jusslilstoner -&gt;4mysquad acejinev -&gt;4mysquad [9] adrgreerr -&gt;4mysquad bigboyjasiah -&gt;4mysquad [11] blacknewsoutlet-&gt;4mysquad camosaseko -&gt;4mysquad [13] cannonsher -&gt;4mysquad cliftonhughes_ -&gt;4mysquad [15] cornellburchet -&gt;4mysquad crystal1johnson-&gt;4mysquad + ... omitted several edges [[2]] IGRAPH b6a1712 DN-- 2 1 -- + attr: name (v/c), Type (e/c), Rel (e/c) + edge from b6a1712 (vertex names): [1] sto_christi-&gt;ingrkoch_koch The decompose() function produces a list of subnetworks. We can see from the output that the largest weak component (the main component) is the first component listed. If we want to extract it, we can use the [[ accessor and tell R we want to extract the first item on the list. w_comp[[1]] IGRAPH b68efa8 DN-- 1702 9824 -- + attr: name (v/c), Type (e/c), Rel (e/c) + edges from b68efa8 (vertex names): [1] 1d_nicole_ -&gt;ina_malone 1d_nicole_ -&gt;willie_bign [3] 4ever1937 -&gt;bydrbre_ 4ever1937 -&gt;novostidamask [5] 4ever1937 -&gt;novostinn 4mysquad -&gt;crystal1johnson [7] jusslilstoner -&gt;4mysquad acejinev -&gt;4mysquad [9] adrgreerr -&gt;4mysquad bigboyjasiah -&gt;4mysquad [11] blacknewsoutlet-&gt;4mysquad camosaseko -&gt;4mysquad [13] cannonsher -&gt;4mysquad cliftonhughes_ -&gt;4mysquad [15] cornellburchet -&gt;4mysquad crystal1johnson-&gt;4mysquad + ... omitted several edges You could proceed to run your analysis or plot the main component. plot(w_comp[[1]], layout = layout_with_graphopt, main = &quot;Troll Network&quot;, sub = paste0(&quot;Network size: &quot;, vcount(w_comp[[1]])), vertex.color = &quot;lightblue&quot;, vertex.label = NA, vertex.size = 5, edge.arrow.size = 0.1) As mentioned above, we will use the clusters() function to identify our strongly connected components. s_comp &lt;- clusters(troll_g, mode = &quot;strong&quot;) The clusters() function returns a named list with three components. Using the names() function, we can see the names of the components on the list. membership is a numeric vector corresponding to the cluster id to which each vertex belongs csize is a numeric vector denoting the size of each cluster no is the number of clusters names(s_comp) [1] &quot;membership&quot; &quot;csize&quot; &quot;no&quot; Feel free to call each one (s_comp$size, s_comp$membership), but the code below only calls for the number of clusters (n=1314). s_comp$no [1] 1314 Here, the largest strongly connected component is not listed first. To identify and extract the largest strongly connected component, we can use the following lines of code. Notice the largest component consists of 168 actors. lg_s_comp &lt;- induced_subgraph(troll_g, vids = V(troll_g)[which( s_comp$membership == which.max(s_comp$csize))] ) lg_s_comp IGRAPH bdc091a DN-- 168 974 -- + attr: name (v/c), Type (e/c), Rel (e/c) + edges from bdc091a (vertex names): [1] 4mysquad -&gt;crystal1johnson acejinev -&gt;4mysquad [3] acejinev -&gt;blacknewsoutlet acejinev -&gt;tessyelmore [5] acejinev -&gt;wokefromday1 acejinev -&gt;blacktolive [7] acejinev -&gt;trayneshacole acejinev -&gt;randolphburrr [9] acejinev -&gt;jenn_abrams acejinev -&gt;ten_gop [11] blacknewsoutlet-&gt;4mysquad blacknewsoutlet-&gt;crystal1johnson [13] blacknewsoutlet-&gt;blacktolive blacknewsoutlet-&gt;nj_blacknews [15] cornellburchet -&gt;4mysquad cornellburchet -&gt;blacknewsoutlet + ... omitted several edges Lets plot the largest, strongly connected component. plot(lg_s_comp, layout = layout_with_graphopt, main = &quot;Troll Network (Main Strong Comp)&quot;, vertex.color = &quot;lightblue&quot;, vertex.label = NA, vertex.size = 5, edge.arrow.size = .1) 9.5 Cliques in igraph We can identify cliques in igraph with the following commands. First, we can ask for all cliques. The cliques() function returns a list containing all complete subgraphs in the input graph, which, is likely to be rather large on a network this size. Rather than print it to the console, store it into a new object and measure the length of the list to determine how many cliques are found in the graph. # Extract the largest weak component w_comp_main &lt;- w_comp[[1]] # Find all cliques w_comp_main_cliques &lt;- cliques(w_comp_main) # Measure the output length(w_comp_main_cliques) [1] 29305 Typically, cliques contain 3 or more nodes, which we can achieve by adding an additional argument (min = 3). # Find all cliques w_comp_main_cliques &lt;- cliques(w_comp_main, min = 3) # Measure the output length(w_comp_main_cliques) [1] 17955 In addition to the clique() function, igraph includes a function to find maximal cliques, which are those that cannot be extended to a larger clique (Csardi and Nepusz 2006). The function to identify maximal cliques is max_cliques(); however, rather than identifying the cliques lets focus on counting how many are present in the input graph. count_max_cliques(w_comp_main) [1] 7252 Like before, we can specify the lower limit on the size of the cliques to find. count_max_cliques(w_comp_main, min = 3) [1] 5276 The clique_num() function tells us the size of the largest clique(s), which is 8 accounts. clique_num(w_comp_main) [1] 8 Finally, identify how many cliques exist with 8 actors. How about 9? count_max_cliques(w_comp_main, min = 8) [1] 2 9.6 K-Cores in igraph In igraph we identify k-cores with the following code. We will examine the original network that we imported at the beginning of the lab (i.e., troll_g). The coreness() function calculates the maximal subgraph in which each vertex has at least degree k. The output is a numeric named vector giving the coreness of each vertex. However, there are 1,704 nodes in this graph, so the vector length would be 1,704. Rather than printing all values, lets return only the first five to examine the output. troll_kcore &lt;- coreness(troll_g) troll_kcore[1:5] 1d_nicole_ 4ever1937 4mysquad jusslilstoner acejinev 2 3 8 1 8 In case youre having trouble understanding the output, we can summarize the output by building a contingency table of the counts of each k level. The first line of the output below tells us the k-core and the second tells us how many actors make up that core (e.g., 156 accounts are members of the 1-core and 177 in the 12-core). table(troll_kcore) troll_kcore 1 2 3 4 5 6 7 8 9 10 11 12 156 145 146 173 158 153 167 173 149 55 52 177 Since the output of the k-core contains a value for each vertex, we could use this as a vertex level attribute and use it to subset the graph. First, lets assign the k-core values to the nodes. troll_g &lt;- set.vertex.attribute(troll_g, # Name the vertex attribute name = &quot;kcore&quot;, # The values for all vertices value = coreness(troll_g)) # Take a look at the graph with a new vertex attribute troll_g IGRAPH af724af DN-- 1704 9825 -- + attr: name (v/c), kcore (v/n), Type (e/c), Rel (e/c) + edges from af724af (vertex names): [1] 1d_nicole_ -&gt;ina_malone 1d_nicole_ -&gt;willie_bign [3] 4ever1937 -&gt;bydrbre_ 4ever1937 -&gt;novostidamask [5] 4ever1937 -&gt;novostinn 4mysquad -&gt;crystal1johnson [7] jusslilstoner -&gt;4mysquad acejinev -&gt;4mysquad [9] adrgreerr -&gt;4mysquad bigboyjasiah -&gt;4mysquad [11] blacknewsoutlet-&gt;4mysquad camosaseko -&gt;4mysquad [13] cannonsher -&gt;4mysquad cliftonhughes_ -&gt;4mysquad [15] cornellburchet -&gt;4mysquad crystal1johnson-&gt;4mysquad + ... omitted several edges Lets plot the graph depicting some of the larger k-cores. This can be accomplished using the induced_subgraph() function and specifying which vertices to keep. # Extract 9-core induced_subgraph(troll_g, # Create a logical vector of which vertices to keep using the # V() function and the $ accessor to set up a logical test vids = V(troll_g)$kcore == 9) IGRAPH c0fbc03 DN-- 149 412 -- + attr: name (v/c), kcore (v/n), Type (e/c), Rel (e/c) + edges from c0fbc03 (vertex names): [1] _nickluna_ -&gt;andyhashtagger _nickluna_ -&gt;danageezus [3] _nickluna_ -&gt;xdwillie _nickluna_ -&gt;thefoundingson [5] _nickluna_ -&gt;pamela_moore13 ameliebaldwin-&gt;_nickluna_ [7] ameliebaldwin-&gt;andyhashtagger ameliebaldwin-&gt;danageezus [9] ameliebaldwin-&gt;pati_cooper ameliebaldwin-&gt;tpartynews [11] ameliebaldwin-&gt;matevidence ameliebaldwin-&gt;usa_gunslinger [13] ameliebaldwin-&gt;pigeontoday cassishere -&gt;_nickluna_ [15] cassishere -&gt;andyhashtagger cassishere -&gt;evewebster373 + ... omitted several edges Using this form, we can create 9, 10, 11 and 12 core subgraphs and plot them side-by-side. par(mfrow = c(2, 2)) # Save the coordinates coords &lt;- layout_with_graphopt(troll_g) # Create a 9-core graph induced_subgraph(troll_g, vids = V(troll_g)$kcore == 9) %&gt;% # &#39;and then&#39; plot it plot(main = &quot;9-Core&quot;, sub = paste0(&quot;Number of nodes: &quot;, vcount(.)), layout = coords, vertex.label = NA, vertex.size = 5, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.1) # Create a 10-core graph induced_subgraph(troll_g, vids = V(troll_g)$kcore == 10) %&gt;% # &#39;and then&#39; plot it plot(main = &quot;10-Core&quot;, sub = paste0(&quot;Number of nodes: &quot;, vcount(.)), layout = coords, vertex.label = NA, vertex.size = 5, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.1) # Create a 11-core graph induced_subgraph(troll_g, vids = V(troll_g)$kcore == 11) %&gt;% # &#39;and then&#39; plot it plot(main = &quot;11-Core&quot;, sub = paste0(&quot;Number of nodes: &quot;, vcount(.)), layout = coords, vertex.label = NA, vertex.size = 5, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.1) # Create a 12-core graph induced_subgraph(troll_g, vids = V(troll_g)$kcore == 12) %&gt;% # &#39;and then&#39; plot it plot(main = &quot;12-Core&quot;, sub = paste0(&quot;Number of nodes: &quot;, vcount(.)), layout = coords, vertex.label = NA, vertex.size = 5, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.1) # Turn off the graphic devices dev.off() null device 1 The previous graphs are a little misleading, however, because the 12-core is part of the 11-core, and the 12- and 11-cores are part of the 10-core, and so on. In other words, the 9-core wont have isolates if the 10, 11, and 12 cores are included. induced_subgraph(troll_g, troll_kcore &gt;= 9) %&gt;% plot(main = &quot;9-12 Core&quot;, sub = paste0(&quot;Number of nodes: &quot;, vcount(.)), layout = coords, vertex.label = NA, vertex.size = 5, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.1) One last thing, working with directed and/or valued data will provide different results than working with symmetric and dichotomous data. For instance, UCINET (2002) defines a k-core as: A k-core in an undirected graph is a connected maximal induced subgraph which has minimum degree greater than or equal to k. For a valued graph we require the sum of all the edges incident with a vertex is greater than k. Please keep that in mind as you move forward. 9.7 Community Detection in igraph Well use the largest, strongly connected component for this section but we will symmetrize it for demonstration purposes and because some of these algorithms (e.g., Girvan-Newman) work for undirected networks only. lg_s_comp_simp &lt;- lg_s_comp %&gt;% as.undirected() lg_s_comp_simp IGRAPH c7f9f1b UN-- 168 922 -- + attr: name (v/c) + edges from c7f9f1b (vertex names): [1] 4mysquad --acejinev 4mysquad --blacknewsoutlet [3] acejinev --blacknewsoutlet 4mysquad --cornellburchet [5] blacknewsoutlet--cornellburchet 4mysquad --crystal1johnson [7] blacknewsoutlet--crystal1johnson 4mysquad --errivvvvers [9] blacknewsoutlet--errivvvvers 4mysquad --jaedenmassey [11] 4mysquad --kaydenmelton crystal1johnson--kaydenmelton [13] 4mysquad --ramonasnails cornellburchet --ramonasnails [15] 4mysquad --sadieeatonn 4mysquad --sincerepruitt + ... omitted several edges 9.7.1 Girvan-Newman Now, lets look at a few community detection algorithms. Well begin with Girvan-Newman, which in igraph is called edge betweenness. To do so, use the cluster_edge_betweenness() function, which returns a communities object. In order to take a closer look at the communities object, we will store the output in a new object to further analyze it. This first line of code will take a few minutes, so please be patient and save your work before running it. gn &lt;- cluster_edge_betweenness(lg_s_comp_simp) # Take a look at the object gn IGRAPH clustering edge betweenness, groups: 5, mod: 0.54 + groups: $`1` [1] &quot;4mysquad&quot; &quot;acejinev&quot; &quot;blacknewsoutlet&quot; &quot;cornellburchet&quot; [5] &quot;crystal1johnson&quot; &quot;errivvvvers&quot; &quot;jaedenmassey&quot; &quot;kaydenmelton&quot; [9] &quot;ramonasnails&quot; &quot;sadieeatonn&quot; &quot;sincerepruitt&quot; &quot;willisbonnerr&quot; [13] &quot;bricegeller&quot; &quot;lagonehoe&quot; &quot;adamchapmanjr&quot; &quot;imapharrelfake&quot; [17] &quot;tessyelmore&quot; &quot;youjustctrlc&quot; &quot;baltimore0nline&quot; &quot;bleepthepolice&quot; [21] &quot;nojonathonno&quot; &quot;baobaeham&quot; &quot;bigboysneed&quot; &quot;missourinewsus&quot; [25] &quot;blackmattersus&quot; &quot;chadsloyer&quot; &quot;drmichaelgarcia&quot; &quot;ilovesarahrich&quot; [29] &quot;mrclydepratt&quot; &quot;peytoncashout&quot; &quot;gwennythot&quot; &quot;jani_s_jac&quot; [33] &quot;javonhidp&quot; &quot;pamblmdaniels&quot; &quot;robertebonyking&quot; &quot;siccerthanyou&quot; + ... omitted several groups/vertices As in the previous section, the names() function tells us the various types of information that igraph calculated. names(gn) [1] &quot;removed.edges&quot; &quot;edge.betweenness&quot; &quot;merges&quot; &quot;bridges&quot; [5] &quot;modularity&quot; &quot;membership&quot; &quot;names&quot; &quot;vcount&quot; [9] &quot;algorithm&quot; We can access this information using igraph functions. For example, lets take a look at the modularity score and the number of groups the algorithm identified. # Print modularity score modularity(gn) [1] 0.5350271 # Print the number of identified communities length(gn) [1] 5 This information can help us to calculate normalized modularity as well. Why are we interested in normalized modularity? Because there is an upper limit to the modularity score, which is a function of how many communities (subgroups) the algorithm identifies. With a large number of groups (communities), the normalized score differs little from the regular score. However, with a small number of groups, it can vary considerably. The following commands demonstrates how to calculate normalized modularity. # Calculate normalized modularity (qprime) by dividing the modularity by the # product of 1 minus the inverse of the number of communities. qprime_gn &lt;- modularity(gn)/(1 - (1/length(gn))) # Take a look qprime_gn [1] 0.6687839 The communities object generated by the cluster_edge_betweenness() command also generates a vector that identifies which communities each actor belongs to. Like before, we can extract this vector with a function (here membership()), assign it as an attribute to the vertices of the graph, and use these to color the nodes. # Add membership attribute to graph lg_s_comp_simp &lt;- set.vertex.attribute(lg_s_comp_simp, name = &quot;gn_membership&quot;, value = membership(gn)) # Store coordinates to use later coords &lt;- layout_with_fr(lg_s_comp_simp) # Plot plot(lg_s_comp_simp, layout = coords, main = &quot;Girvan-Newman Communities&quot;, vertex.label = NA, vertex.size = 5, vertex.color = get.vertex.attribute(lg_s_comp_simp, &quot;gn_membership&quot;) ) We can place convex hulls around the various communities by placing the gn object at the beginning of the plot command. plot(gn, lg_s_comp_simp, layout = coords, main = &quot;Girvan-Newman Communities (w/Convex Hulls)&quot;, vertex.label = NA, vertex.size = 5, vertex.color = get.vertex.attribute(lg_s_comp_simp, &quot;gn_membership&quot;) ) We can do so with and without coloring the individual nodes. In fact, if you compare the two graphs, youll see that they are almost identical, so telling igraph what color to color the nodes is unnecessary; igraph chooses its own colors based on the convex hulls. plot(gn, lg_s_comp_simp, layout = coords, main = &quot;Girvan-Newman Communities (w/Convex Hulls) #2&quot;, vertex.label = NA, vertex.size = 5) 9.7.2 Walktrap There are numerous other community detection algorithms included in igraph, such as Walktrap, Spin Glass, and Louvain. We briefly compare and contrast them below. Lets begin with Walktrap, which tries to identify subgraphs via random walks. The ideas is that short random walks tend to stay in the same community. wt &lt;- cluster_walktrap(lg_s_comp_simp) Calculate modularity, normalized modularity, and identify the number of communities. data.frame( &quot;modularity&quot; = modularity(wt), &quot;normalized modularity&quot; = modularity(wt)/(1 - (1/length(wt))), &quot;communities&quot; = length(wt) ) modularity normalized.modularity communities 1 0.5365493 0.804824 3 Now plot it. plot(wt, lg_s_comp_simp, layout = coords, main = &quot;Walktrap (w/ Convex Hulls)&quot;, vertex.label = NA, vertex.size = 5) 9.7.3 Spin-Glass Now take a look at the spin-glass community detection algorithm. This function tries to identify communities via a spin-glass model and simulated annealing (Csardi and Nepusz 2006). First, create the communities object. sg &lt;- cluster_spinglass(lg_s_comp_simp) Calculate modularity, normalized modularity, and identify the number of communities. data.frame( &quot;modularity&quot; = modularity(sg), &quot;normalized modularity&quot; = modularity(sg)/(1 - (1/length(sg))), &quot;communities&quot; = length(sg) ) modularity normalized.modularity communities 1 0.537671 0.6720887 5 Now plot it. plot(sg, lg_s_comp_simp, layout = coords, main = &quot;Spin-Glass (w/ Convex Hulls)&quot;, vertex.label = NA, vertex.size = 5) 9.7.4 Louvain And finally, Louvain, which implements the multi-level modularity optimization algorithm. lo &lt;- cluster_louvain(lg_s_comp_simp) Calculate modularity, normalized modularity, and identify the number of communities. data.frame( &quot;modularity&quot; = modularity(lo), &quot;normalized modularity&quot; = modularity(lo)/(1 - (1/length(lo))), &quot;communities&quot; = length(lo) ) modularity normalized.modularity communities 1 0.5385144 0.8077716 3 Now plot it. plot(lo, lg_s_comp_simp, layout = coords, main = &quot;Louvain (w/ Convex Hulls)&quot;, vertex.label = NA, vertex.size = 5) 9.7.5 Table of Results Lets compare community detection algorithms by putting them all in a table. # Create the data.frame community &lt;- data.frame(&quot;Algorithm&quot; = c(&quot;Girvan-Newman&quot;, &quot;Walktrap&quot;, &quot;Spin Glass&quot;, &quot;Louvain&quot;), &quot;Number of Groups&quot; = c(length(gn), length(wt), length(sg), length(lo)), &quot;Modularity&quot; = c(modularity(gn), modularity(wt), modularity(sg), modularity(lo)), &quot;Normalized Modularity&quot; = c( modularity(gn)/(1 - (1/length(gn))), modularity(wt)/(1 - (1/length(wt))), modularity(sg)/(1 - (1/length(sg))), modularity(lo)/(1 - (1/length(lo))) )) community Algorithm Number.of.Groups Modularity Normalized.Modularity 1 Girvan-Newman 5 0.5350271 0.6687839 2 Walktrap 3 0.5365493 0.8048240 3 Spin Glass 5 0.5376710 0.6720887 4 Louvain 3 0.5385144 0.8077716 If we were to base our analysis strictly on normalized modularity where a higher score is generally understood to indicate a better fit, then wed go with the Louvain algorithms community detection results (or Walktraps). Thats all for igraph now. References "],["detecting-subgroups-in-statnet.html", "10 Detecting Subgroups in statnet 10.1 Setup 10.2 Load Libraries 10.3 Import and Plot Network 10.4 Components in statnet 10.5 Cliques in statnet 10.6 K-Cores in statnet 10.7 Community Detection in statnet", " 10 Detecting Subgroups in statnet 10.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ################################################# # What: Detecting Subgroups in R with statnet # Created: 02.28.14 # Revised: 01.24.22 ################################################# If you have not set up your RStudio Project to clear the workspace on exit, your environment may contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (Troll_EL.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. Social scientists generally assume that social interaction is the basis for solidarity, shared norms, identity, and collective behavior, so people who interact intensively are likely to consider themselves a social group. Thus, a major focus of social network analysis is to identify dense clusters of actors among whom there are relatively strong, direct, intense, and/or positive ties. These are typically referred to as cohesive subgroups, subnetworks, or sub-clusters. One way to cluster actors is based on shared attributes (e.g., race, gender, etc.). Another is to use the pattern of ties among actors. That is what this lab demonstrates. In an ideal world, there would be a single algorithm for identifying cohesive subgroups, but this is not an ideal world, so social network analysts have developed a variety of algorithms for identifying subnetworks. For this exercise, well use a portion of the Twitter Russian Troll network that FiveThirtyEight released in mid-2018.2 This subset is comprised of retweet ties among approximately 1,700 Twitter handles connected to the Internet Research Agency (a Russian troll factory) that were active from at least May 2015 and up through the 2016 election. From the original data set, weve created an edge list for this subset that contains directed ties among the accounts. 10.2 Load Libraries Load the statnet library. library(statnet) 10.3 Import and Plot Network Lets import the data, which weve stored as an edge list, using the read.csv() function. troll_el &lt;- read.csv(&quot;data/Troll_EL.csv&quot;, header = TRUE) # Take a look at the top 5 rows: head(troll_el) Source Target Type Rel 1 1d_nicole_ ina_malone Directed Retweet 2 1d_nicole_ willie_bign Directed Retweet 3 4ever1937 bydrbre_ Directed Retweet 4 4ever1937 novostidamask Directed Retweet 5 4ever1937 novostinn Directed Retweet 6 4mysquad crystal1johnson Directed Retweet Next, convert the edge list to a network object using the as.network() function. Note we use matrix.type argument to identify the structure of the data as \"edgelist and directed = TRUE to indicate that our data are directed. troll_net &lt;- as.network(troll_el, directed = TRUE, loops = FALSE, matrix.type = &quot;edgelist&quot;) # Take a look at the object troll_net Network attributes: vertices = 1704 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 9825 missing edges= 0 non-missing edges= 9825 Vertex attribute names: vertex.names Edge attribute names not shown Lets plot the network to see what it looks like. Since this is a larger network, it will take a little longer to plot the network. Our recommendation is to be patient and avoid clicking on anything until the plot appears. gplot(troll_net, gmode = &quot;graph&quot;, main = &quot;Troll Network&quot;, jitter = TRUE, usearrows = FALSE, edge.col = &quot;gray&quot;) 10.4 Components in statnet It is fairly straightforward to identify strong and weak components in statnet using the components() function. Our data are directed, so it makes sense to identify both types of components. You can use the is.directed() function to double-check if you have a directed network or not (the answer is TRUE in this case). is.directed(troll_net) [1] TRUE Knowing the size of the network will be useful as we start looking at the distribution of nodes across various subgroups we detect throughout this lab (n = 1,704). network.size(troll_net) [1] 1704 Now, well go ahead and identify strong components first. s_comp &lt;- components(troll_net, connected = &quot;strong&quot;, comp.dist.precomp = NULL) You should see this network contains 1,314 strong components. s_comp [1] 1314 Next, identify weak components. w_comp &lt;- components(troll_net, connected = &quot;weak&quot;, comp.dist.precomp = NULL) Your results should indicate there are 11 weak components. w_comp [1] 2 We can determine the distribution of both strong and weak components using the following commands. The first, component.dist(), calculates the distribution of component sizes. This command returns a named list containing three items: component membership ($membership), component size ($csize), and a vector that contains the un-normalized distribution function of component sizes ($cdist). s_comp_d &lt;- component.dist(troll_net, connected = &quot;strong&quot;) # Inspect the first five entries in the csize vector s_comp_d$csize[1:5] [1] 1 1 168 1 1 We are interested in component size here. To find it, use the table() function to tabulate the results. In terms of strong components, the results indicate that there are 1,307 strong components the size of 1, 2 strong components containing 2 nodes, 1 comprised of 3 nodes, 1 made up of 22 nodes, 1 consisting of 35 actors, 1 with 165 nodes, and a main component made up of 168 nodes (adds up to 1,704 nodes). table(s_comp_d$csize) 1 2 3 22 35 165 168 1307 2 1 1 1 1 1 Next, do the same for weak components. Once you use the table() function, you should see a large weak component of 1,702 accounts and another comprised of 2 accounts (again, that 1,704 accounts, which is what we got when we ran network size earlier). w_comp_d &lt;- component.dist(troll_net, connected = &quot;weak&quot;) table(w_comp_d$csize) 2 1702 1 1 Sometimes it can be helpful to extract the largest component (also known as the main component), such as when working with very large networks or calculating the traditional measure of closeness centrality (Freeman), which doesnt handle infinite distances. Heres how to extract the largest (main) component for both strong and weak components, which we will convert to a network object and then visualize (again, be patientit may take a second or two to plot). Lets begin with strong components. To extract it, you will use the component.largest() function, which by default returns a matrix (though the output could be an edge list return.as.edgelist = TRUE). troll_net_strongmain &lt;- component.largest(troll_net, connected = &quot;strong&quot;, result = &quot;graph&quot;) class(troll_net_strongmain) [1] &quot;matrix&quot; &quot;array&quot; You could graph the matrix or you could transform it to a network object. We will do the latter. # Transform to network class object troll_net_strongmain &lt;- as.network(troll_net_strongmain, directed = TRUE) # Plot it gplot(troll_net_strongmain, gmode = &quot;digraph&quot;, main = &quot;Main Strong Component&quot;, jitter = TRUE, usearrows = TRUE, edge.col = &quot;gray&quot;) Now, lets do the same for weak components (though its not super different from our initial network). # Extract weak largest component troll_net_weakmain &lt;- component.largest(troll_net, connected = &quot;weak&quot;, result = &quot;graph&quot;) # Transform to network class object troll_net_weakmain &lt;- as.network(troll_net_weakmain, directed = TRUE) # Plot it gplot(troll_net_weakmain, gmode = &quot;digraph&quot;, main = &quot;Main Weak Component&quot;, jitter = TRUE, usearrows = TRUE, edge.col = &quot;gray&quot;) 10.5 Cliques in statnet Lets look at cliques in statnet, which is useful for observing shared cliques among actors. For this section well just focus on the main weak component (i.e., troll_net_weakmain). The clique.census() function computes clique census statistics for a given graph. Adding the parameter to save a co-membership matrix (clique.comembership = \"sum\"), we sum all the times that two nodes appear in the same clique (Butts 2020). weakmain_clique &lt;- clique.census(troll_net_weakmain, # Using &#39;graph&#39; for undirected graph mode = &quot;graph&quot;, clique.comembership = &quot;sum&quot;) The output of the clique.census() function is a named list with three elements. A vector of aggregate counts by clique size ($clique.count), a matrix or array containing co-membership in cliques ($clique.comemb), and a list of lists containing cliques of a corresponding size ($cliques). Here we can extract the $clique.comemb element and transform it into a network object. weakmain_comemb &lt;- as.network(weakmain_clique$clique.comemb) # Take a look at the object weakmain_comemb Network attributes: vertices = 1702 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 456 missing edges= 0 non-missing edges= 456 Vertex attribute names: vertex.names No edge attributes The ties among the accounts in the following visualization represent shared cliques. We will not pay attention to weights here (i.e., how many shared cliques two nodes share). gplot(weakmain_comemb, gmode = &quot;onemode&quot;, usearrows = FALSE, main = &quot;Clique Co-Membership&quot;, displayisolates = FALSE) statnet calculates the number cliques of a particular size in which each actor is embedded as well. Here, we reshape it as a data.frame, which gives us a little flexibility down the line. For instance, the account acejinev is involved in two cliques the size of two and one clique of size one. clique_count &lt;- data.frame( &quot;account&quot; = names(weakmain_clique$clique.count[1, ]), &quot;one&quot; = weakmain_clique$clique.count[1, ], &quot;two&quot; = weakmain_clique$clique.count[2, ], &quot;three&quot; = weakmain_clique$clique.count[3, ], row.names = NULL # Remove the aggregate row )[-1, ] # Plot first five rows clique_count[1:5, ] account one two three 2 1d_nicole_ 1 0 0 3 4ever1937 1 0 0 4 4mysquad 0 1 0 5 jusslilstoner 1 0 0 6 acejinev 1 2 0 10.6 K-Cores in statnet Now, lets identify the k-cores in the original troll network (i.e., troll_net). The output is a numeric named vector which includes the max core for each vertex. troll_cores &lt;- kcores(troll_net) troll_cores[1:5] 1d_nicole_ 4ever1937 4mysquad jusslilstoner acejinev 2 3 8 1 8 In case youre having trouble understanding the output, we can summarize the output by building a contingency table of the counts of each k level. The first line of the output below tells us the k-core and the second tells us how many actors make up that core (e.g., 156 accounts are members of the 1-core and 177 in the 12-core). table(troll_cores) troll_cores 1 2 3 4 5 6 7 8 9 10 11 12 156 145 146 173 158 153 167 173 149 55 52 177 Since the output of the k-core contains a value for each vertex, we could use this as a vertex level attribute and use it to subset the graph. First, lets assing the k-core values to the nodes. troll_net &lt;- set.vertex.attribute(troll_net, attrname = &quot;kcore&quot;, value = kcores(troll_net)) # Take a look at the graph with a new vertex attribute troll_net Network attributes: vertices = 1704 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 9825 missing edges= 0 non-missing edges= 9825 Vertex attribute names: kcore vertex.names Edge attribute names not shown Now plot it and highlight k-core membership. gplot(troll_net, gmode = &quot;graph&quot;, main = &quot;K-Cores&quot;, jitter = TRUE, vertex.cex = 2, vertex.col = get.vertex.attribute(troll_net, &quot;kcore&quot;), usearrows = FALSE, edge.col = &quot;gray&quot;) Lets plot the graph depicting some of the larger k-cores. This can be accomplished using the get.inducedSubgraph() function and specifying which vertices to keep. get.inducedSubgraph(troll_net, # Provide a vector of which vertex IDs to keep v = which(troll_net %v% &#39;kcore&#39; == 9)) Network attributes: vertices = 149 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 412 missing edges= 0 non-missing edges= 412 Vertex attribute names: kcore vertex.names Edge attribute names not shown Note that the number of vertices above matches the coreness table previously calculated. Using this form, we can create 9, 10, 11 and 12 core subgraphs and plot them side-by-side. par(mfrow = c(2, 2)) # Create a 9-core graph gplot( get.inducedSubgraph(troll_net, v = which(troll_net %v% &#39;kcore&#39; == 9)), displaylabels = FALSE, main = &quot;9-Core&quot; ) # Create a 10-core graph gplot( get.inducedSubgraph(troll_net, v = which(troll_net %v% &#39;kcore&#39; == 10)), displaylabels = FALSE, main = &quot;10-Core&quot; ) # Create a 11-core graph gplot( get.inducedSubgraph(troll_net, v = which(troll_net %v% &#39;kcore&#39; == 11)), displaylabels = FALSE, main = &quot;11-Core&quot; ) # Create a 12-core graph gplot( get.inducedSubgraph(troll_net, v = which(troll_net %v% &#39;kcore&#39; == 12)), displaylabels = FALSE, main = &quot;12-Core&quot; ) The previous graphs are a little misleading, however, because the 12-core is part of the 11-core, and the 12- and 11-cores are part of the 10-core, and so on. In other words, the 9-core wont have isolates if the 10, 11, and 12 cores are included. gplot(get.inducedSubgraph(troll_net, v = which(troll_net %v% &#39;kcore&#39; &gt;= 12)), displaylabels = FALSE, main = &quot;9-12 Core&quot;) One last thing, working with directed and/or valued data will provide different results than working with symmetric and dichotomous data. For instance, UCINET (2002) defines a k-core as: A k-core in an undirected graph is a connected maximal induced subgraph which has minimum degree greater than or equal to k. For a valued graph we require the sum of all the edges incident with a vertex is greater than k. Please keep that in mind as you move forward. 10.7 Community Detection in statnet Psych!!!!! statnet does not offer any community detection algorithms at this point. You will need to turn to igraph to utilize them. Thats all for statnet for now. References "],["centrality-and-brokerage-in-igraph.html", "11 Centrality and Brokerage in igraph 11.1 Setup 11.2 Load Libraries 11.3 Load Data 11.4 Centrality and Power (Undirected Networks) 11.5 Centrality and Prestige (Directed Networks) 11.6 Brokerage", " 11 Centrality and Brokerage in igraph 11.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ################################################# # What: Centrality and Brokerage in igraph # Created: 02.28.14 # Revised: 01.31.22 ################################################# If you have not set up your RStudio Project to clear the workspace on exit, your environment may contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (SouthFront_EL.csv, SouthFront_NL.csv, Strike.net, and Strikegroups.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. In this lab we will consider a handful of actor-level measures. Specifically, we will walk through the concepts of centrality and brokerage on two different networks. Centrality is one of SNAs oldest concepts. When working with undirected data, a central actor can be someone who has numerous ties to other actors (degree), someone who is closer (in terms of path distance) to all other actors (closeness), someone who lies on the shortest path (geodesic) between any two actors (betweenness), or someone who has ties to other highly central actors (eigenvector). In some networks, the same actors will score high on all four measures. In others, they wont. There are, of course, more than four measures of centrality. For the centrality portion of this exercise, well look at a subset of South Fronts YouTube network that weve collected using YouTubes open application programming interface. Specifically, we will examine subscription-based ties among accounts (note the names are a string of what appears to be random combinations of letters and numbers) within South Fronts ego network (excluding South Front), which leaves us with a network of 310 subscriptions among 236 accounts. We will consider this network undirected for the Centrality and Power section, but directed for the Centrality and Prestige portion of this lab. Next, we will turn to measures that operationalize various aspects of brokerage. For that section, we will demonstrate the concept of brokerage by looking at a communication network of a wood-processing facility where workers rejected a new compensation package and eventually went on strike. Management then brought in an outside consultant to analyze the employees communication structure because it felt that information about the package was not being effectively communicated to all employees by the union negotiators. The outside consultant asked all employees to indicate, on a 5-point scale, the frequency that they discussed the strike with each of their colleagues, ranging from almost never (less than once per week) to very often (several times per day). The consultant used 3 as a cut-off value in order to identify a tie between two employees. If at least one of two persons indicated they discussed work with a frequency of three or more, a tie between them was included in the network. The data accompany the book, Exploratory Social Network Analysis with Pajek, also published by Cambridge. Hence, weve shared the data with you as a Pajek file. 11.2 Load Libraries Load the igraph library. library(igraph) Note: igraph imports the %&gt;% operator on load (library(igraph)). This series of exercises leverages the operator because we find it very useful in chaining functions. We will also be using other libraries in this exercise such as CINNA, DT, keyplayer, psych, scales, and influenceR. This might be the first time you use these, so you may need to install them. to_install &lt;- c(&quot;CINNA&quot;, &quot;DT&quot;, &quot;influenceR&quot;, &quot;keyplayer&quot;, &quot;psych&quot;, &quot;scales&quot;) install.packages(to_install) If you have installed these, proceed to load CINNA, keyplayer, and psych. We will namespace functions from influenceR, DT, and scales libraries (e.g., influcenceR::betweenness()) as these have functions that mask others from igraph. library(CINNA) library(keyplayer) library(psych) 11.3 Load Data Weve stored South Fronts YouTube network as an edge list. Go ahead and import it with the read.csv() function to read the data. Then transform the data.frame to an igraph object using the graph_from_data_frame() function. For now we will import it as an undirected network by setting the directed argument to FALSE. # Read data sf_el &lt;- read.csv(&quot;data/SouthFront_EL.csv&quot;, header = TRUE) # Create graph with edge list sf_g &lt;- graph_from_data_frame(d = sf_el, directed = FALSE) # Take a look at it sf_g IGRAPH 826faa6 UN-- 236 310 -- + attr: name (v/c), Type (e/c), Id (e/n), Label (e/l), timeset (e/l), | Weight (e/n) + edges from 826faa6 (vertex names): [1] UCYE61Gy3RxiI2hSdCmAgP9w--UClvD6c1VI75QZWJjA_yiWhg [2] UCFpuO2wt_3WSrk-QG7VjUhQ--UC2C_jShtL725hvbm1arSV9w [3] UCqxZhJewxqhB4cNsxJFjIhg--UClvD6c1VI75QZWJjA_yiWhg [4] UCWNbidLi4FXBd83ixoB1v-A--UClvD6c1VI75QZWJjA_yiWhg [5] UCShSHheWVd42CdiVAYn-9xQ--UCLoNQH9RCndfUGOb2f7E1Ew [6] UCNMbegBD9OjH4Eza8vVjBMg--UCFWjEwhX6cSAKBQ28pufG3w [7] UC8zZkogm0hU7_zifdxSQa0Q--UCK09g6gYGMvU-0x1VCF1hgA + ... omitted several edges You may want to plot it. plot(sf_g, main = &quot;South Front&quot;, sub = paste0(&quot;Vertices: &quot;, vcount(sf_g), &quot; | Edges: &quot;, ecount(sf_g)), layout = layout_with_kk, vertex.label = NA, vertex.color = &quot;lightblue&quot;, vertex.size = 10, edge.arrow.mode = 0) Next, load the Stike.net file and Strikegroups.csv, convert the relational data to an igraph object and add the node attributes to this graph. # Read graph strike_g &lt;- read_graph(&quot;data/Strike.net&quot;, format = &quot;pajek&quot;) # Read attributes strike_attrs &lt;- read.csv(&quot;data/Strikegroups.csv&quot;, col.names = c(&quot;Name&quot;, &quot;Group&quot;)) # Add vertex attributes strike_g &lt;- set_vertex_attr(strike_g, name = &quot;Group&quot;, value = strike_attrs[[&quot;Group&quot;]]) Lastly, plot the new network. plot(strike_g, layout = layout_with_kk, main = &quot;Strike Network&quot;, sub = paste0(&quot;Vertices: &quot;, vcount(strike_g), &quot; | Edges: &quot;, ecount(strike_g)), vertex.size = 10, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.color = get.vertex.attribute(strike_g, &quot;Group&quot;), edge.arrow.mode = 0) 11.4 Centrality and Power (Undirected Networks) 11.4.1 Degree, Closeness, Betweenness, and Eigenvector We will begin by calculating the four primary measures of centrality for undirected networks. Note that the eigenvector function (i.e., evcent()) returns a named list with three components, so we have to extract the vector including the scores ($vector). Note the centrality scores correlate 100% with their statnet counterparts (as they should). We can calculate ARD/Harmonic closeness using CINNAs harmonic_centrality() function. It generates raw scores, so if we want to normalize it, we need to divide by the number of nodes in the network less one. # Add centrality metrics as vertex attributes sf_g &lt;- sf_g %&gt;% # igraph centrality measures set.vertex.attribute(name = &quot;degree&quot;, value = degree(sf_g)) %&gt;% set.vertex.attribute(name = &quot;closeness&quot;, value = closeness(sf_g)) %&gt;% set.vertex.attribute(name = &quot;betweenness&quot;, value = betweenness(sf_g)) %&gt;% set.vertex.attribute(name = &quot;eigenvector&quot;, value = evcent(sf_g)$vector) %&gt;% # CINNA ARD/Harmonic centrality set.vertex.attribute(name = &quot;ard&quot;, value = harmonic_centrality(sf_g)/(vcount(sf_g) - 1)) # If you are not familiar with the %&gt;%, you do not have to use it. # The base R equivalent is: # sf_g &lt;- set.vertex.attribute(sf_g, name = &quot;degree&quot;, value = degree(sf_g)) # ... We can get back the node attributes using the get.data.frame() function and setting the what argument to \"vertices\". Take a look at the first five rows of executing this command. head(get.data.frame(sf_g, what = &quot;vertices&quot;), n = 5) name degree closeness UCYE61Gy3RxiI2hSdCmAgP9w UCYE61Gy3RxiI2hSdCmAgP9w 1 0.001256281 UCFpuO2wt_3WSrk-QG7VjUhQ UCFpuO2wt_3WSrk-QG7VjUhQ 1 0.001011122 UCqxZhJewxqhB4cNsxJFjIhg UCqxZhJewxqhB4cNsxJFjIhg 1 0.001256281 UCWNbidLi4FXBd83ixoB1v-A UCWNbidLi4FXBd83ixoB1v-A 1 0.001256281 UCShSHheWVd42CdiVAYn-9xQ UCShSHheWVd42CdiVAYn-9xQ 1 0.001070664 betweenness eigenvector ard UCYE61Gy3RxiI2hSdCmAgP9w 0 0.116170167 0.3251773 UCFpuO2wt_3WSrk-QG7VjUhQ 0 0.004754002 0.2522695 UCqxZhJewxqhB4cNsxJFjIhg 0 0.116170167 0.3251773 UCWNbidLi4FXBd83ixoB1v-A 0 0.116170167 0.3251773 UCShSHheWVd42CdiVAYn-9xQ 0 0.011670789 0.2702837 # If you wanted to write this data.frame as a CSV, you could do so: # write.csv(x = get.data.frame(sf_g, what = &quot;vertices&quot;), # row.names = FALSE) Lets plot the network where we vary node size by the centrality measures; note that weve rescaled them so that the nodes dont get overwhelmingly big or way too small. Weve turned off the labels, which are YouTube Channel IDs (i.e., really long), so you can see the results clearly. par(mfrow = c(2, 3)) # Save the coordinates coords &lt;- layout_with_kk(sf_g) # Plot graph with rescaled nodes plot(sf_g, asp = 0, main = &quot;South Front (Degree)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_g, name = &quot;degree&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;) plot(sf_g, main = &quot;South Front (Closeness)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_g, name = &quot;closeness&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;) plot(sf_g, main = &quot;South Front (ARD Closeness)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_g, name = &quot;ard&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;) plot(sf_g, main = &quot;South Front (Betweenness)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_g, name = &quot;betweenness&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;) plot(sf_g, main = &quot;South Front (Eigenvector)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_g, name = &quot;eigenvector&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;) 11.4.2 Correlations To run a correlation between variables, use the cor() function. #Run correlations for columns containing centrality scores, which is all except # the first column. cor(get.data.frame(sf_g, what = &quot;vertices&quot;)[, -1]) degree closeness betweenness eigenvector ard degree 1.0000000 0.4957346 0.9861270 0.7559173 0.5838876 closeness 0.4957346 1.0000000 0.5346211 0.8023483 0.9908909 betweenness 0.9861270 0.5346211 1.0000000 0.7944660 0.6192361 eigenvector 0.7559173 0.8023483 0.7944660 1.0000000 0.8421519 ard 0.5838876 0.9908909 0.6192361 0.8421519 1.0000000 Note that, for the most part, the centrality measures correlate highly with degree, especially betweenness. Heres a really nice function for visualizing correlation (i.e., pairs.panels()) associated with the psych package. pairs.panels(get.data.frame(sf_g, what = &quot;vertices&quot;)[, -1]) 11.4.3 Interactive Table The R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as HTML table widgets. The interactive widgets provide filtering, pagination, sorting, and many other features for the tables. We will namespace the datatable() function from library and provide it the node table for the sf_g graph. DT::datatable(get.data.frame(sf_g, what = &quot;vertices&quot;), rownames = FALSE) Using the magrittr pipe (%&gt;%) we can reshape the grammar a bit. get.data.frame(sf_g, what = &quot;vertices&quot;) %&gt;% DT::datatable(rownames = FALSE) Lets extract the data.frame and modify the numeric variables, rounding them to 3 decimal places. centralities &lt;- get.data.frame(sf_g, what = &quot;vertices&quot;) # Round up numeric values centralities &lt;- as.data.frame( sapply(names(centralities), function(s) { centralities[[s]] &lt;- ifelse(is.numeric(centralities[[s]]), yes = round(centralities[s], digits = 3), no = centralities[s]) }) ) Take a look at the table: centralities %&gt;% DT::datatable(rownames = FALSE) You may want to clean up this table. Begin by looking at the datatable arguments by reading the documentation ?DT::datatable. Here we clean the data in base R, then modify the HTML widget parameters. # Order the data.frame by decreasing degree value centralities[order(centralities$degree, decreasing = TRUE), ] %&gt;% # Change column names for the data.frame `colnames&lt;-`(c(&quot;Channel&quot;, &quot;Degree&quot;, &quot;Closeness&quot;, &quot;Betweenness&quot;, &quot;Eigenvector&quot;, &quot;ARD&quot;)) %&gt;% # Create and HTML widget table DT::datatable( # The table caption caption = &quot;Table 1: South Front - Centrality and Power&quot;, # Select the CSS class: https://datatables.net/manual/styling/classes class = &#39;cell-border stripe&#39;, # Show rownames? rownames = FALSE, # Whether/where to use/put column filters filter = &quot;top&quot;, # The row/column selection mode selection = &quot;multiple&quot;, # Pass along a list of initialization options # Details here: https://datatables.net/reference/option/ options = list( # Is the x-axis (horizontal) scrollable? scrollX = TRUE, # How many rows returned in a page? pageLength = 10, # Where in the DOM you want the table to inject various controls? # Details here: https://legacy.datatables.net/ref#sDom sDom = &#39;&lt;&quot;top&quot;&gt;lrt&lt;&quot;bottom&quot;&gt;ip&#39;) ) 11.5 Centrality and Prestige (Directed Networks) We will re-import the South Front data set one more time but consider it a directed network this time to look at the concepts of centrality and prestige. Specifically, make sure you use the directed = TRUE parameter within the graph_from_data_frame() function. sf_gd &lt;- read.csv(file = &quot;data/SouthFront_EL.csv&quot;, header = TRUE) %&gt;% graph_from_data_frame(directed = TRUE) Take a look at the new igraph object. sf_gd IGRAPH 8386450 DN-- 236 310 -- + attr: name (v/c), Type (e/c), Id (e/n), Label (e/l), timeset (e/l), | Weight (e/n) + edges from 8386450 (vertex names): [1] UCYE61Gy3RxiI2hSdCmAgP9w-&gt;UClvD6c1VI75QZWJjA_yiWhg [2] UCFpuO2wt_3WSrk-QG7VjUhQ-&gt;UC2C_jShtL725hvbm1arSV9w [3] UCqxZhJewxqhB4cNsxJFjIhg-&gt;UClvD6c1VI75QZWJjA_yiWhg [4] UCWNbidLi4FXBd83ixoB1v-A-&gt;UClvD6c1VI75QZWJjA_yiWhg [5] UCShSHheWVd42CdiVAYn-9xQ-&gt;UCLoNQH9RCndfUGOb2f7E1Ew [6] UCNMbegBD9OjH4Eza8vVjBMg-&gt;UCFWjEwhX6cSAKBQ28pufG3w [7] UC8zZkogm0hU7_zifdxSQa0Q-&gt;UCK09g6gYGMvU-0x1VCF1hgA + ... omitted several edges 11.5.1 In-N-Out Degree, Hubs and Authorities Lets first calculate in-degree and out-degree for the network. sf_gd &lt;- sf_gd %&gt;% set.vertex.attribute(name = &quot;in&quot;, value = degree(sf_gd, mode = &quot;in&quot;)) %&gt;% set.vertex.attribute(name = &quot;out&quot;, value = degree(sf_gd, mode = &quot;out&quot;)) sf_gd IGRAPH 8386450 DN-- 236 310 -- + attr: name (v/c), in (v/n), out (v/n), Type (e/c), Id (e/n), Label | (e/l), timeset (e/l), Weight (e/n) + edges from 8386450 (vertex names): [1] UCYE61Gy3RxiI2hSdCmAgP9w-&gt;UClvD6c1VI75QZWJjA_yiWhg [2] UCFpuO2wt_3WSrk-QG7VjUhQ-&gt;UC2C_jShtL725hvbm1arSV9w [3] UCqxZhJewxqhB4cNsxJFjIhg-&gt;UClvD6c1VI75QZWJjA_yiWhg [4] UCWNbidLi4FXBd83ixoB1v-A-&gt;UClvD6c1VI75QZWJjA_yiWhg [5] UCShSHheWVd42CdiVAYn-9xQ-&gt;UCLoNQH9RCndfUGOb2f7E1Ew [6] UCNMbegBD9OjH4Eza8vVjBMg-&gt;UCFWjEwhX6cSAKBQ28pufG3w [7] UC8zZkogm0hU7_zifdxSQa0Q-&gt;UCK09g6gYGMvU-0x1VCF1hgA + ... omitted several edges Remember, you can get back the node attributes using the get.data.frame() function. head(get.data.frame(sf_gd, what = &quot;vertices&quot;)) name in out UCYE61Gy3RxiI2hSdCmAgP9w UCYE61Gy3RxiI2hSdCmAgP9w 0 1 UCFpuO2wt_3WSrk-QG7VjUhQ UCFpuO2wt_3WSrk-QG7VjUhQ 0 1 UCqxZhJewxqhB4cNsxJFjIhg UCqxZhJewxqhB4cNsxJFjIhg 0 1 UCWNbidLi4FXBd83ixoB1v-A UCWNbidLi4FXBd83ixoB1v-A 0 1 UCShSHheWVd42CdiVAYn-9xQ UCShSHheWVd42CdiVAYn-9xQ 0 1 UCNMbegBD9OjH4Eza8vVjBMg UCNMbegBD9OjH4Eza8vVjBMg 0 1 Now, lets vary node size of plots by in-degree and out-degree. Again, we will hide the labels so you can see patterns more clearly. par(mfrow = c(1, 2)) # Save the coordinates coords &lt;- layout_with_kk(sf_gd) # Plot graph with rescaled nodes plot(sf_gd, asp = 0, main = &quot;South Front (In-Degree)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_gd, name = &quot;in&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.25) plot(sf_gd, asp = 0, main = &quot;South Front (Out-Degree)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_gd, name = &quot;out&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.25) We can correlate the two measures if we want. The negative correlation makes sense when we look at the in-degree and out-degree plots. cor(get.data.frame(sf_gd, what = &quot;vertices&quot;)[, -1]) in out in 1.0000000 -0.3170374 out -0.3170374 1.0000000 Lets now turn to Hubs (out-degree eigenvector) and Authorities (in-degree eigenvector), which we cannot run in igraph. Like evcent(), the authority_score() and hub_score() functions return a named list, so we must extract the vector of node scores using $vector . sf_gd &lt;- sf_gd %&gt;% set.vertex.attribute(name = &quot;hubs&quot;, value = hub_score(sf_gd)$vector) %&gt;% set.vertex.attribute(name = &quot;auth&quot;, value = authority_score(sf_gd)$vector) # Note the new node attributes sf_gd IGRAPH 8386450 DN-- 236 310 -- + attr: name (v/c), in (v/n), out (v/n), hubs (v/n), auth (v/n), Type | (e/c), Id (e/n), Label (e/l), timeset (e/l), Weight (e/n) + edges from 8386450 (vertex names): [1] UCYE61Gy3RxiI2hSdCmAgP9w-&gt;UClvD6c1VI75QZWJjA_yiWhg [2] UCFpuO2wt_3WSrk-QG7VjUhQ-&gt;UC2C_jShtL725hvbm1arSV9w [3] UCqxZhJewxqhB4cNsxJFjIhg-&gt;UClvD6c1VI75QZWJjA_yiWhg [4] UCWNbidLi4FXBd83ixoB1v-A-&gt;UClvD6c1VI75QZWJjA_yiWhg [5] UCShSHheWVd42CdiVAYn-9xQ-&gt;UCLoNQH9RCndfUGOb2f7E1Ew [6] UCNMbegBD9OjH4Eza8vVjBMg-&gt;UCFWjEwhX6cSAKBQ28pufG3w [7] UC8zZkogm0hU7_zifdxSQa0Q-&gt;UCK09g6gYGMvU-0x1VCF1hgA + ... omitted several edges Go ahead and plot the network with node size varying by hub and authority scores. par(mfrow = c(1, 2)) # Plot graph with rescaled nodes plot(sf_gd, asp = 0, main = &quot;South Front (Authority)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_gd, name = &quot;auth&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.25) plot(sf_gd, asp = 0, main = &quot;South Front (Hubs)&quot;, layout = coords, vertex.size = scales::rescale(get.vertex.attribute(sf_gd, name = &quot;hubs&quot;), to = c(1, 10)), vertex.label = NA, vertex.color = &quot;lightblue&quot;, edge.arrow.size = 0.25) 11.5.2 Correlations Now create a table of prestige scores for South Fronts network. #Run correlations for columns containing centrality scores, which is all except # the first column. cor(get.data.frame(sf_gd, what = &quot;vertices&quot;)[, -1]) in out hubs auth in 1.0000000 -0.3170374 -0.2200667 0.9375712 out -0.3170374 1.0000000 0.5217801 -0.2445618 hubs -0.2200667 0.5217801 1.0000000 -0.1704381 auth 0.9375712 -0.2445618 -0.1704381 1.0000000 Take a look at the pairs.panels() output. pairs.panels(get.data.frame(sf_gd, what = &quot;vertices&quot;)[, -1]) 11.5.3 Interactive Table Lets create another interactive table for our prestige-based centrality measures. Again, lets extract the nodes data.frame from the graph and then recode numeric variables to clean up the table. centralities &lt;- get.data.frame(sf_gd, what = &quot;vertices&quot;) # Round up numeric values centralities &lt;- as.data.frame( sapply(names(centralities), function(s) { centralities[[s]] &lt;- ifelse(is.numeric(centralities[[s]]), yes = round(centralities[s], digits = 3), no = centralities[s]) }) ) Use datatable and some base R to clean up the data.frame and create a good looking widget. centralities[order(centralities$in., decreasing = TRUE), ] %&gt;% `colnames&lt;-`(c(&quot;Channel&quot;, &quot;In-Degree&quot;, &quot;Out-Degree&quot;, &quot;Hubs&quot;, &quot;Authority&quot;)) %&gt;% DT::datatable( caption = &quot;Table 2: South Front - Centrality and Prestige&quot;, class = &#39;cell-border stripe&#39;, rownames = FALSE, filter = &quot;top&quot;, selection = &quot;multiple&quot;, options = list( scrollX = TRUE, pageLength = 10, sDom = &#39;&lt;&quot;top&quot;&gt;lrt&lt;&quot;bottom&quot;&gt;ip&#39;) ) 11.6 Brokerage For this section, we will use the strike_g object. Begin by plotting the network side-by-side. The initial plot is without group membership but the second highlights the groups. par(mfrow = c(1, 2)) # Save coordinates coords &lt;- layout_with_kk(strike_g) # Plot them plot(strike_g, main = &quot;Strike Network&quot;, layout = coords, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.color = &quot;lightblue&quot;, edge.arrow.mode = 0) plot(strike_g, main = &quot;Strike Network (Groups)&quot;, layout = coords, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.color = get.vertex.attribute(strike_g, &quot;Group&quot;), edge.arrow.mode = 0) 11.6.1 Cutpoints igraph has two functions that we can use to explore cutopoints (articulation_points() and biconnected_components()). However, biconnected_components() identifies both cutpoints (aka, articulation points) and bicomponents. The output of this function is a named list with five elements: no: the number of biconnected components in the graph tree_edges: a list with sets of edge ids in a given biconnected component component_edges: all edges in components components: vertices in components articulation_points: the articulation points in the graph strike_bicomp &lt;- biconnected_components(strike_g) # Take a look at the list names names(strike_bicomp) [1] &quot;no&quot; &quot;tree_edges&quot; &quot;component_edges&quot; [4] &quot;components&quot; &quot;articulation_points&quot; Lets get a list of which actors belong to which bicomponent (note that some belong to more than one  these are cutpoints) and a list of cutpoints; note that igraph identifies bicomponents of size 2 or greater, while statnet only identifies bicomponents of size 3 or greater. strike_bicomp$components [[1]] + 2/24 vertices, named, from 8298674: [1] Gill Frank [[2]] + 4/24 vertices, named, from 8298674: [1] Carlos Eduardo Domingo Alejandro [[3]] + 2/24 vertices, named, from 8298674: [1] Bob Alejandro [[4]] + 10/24 vertices, named, from 8298674: [1] Karl Lanny Ozzie John Gill Ike Mike Hal Bob Norm [[5]] + 8/24 vertices, named, from 8298674: [1] Quint Paul Russ Ted Vern Norm Utrecht Sam [[6]] + 3/24 vertices, named, from 8298674: [1] Wendle Sam Xavier strike_bicomp$articulation_points + 5/24 vertices, named, from 8298674: [1] Gill Alejandro Bob Norm Sam We can use the character vector in $articulation_points to depict cutpoints. To do so, we can create a node attribute using the ifelse() function. strike_g &lt;- set.vertex.attribute(strike_g, name = &quot;cutpoint&quot;, value = ifelse( V(strike_g) %in% strike_bicomp$articulation_points, TRUE, FALSE) ) # Plot it and colorize by new node attribute plot(strike_g, layout = coords, main = &quot;Strike Network (Cutpoints)&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.mode = 0, vertex.color = get.vertex.attribute(strike_g, &quot;cutpoint&quot;) ) 11.6.2 Cutsets (Key Player) Cutsets are sets of actors/nodes whose removal maximizes some metric. In Steve Borgattis original article (2006), he sought to identify the set of actors that maximized the level of fragmentation in a network. With cutsets, we indicate the size of the set. We can get cutsets with the influenceR package. Here, weve only asked for a cutset of three actors because it is such a small network. If you run this repeatedly, youll notice that it will return different solutions. Thats because there are multiple solutions. First, run the function. cutset &lt;- influenceR::keyplayer(strike_g, k = 3) cutset + 3/24 vertices, named, from 8298674: [1] Norm Eduardo John Notice the output is a vector of names. Like before, we can assign the output to the vertex attributes using the ifelse() function. strike_g &lt;- set.vertex.attribute(strike_g, name = &quot;cutset_3&quot;, value = ifelse(V(strike_g) %in% cutset, TRUE, FALSE)) # Plot it and colorize by new node attribute plot(strike_g, layout = coords, main = &quot;Strike Network (Cutset = 3)&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.mode = 0, vertex.color = get.vertex.attribute(strike_g, &quot;cutset_3&quot;) ) We can also see how much the removal of the cutset fragments the network. First, lets calculate the level of fragmentation before the removal of the nodes (it should be 0.00). strike_distance &lt;- distance_table(strike_g, directed = FALSE) frag_before &lt;- (1 - sum(strike_distance$res) / (sum(strike_distance$res) + strike_distance$unconnected)) frag_before [1] 0 Note, that we use the same commands that we did in the topography lab to calculate fragmentation. Now, calculate the increase in fragmentation after the cutsets removal. We have to remove the cutset before calculating it, of course. strike2_g &lt;- induced_subgraph(strike_g, vids = which(V(strike_g)$cutset_3 == &quot;FALSE&quot;)) strike2_distance &lt;- distance_table(strike2_g, directed = FALSE) frag_after &lt;- (1 - sum(strike2_distance$res) / (sum(strike2_distance$res) + strike2_distance$unconnected)) frag_after [1] 0.5142857 frag_after - frag_before [1] 0.5142857 Another package, keyplayer, offers more flexibility in the sense that you can choose what centrality measure you want to use to identify the initial set of seeds. Here well use fragmentation centrality, which is what Steve Borgatti uses in the standalone program, keyplayer. We didnt discuss fragmentation centrality above, but it measures the extent to which individual nodes fragment the network if they are removed. Other options include closeness, eigenvector, etc. (see ?keyplayer::kpset). The function does requires an adjacency matrix as a input, though, so note the slight difference in commands. Additionally, the output is a named list, we will use the $keyplayer element. cutset2 &lt;- keyplayer::kpset(as_adjacency_matrix(strike_g), size = 3, type = &quot;fragment&quot;)$keyplayers cutset2 [1] 5 9 17 Note that this time we assign the cutset as a color attribute, so we dont even have to tell igraph what variable to use for color. strike_g &lt;- set.vertex.attribute(strike_g, name = &quot;color&quot;, value = ifelse(V(strike_g) %in% cutset2, &quot;red&quot;, &quot;skyblue&quot;)) # Plot it and colorize by new node attribute plot(strike_g, layout = coords, main = &quot;Strike Network (Cutset = 3)&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.mode = 0) And, now, lets determine the increase in fragmentation after the cutsets removal. strike3_g &lt;- induced_subgraph(strike_g, vids = which(V(strike_g)$color == &quot;skyblue&quot;)) strike3_distance &lt;- distance_table(strike3_g, directed = FALSE) frag_after2 &lt;- (1 - sum(strike3_distance$res) / (sum(strike3_distance$res) + strike3_distance$unconnected)) frag_after2 [1] 0.747619 frag_after2 - frag_before [1] 0.747619 Finally, lets plot the network after the cutsets been removed. plot(strike3_g, layout = layout_with_kk, main = &quot;Fragmented Strike Network&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, edge.arrow.mode = 0) 11.6.3 Burts Constraint Next, we will calculate Burts constraint and its additive inverse (autonomy). To calculate constraint, we will use the aptly named function from the igraph library. Like the centrality functions, this one generates a numeric vector of scores for each node. As such, we can assign it to the graph vertices as an attribute. In order to calculate autonomy, we can substract the constraint from one. strike_g &lt;- strike_g %&gt;% set.vertex.attribute(name = &quot;constraint&quot;, value = constraint(strike_g)) %&gt;% set.vertex.attribute(name = &quot;autonomy&quot;, value = 1 - constraint(strike_g)) Plot the graph with nodes sized by both measures. The color will still reflect the cutset from the prior step. par(mfrow = c(1, 2)) plot(strike_g, layout = coords, main = &quot;Strike Network (Constraint &amp; Cutset)&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = scales::rescale(get.vertex.attribute(strike_g, &quot;constraint&quot;), to = c(1, 10)), edge.arrow.mode = 0) plot(strike_g, layout = coords, main = &quot;Strike Network (Autonomy &amp; Cutset)&quot;, vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = scales::rescale(get.vertex.attribute(strike_g, &quot;autonomy&quot;), to = c(1, 10)), edge.arrow.mode = 0) 11.6.4 Bridges: Edge Betweenness Finally, lets calculate edge betweenness. Like with vertices, edges can also get attribues in igraph. strike_g &lt;- strike_g %&gt;% set.edge.attribute(name = &quot;e_betweenness&quot;, value = edge_betweenness(strike_g, directed = FALSE, weights = NULL)) And then plot where edge width equals edge betweenness. plot(strike_g, layout = coords, main = &quot;Strike Network&quot;, sub = paste0(&quot;Edges sized by betweennes | Nodes colored by cutset and sized by autonomy&quot;), vertex.label.cex = 0.6, vertex.label.color = &quot;black&quot;, vertex.size = scales::rescale(get.vertex.attribute(strike_g, &quot;autonomy&quot;), to = c(1, 10)), edge.width = scales::rescale(get.edge.attribute(strike_g, &quot;e_betweenness&quot;), to = c(0.25, 5)), edge.arrow.mode = 0) We will hold off for now on creating an interactive table for brokerage but feel free to give it a shot on your own. Thats all for igraph for now. "],["centrality-and-brokerage-in-statnet.html", "12 Centrality and Brokerage in statnet 12.1 Setup 12.2 Libraries 12.3 Load Data 12.4 Centrality and Power (Undirected Networks) 12.5 Centrality and Prestige (Directed Networks) 12.6 Brokerage", " 12 Centrality and Brokerage in statnet 12.1 Setup Find and open your RStudio Project associated with this class. Begin by opening a new script. Its generally a good idea to place a header at the top of your scripts that tell you what the script does, its name, etc. ################################################# # What: Centrality and Brokerage in statnet # Created: 02.28.14 # Revised: 01.31.22 ################################################# If you have not set up your RStudio Project to clear the workspace on exit, your environment may contain the objects and functions from your prior session. To clear these before beginning use the following command. rm(list = ls()) Proceed to place the data required for this lab (SouthFront_EL.csv, SouthFront_NL.csv, Strike.net, and Strikegroups.csv) also inside your R Project folder. We have placed it in a sub folder titled data for organizational purposes; however, this is not necessary. In this lab we will consider a handful of actor-level measures. Specifically, we will walk through the concepts of centrality and brokerage on two different networks. Centrality is one of SNAs oldest concepts. When working with undirected data, a central actor can be someone who has numerous ties to other actors (degree), someone who is closer (in terms of path distance) to all other actors (closeness), someone who lies on the shortest path (geodesic) between any two actors (betweenness), or someone who has ties to other highly central actors (eigenvector). In some networks, the same actors will score high on all four measures. In others, they wont. There are, of course, more than four measures of centrality. For the centrality portion of this exercise, well look at a subset of South Fronts YouTube network that weve collected using YouTubes open API. Specifically, we will examine subscription-based ties among accounts (note the names are a string of what appears to be random combinations of letters and numbers) within South Fronts ego network (excluding South Front), which leaves us with a network of 310 subscriptions among 236 accounts. We will consider this network undirected for the Centrality and Power section, but directed for the Centrality and Prestige portion of this lab. Next, we will turn to measures that operationalize various aspects of brokerage. For that section, we will demonstrate the concept of brokerage by looking at a communication network of a wood-processing facility where workers rejected a new compensation package and eventually went on strike. Management then brought in an outside consultant to analyze the employees communication structure because it felt that information about the package was not being effectively communicated to all employees by the union negotiators. The outside consultant asked all employees to indicate, on a 5-point scale, the frequency that they discussed the strike with each of their colleagues, ranging from almost never (less than once per week) to very often (several times per day). The consultant used 3 as a cut-off value in order to identify a tie between two employees. If at least one of two persons indicated they discussed work with a frequency of three or more, a tie between them was included in the network. The data accompany the book, Exploratory Social Network Analysis with Pajek, also published by Cambridge. Hence, weve shared the data with you as a Pajek file. 12.2 Libraries Load the statnet library. library(statnet) We will also be using other libraries in this exercise such as DT, keyplayer, scales, and psych. This might be the first time you use these, so you may need to install them. to_install &lt;- c(&quot;DT&quot;,&quot;keyplayer&quot;, &quot;psych&quot;, &quot;scales&quot;) install.packages(to_install) If you have installed these, proceed to load CINNA, keyplayer, and psych. We will namespace functions from DT, and scales libraries (e.g., scales::rescale()) as these have functions that mask others from statnet. library(CINNA) library(keyplayer) library(psych) 12.3 Load Data Weve stored South Fronts YouTube network as an edge list. Go ahead and import it with the read.csv() function, convert it to a network object, and visualize it. For now we will import it as an undirected network. # Read data sf_el &lt;- read.csv(&quot;data/SouthFront_EL.csv&quot;, header = TRUE) # Remove parallel edges sf_el &lt;- sf_el[!duplicated(cbind(pmin(sf_el$Source, sf_el$Target), pmax(sf_el$Source, sf_el$Target))), ] # Create graph with edge list sf_net &lt;- as.network(sf_el, directed = FALSE, loops = FALSE, multiple = FALSE, matrix.type = &quot;edgelist&quot;) # Take a look at it sf_net Network attributes: vertices = 236 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 309 missing edges= 0 non-missing edges= 309 Vertex attribute names: vertex.names Edge attribute names: Id Label timeset Type Weight Plot it. gplot(sf_net, gmode = &quot;graph&quot;, mode = &quot;kamadakawai&quot;, vertex.col = &quot;lightblue&quot;, usearrows = FALSE, main = &quot;South Front&quot;) Next, load the Stike.net file and Strikegroups.csv, convert the relational data to an network object and add the node attributes to this graph. # Read graph strike_net &lt;- as.network( x = read.paj(file = &quot;data/Strike.net&quot;), directed = FALSE ) # Read attributes strike_attrs &lt;- read.csv(&quot;data/Strikegroups.csv&quot;, col.names = c(&quot;Name&quot;, &quot;Group&quot;)) # Add vertex attributes strike_net &lt;- set.vertex.attribute(strike_net, attrname = &quot;Group&quot;, value = strike_attrs[[&quot;Group&quot;]]) # Take a look at the graph object strike_net Network attributes: vertices = 24 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE title = Strike total edges= 76 missing edges= 0 non-missing edges= 76 Vertex attribute names: Group vertex.names x y Edge attribute names: Strike Lastly, plot the new network. gplot(strike_net, gmode = &quot;graph&quot;, mode = &quot;kamadakawai&quot;, main = &quot;Strike Network&quot;, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = strike_net %v% &quot;Group&quot;) 12.4 Centrality and Power (Undirected Networks) 12.4.1 Degree, Closeness, Betweenness and Eigenvector Centrality We will begin by calculating the four primary measures of centrality for undirected networks. Note that there are two closeness commands. The first is the standard measure of closeness (Freeman). Unfortunately, it doesnt handle infinite distances, so we show how to calculate an alternative (ARD) that does. Note that our use of both closeness scores is for demonstration purposes because our network is a connected graph with a single component. # Add centrality metrics as vertex attributes sf_net &lt;- set.vertex.attribute(sf_net, attrname = &quot;degree&quot;, value = degree(sf_net, gmode = &quot;graph&quot;, ignore.eval = FALSE)) sf_net &lt;- set.vertex.attribute(sf_net, attrname = &quot;closeness&quot;, value = closeness(sf_net, gmode = &quot;graph&quot;)) sf_net &lt;- set.vertex.attribute(sf_net, attrname = &quot;ard&quot;, value = closeness(sf_net, gmode = &quot;graph&quot;, # the cmode=&quot;suminvundir&quot; gives # us ARD, which works with # disconnected networks. cmode = &quot;suminvundir&quot;)) sf_net &lt;- set.vertex.attribute(sf_net, attrname = &quot;betweenness&quot;, value = betweenness(sf_net, gmode = &quot;graph&quot;)) sf_net &lt;- set.vertex.attribute(sf_net, attrname = &quot;eigenvector&quot;, value = evcent(sf_net, # use.eigen = TRUE is not always # necessary; we&#39;ve included it # here to get more &quot;robust&quot; # results. use.eigen = FALSE)) # Take a look at the graph object sf_net Network attributes: vertices = 236 directed = FALSE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 309 missing edges= 0 non-missing edges= 309 Vertex attribute names: ard betweenness closeness degree eigenvector vertex.names Edge attribute names: Id Label timeset Type Weight 12.4.2 Correlations Lets create a data.frame with vertex centralities. centrality &lt;- data.frame(&quot;Channel&quot; = network.vertex.names(sf_net), &quot;Degree&quot; = sf_net %v% &quot;degree&quot;, &quot;Closeness (Freeman)&quot; = sf_net %v% &quot;closeness&quot;, &quot;Closeness (ARD)&quot; = sf_net %v% &quot;ard&quot;, &quot;Betweenness&quot; = sf_net %v% &quot;betweenness&quot;, &quot;Eigenvector&quot; = sf_net %v% &quot;eigenvector&quot;) head(centrality, n = 5) Channel Degree Closeness..Freeman. Closeness..ARD. 1 UCYE61Gy3RxiI2hSdCmAgP9w 1 0.2952261 0.3251773 2 UCFpuO2wt_3WSrk-QG7VjUhQ 1 0.2376138 0.2522695 3 UCqxZhJewxqhB4cNsxJFjIhg 1 0.2952261 0.3251773 4 UCWNbidLi4FXBd83ixoB1v-A 1 0.2952261 0.3251773 5 UCShSHheWVd42CdiVAYn-9xQ 1 0.2516060 0.2702837 Betweenness Eigenvector 1 0 0.062031013 2 0 0.002549735 3 0 0.062031013 4 0 0.062031013 5 0 0.006221302 To run a correlation between variables, use the cor() function. Note our data.frame (i.e., centrality) has six columns, including the first column containing channel names, which means we want to correlate the columns containing centrality scores only (i.e., columns 2-6). cor(centrality[, 2:6]) Degree Closeness..Freeman. Closeness..ARD. Betweenness Degree 1.0000000 0.4941497 0.5824820 0.9863708 Closeness..Freeman. 0.4941497 1.0000000 0.9908909 0.5346233 Closeness..ARD. 0.5824820 0.9908909 1.0000000 0.6192383 Betweenness 0.9863708 0.5346233 0.6192383 1.0000000 Eigenvector 0.7558708 0.8021207 0.8420753 0.7965928 Eigenvector Degree 0.7558708 Closeness..Freeman. 0.8021207 Closeness..ARD. 0.8420753 Betweenness 0.7965928 Eigenvector 1.0000000 Note that, for the most part, the centrality measures correlate highly with degree, especially betweenness. The two closeness measures correlate very high with each other as well, which is a good sign that theyre tapping into the same phenomenon. Heres a really nice correlation function (i.e., pairs.panels()) associated with the psych package, which you may need to install first. pairs.panels(centrality[, 2:6]) Lets plot the network where we vary node size by the centrality measures; note that weve rescaled them so that the nodes dont get overwhelmingly big or way too small. Weve turned off the labels, which are YouTube Channel IDs (i.e., really long), so you can see the results clearly. par(mfrow = c(2, 3)) # Save the coordinates coords &lt;- gplot.layout.kamadakawai(sf_net, layout.par = NULL) # Plot graph with rescaled nodes gplot(sf_net, main = &quot;South Front Degree&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net %v% &quot;degree&quot;, to = c(1, 5)), usearrows = FALSE) gplot(sf_net, main = &quot;South Front Closeness&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net %v% &quot;closeness&quot;, to = c(1, 5)), usearrows = FALSE) gplot(sf_net, main = &quot;South Front ARD&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net %v% &quot;ard&quot;, to = c(1, 5)), usearrows = FALSE) gplot(sf_net, main = &quot;South Front Betweenness&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net %v% &quot;betweenness&quot;, to = c(1, 5)), usearrows = FALSE) gplot(sf_net, main = &quot;South Front Eigenvector&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net %v% &quot;eigenvector&quot;, to = c(1, 5)), usearrows = FALSE) 12.4.3 Interactive Table The R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as HTML table widgets. The interactive widgets provide filtering, pagination, sorting, and many other features for the tables. We will namespace the datatable() function from library and provide it the centrality node table we created for the sf_net graph. DT::datatable(centrality, rownames = FALSE) Modify the numeric variables, rounding them to 3 decimal places. centrality &lt;- as.data.frame( sapply(names(centrality), function(s) { centrality[[s]] &lt;- ifelse(is.numeric(centrality[[s]]), yes = round(centrality[s], digits = 3), no = centrality[s]) }) ) Take a look at the table: DT::datatable(centrality, rownames = FALSE) You may want to clean up this table. Begin by looking at the datatable arguments by reading the documentation ?DT::datatable. Here we clean it the data in base R, then modify the HTML widget parameters. # Order the data.frame by decreasing degree value centrality &lt;- centrality[order(centrality$Degree, decreasing = TRUE), ] # Change column names for the data.frame colnames(centrality) &lt;- c(&quot;Channel&quot;, &quot;Degree&quot;, &quot;Closeness&quot;, &quot;ARD&quot;, &quot;Betweenness&quot;, &quot;Eigenvector&quot;) # Create and HTML widget table DT::datatable(centrality, # The table caption caption = &quot;Table 1: South Front - Centrality and Power&quot;, # Select the CSS class: https://datatables.net/manual/styling/classes class = &#39;cell-border stripe&#39;, # Show rownames? rownames = FALSE, # Whether/where to use/put column filters filter = &quot;top&quot;, # The row/column selection mode selection = &quot;multiple&quot;, # Pass along a list of initialization options # Details here: https://datatables.net/reference/option/ options = list( # Is the x-axis (horizontal) scrollable? scrollX = TRUE, # How many rows returned in a page? pageLength = 10, # Where in the DOM you want the table to inject various controls? # Details here: https://legacy.datatables.net/ref#sDom sDom = &#39;&lt;&quot;top&quot;&gt;lrt&lt;&quot;bottom&quot;&gt;ip&#39;) ) 12.5 Centrality and Prestige (Directed Networks) We will re-import the South Front data set one more time but consider it a directed network this time to look at the concepts of centrality and prestige. Specifically, make sure you use the directed = TRUE parameter within the as.network() function. sf_net_d &lt;- as.network(sf_el, directed = TRUE, loops = FALSE, matrix.type = &quot;edgelist&quot;) Take a look at the new network object. sf_net_d Network attributes: vertices = 236 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 309 missing edges= 0 non-missing edges= 309 Vertex attribute names: vertex.names Edge attribute names: Id Label timeset Type Weight 12.5.1 In-N-Out Degree, Input Domain, and Proximity Prestige Lets first calculate in-degree and out-degree for the network. Note that statnet also has a prestige function, which allows you to access a variety of prestige measures (including in and out-degree). We will focus on the former for this exercise. sf_net_d &lt;- set.vertex.attribute(sf_net_d, attrname = &quot;indeg&quot;, value = degree(sf_net_d, gmode = &quot;digraph&quot;, cmode = &quot;indegree&quot;)) sf_net_d &lt;- set.vertex.attribute(sf_net_d, attrname = &quot;outdeg&quot;, value = degree(sf_net_d, gmode = &quot;digraph&quot;, cmode = &quot;outdegree&quot;)) sf_net_d Network attributes: vertices = 236 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 309 missing edges= 0 non-missing edges= 309 Vertex attribute names: indeg outdeg vertex.names Edge attribute names: Id Label timeset Type Weight Now, lets vary node size of plots by in-degree and out-degree. Again, we will hide the labels so you can see patterns more clearly. par(mfrow = c(1, 2)) # Save the coordinates coords &lt;- gplot.layout.kamadakawai(sf_net_d, layout.par = NULL) # Plot graph with rescaled nodes gplot(sf_net_d, main = &quot;South Front In-Degree&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net_d %v% &quot;indeg&quot;, to = c(1, 5)), usearrows = TRUE) gplot(sf_net_d, main = &quot;South Front Out-Degree&quot;, coord = coords, displaylabels = FALSE, vertex.col = &quot;lightblue&quot;, vertex.cex = scales::rescale(sf_net_d %v% &quot;outdeg&quot;, to = c(1, 5)), usearrows = TRUE) We can correlate the two measures if we want. The negative correlation makes sense when we look at the in-degree and out-degree plots. cor(sf_net_d %v% &quot;indeg&quot;, sf_net_d %v% &quot;outdeg&quot;) [1] -0.31852 The prestige() function, which we will now use, can estimate more than in- and out-degree, such as eigenvector, input domain, and proximity prestige. We will stick with the latter two for now. # Input Domain sf_net_d &lt;- set.vertex.attribute(sf_net_d, attrname = &quot;domain&quot;, value = prestige(sf_net_d, cmode = &quot;domain&quot;)) # Proximity Prestige sf_net_d &lt;- set.vertex.attribute(sf_net_d, attrname = &quot;domain.proximity&quot;, value = prestige(sf_net_d, cmode = &quot;domain.proximity&quot;)) Take a look at the count of values in each vector: table(sf_net_d %v% &quot;domain&quot;) 0 1 2 5 6 8 13 16 23 24 32 33 57 64 218 5 1 1 1 1 1 1 2 1 1 1 1 1 table(sf_net_d %v% &quot;domain.proximity&quot;) 0 236 As we can see, however, the proximity command returns almost all 0s for input domain, and all 0s for proximity prestige. 12.5.2 Correlations Create a data frame to display the prestige scores South Fronts YouTube network (well add authorities once we move to igraph). centrality_d &lt;- data.frame(&quot;Channel&quot; = network.vertex.names(sf_net_d), &quot;Indegree&quot; = sf_net_d %v% &quot;indeg&quot;, &quot;Outdegree&quot; = sf_net_d %v% &quot;outdeg&quot;, &quot;Input Domain&quot; = sf_net_d %v% &quot;domain&quot;, &quot;Proximity Prestige&quot; = sf_net_d %v% &quot;domain.proximity&quot;) head(centrality_d) Channel Indegree Outdegree Input.Domain Proximity.Prestige 1 UCYE61Gy3RxiI2hSdCmAgP9w 0 1 0 0 2 UCFpuO2wt_3WSrk-QG7VjUhQ 0 1 0 0 3 UCqxZhJewxqhB4cNsxJFjIhg 0 1 0 0 4 UCWNbidLi4FXBd83ixoB1v-A 0 1 0 0 5 UCShSHheWVd42CdiVAYn-9xQ 0 1 0 0 6 UCNMbegBD9OjH4Eza8vVjBMg 0 1 0 0 Take a look at the pairs.panels() output. pairs.panels(centrality_d[, 2:5]) 12.5.3 Interactive Table Lets create another interactive table for our prestige-based centrality measures. Again, lets extract the nodes data.frame from the graph and then recode numeric variables to clean up the table. # Round up numeric values centrality_d &lt;- as.data.frame( sapply(names(centrality_d), function(s) { centrality_d[[s]] &lt;- ifelse(is.numeric(centrality_d[[s]]), yes = round(centrality_d[s], digits = 3), no = centrality_d[s]) }) ) Use datatable and some base R to clean up the data.frame and create a good looking widget. centrality_d &lt;- centrality_d[order(centrality_d$Indegree, decreasing = TRUE), ] DT::datatable(centrality_d, caption = &quot;Table 2: South Front - Centrality and Prestige&quot;, class = &#39;cell-border stripe&#39;, rownames = FALSE, filter = &quot;top&quot;, selection = &quot;multiple&quot;, options = list( scrollX = TRUE, pageLength = 10, sDom = &#39;&lt;&quot;top&quot;&gt;lrt&lt;&quot;bottom&quot;&gt;ip&#39;) ) 12.6 Brokerage For this section, we will use the strike_net object. Begin by plotting the network side-by-side. The initial plot is without group membership but the second highlights the groups. par(mfrow = c(1, 2)) # Save coordinates coords &lt;- gplot.layout.kamadakawai(strike_net, layout.par = NULL) # Plot them gplot(strike_net, gmode = &quot;graph&quot;, coord = coords, main = &quot;Strike Network&quot;, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5) gplot(strike_net, main = &quot;Strike Network (Groups)&quot;, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = strike_net %v% &quot;Group&quot;) 12.6.1 Cutpoints Next, identify cutpoints and the plot the graph with the cutpoints given a different color. The cutpoints() function identifies the cutpoints in a graph; however, keep in mind that these may vary based on the mode as either directed (digraph) or undirected (graph). The output is a logical vector indicating cutpoint status if return.indicator = TRUE for each vertex (TRUE if cutpoint and FALSE otherwise). Like before, we can assing this output vector to the graph vertices as an attribute. strike_net &lt;- set.vertex.attribute(strike_net, attrname = &quot;cutpoint&quot;, value = cutpoints(strike_net, mode = &quot;graph&quot;, return.indicator = TRUE)) strike_net Network attributes: vertices = 24 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE title = Strike total edges= 76 missing edges= 0 non-missing edges= 76 Vertex attribute names: cutpoint Group vertex.names x y Edge attribute names: Strike Now plot it. gplot(strike_net, main = &quot;Strike Network (Cutpoints)&quot;, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = strike_net %v% &quot;cutpoint&quot;) Lets change-up the color a bit using an ifelse() evaluation. If the vertex is a cutpoint, we will make it \"red\"; otherwise, \"lightblue\". gplot(strike_net, main = &quot;Strike Network (Cutpoints)&quot;, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = ifelse(strike_net %v% &quot;cutpoint&quot;, yes = &quot;red&quot;, no = &quot;lightblue&quot;)) If we vary node size by betweenness centrality, we can see that a correlation exists between cutpoints and betweenness. It isnt perfect, however. gplot(strike_net, main = &quot;Strike Network (Cutpoints + Betweenness)&quot;, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = ifelse(strike_net %v% &quot;cutpoint&quot;, yes = &quot;red&quot;, no = &quot;lightblue&quot;), vertex.cex = scales::rescale(betweenness(strike_net), to = c(0.5, 3))) In order to locate the graph bicomponents, we can use the bicomponent.dist() function, which returns a named list containing multiple elements. After we identify bicomponents, we can use the names() function to see the items the function generates. strike_bc &lt;- bicomponent.dist(strike_net) names(strike_bc) [1] &quot;members&quot; &quot;membership&quot; &quot;csize&quot; &quot;cdist&quot; The command only identifies bicomponents of size 3 or greater, which you can see by typing, strike_bc$csize. Typing strike_bc$membership produces the group membership. Youll also note that when a node belongs to more than one bicomponent, the command randomly assigns it to one of the bicomponents. strike_bc$csize 1 2 3 4 10 8 4 3 strike_bc$membership [1] 4 2 NA 3 2 1 2 1 1 2 4 1 1 2 4 2 1 1 1 3 3 2 3 1 You can use the $membership element to color nodes in the graph. gplot(strike_net, main = &quot;Strike Network (Bicomponents Membership)&quot;, gmode = &quot;graph&quot;, coord = coords, label = network.vertex.names(strike_net), label.col = &quot;black&quot;, label.cex = 0.6, label.pos = 5, vertex.col = strike_bc$membership, vertex.cex = scales::rescale(betweenness(strike_net), to = c(0.5, 3))) 12.6.2 Cutsets (Key Player) Here we will use the keyplayer package to identify a cutset of three because the strike network is such a small one. If you run this repeatedly, youll notice that it will return different solutions. Thats because there are multiple solutions. As seen in the igraph version of this lab, the influenceR package works well with igraph objects. However, it does not work well with network objects, which means wed have to convert our data first using intergraph. Well skip that step here and use the keyplayer package only. In any case, the keyplayer package offers more flexibility than the influenceR package in the sense that you can choose what centrality measure you want to use. Here well use fragmentation centrality, which is what Steve Borgatti uses in the standalone program, keyplayer. We didnt discuss fragmentation centrality above, but it measures the extent to which an individual node fragments the network if its removed. To identify the key player set in keyplayer use the kpset() function, which identifies the most central group of players in a social network based on a specified centrality measure and a target group size. The output of this function, like with many other functions, is a named list with two entries. The first, keyplayers, provides the indices of the nodes identified as key players, which is the relevant output here. First, take a look at the vector of indices. strike.adj &lt;- as.matrix.network(strike_net) keyplayer::kpset(strike.adj, 3, type = &quot;fragment&quot;)$keyplayers [1] 5 9 17 The output tells us which actors make up the cutset. For example, an output vector with 5, 9, 17 would indicate that actors 5, 9, and 17 make up our cutset. To access the node names, you can use the get.vertex.attitute() function paired with the [ accessor and the appropriate index. get.vertex.attribute(strike_net, &quot;vertex.names&quot;)[5] [1] &quot;Norm&quot; This can be further expanded to include multiple indexes at once through the c() function. my_cutset &lt;- get.vertex.attribute(strike_net, &quot;vertex.names&quot;)[c(5, 9, 17)] my_cutset [1] &quot;Norm&quot; &quot;Bob&quot; &quot;Gill&quot; Using the indexes one could create a logical vertex attribute for the cutset. strike_net %v% &quot;cutset&quot; &lt;- ifelse(get.vertex.attribute(strike_net, &quot;vertex.names&quot;) %in% my_cutset, TRUE, FALSE) # Take a look a new vertex attribute strike_net Network attributes: vertices = 24 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE title = Strike total edges= 76 missing edges= 0 non-missing edges= 76 Vertex attribute names: cutpoint cutset Group vertex.names x y Edge attribute names: Strike get.vertex.attribute(strike_net, &quot;cutset&quot;) [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE [13] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE Once weve done that, we can plot the strike network with cutsets. gplot(strike_net, mode = &quot;kamadakawai&quot;, displaylabels = TRUE, label.cex = 0.7, label.pos = 5, label.col = &quot;black&quot;, vertex.col = get.vertex.attribute(strike_net, &quot;cutset&quot;), usearrows = FALSE, main = &quot;Strike Network (Cutset = 3)&quot;) You could always recode the colors based on this logical vertex attribute. gplot(strike_net, mode = &quot;kamadakawai&quot;, displaylabels = TRUE, label.cex = 0.7, label.pos = 5, label.col = &quot;black&quot;, vertex.col = ifelse(get.vertex.attribute(strike_net, &quot;cutset&quot;), &quot;red&quot;, &quot;lightblue&quot;), usearrows = FALSE, main = &quot;Strike Network (Cutset = 3)&quot;) Lets calculate the increase in fragmentation after the cutsets removal. strike2_net &lt;- get.inducedSubgraph(strike_net, v = which( strike_net %v% &quot;cutset&quot; == FALSE)) frag_before &lt;- 1 - connectedness(strike_net) frag_before [1] 0 frag_after &lt;- 1 - connectedness(strike2_net) frag_after [1] 0.747619 frag_after - frag_before [1] 0.747619 Finally plot the fragmented network. gplot(strike2_net, mode = &quot;kamadakawai&quot;, displaylabels = TRUE, label.cex = 0.7, label.pos = 5, label.col = &quot;black&quot;, vertex.col = &quot;lightblue&quot;, usearrows = FALSE, main = &quot;Fragmented Strike Network&quot;) 12.6.3 Gould and Fernandez For the Gould and Fernandez algorithm, we need the strike groups membership data, which weve provided you as an attribute file (Strikegroups.csv) and previously imported. The brokerage() function calculates the brokerage analysis of Gould and Fernandez on an input graph given a partition (e.g., membership) vector. Because the output is a named list, we will store it in a separate object. strike_gf &lt;- brokerage(strike_net, # Pass the Group vector strike_attrs[[&quot;Group&quot;]]) Take a look at the output elements. ls(strike_gf) [1] &quot;cl&quot; &quot;clid&quot; &quot;exp.gli&quot; &quot;exp.grp&quot; &quot;exp.nli&quot; &quot;n&quot; &quot;N&quot; [8] &quot;raw.gli&quot; &quot;raw.nli&quot; &quot;sd.gli&quot; &quot;sd.grp&quot; &quot;sd.nli&quot; &quot;z.gli&quot; &quot;z.nli&quot; For a more detailed description of these elements, take a look at the documentation ??brokerage. The relevant element in this list is raw.nli, which includes a matrix of observed brokerage scores by vertex. Access it with the $ accessor and examine the matrix. strike_gf$raw.nli w_I w_O b_IO b_OI b_O t Xavier 0 0 0 0 0 0 Utrecht 8 0 0 0 0 8 Frank 0 0 0 0 0 0 Domingo 0 0 0 0 0 0 Norm 18 0 5 5 0 28 Hal 2 0 0 0 0 2 Russ 4 0 0 0 0 4 Karl 0 0 2 2 0 4 Bob 14 0 10 10 2 36 Quint 4 0 0 0 0 4 Wendle 0 0 0 0 0 0 Ozzie 0 0 1 1 0 2 Ike 4 0 0 0 0 4 Ted 2 0 0 0 0 2 Sam 8 0 0 0 0 8 Vern 2 0 0 0 0 2 Gill 10 0 0 0 0 10 Lanny 2 0 0 0 0 2 Mike 0 0 0 0 0 0 Carlos 0 0 0 0 0 0 Alejandro 0 0 3 3 0 6 Paul 2 0 0 0 0 2 Eduardo 0 0 0 0 0 0 John 12 0 0 0 0 12 The types of brokerage roles are defined (and codified above) in terms of group membership as follows: w_I: Coordinator role w_O: Itinerant broker role b_IO: Gatekeeper role b_OI: Representative role b_O: Liason role t: Total (cumulative) role occupancy strike_gf$raw.nli w_I w_O b_IO b_OI b_O t Xavier 0 0 0 0 0 0 Utrecht 8 0 0 0 0 8 Frank 0 0 0 0 0 0 Domingo 0 0 0 0 0 0 Norm 18 0 5 5 0 28 Hal 2 0 0 0 0 2 Russ 4 0 0 0 0 4 Karl 0 0 2 2 0 4 Bob 14 0 10 10 2 36 Quint 4 0 0 0 0 4 Wendle 0 0 0 0 0 0 Ozzie 0 0 1 1 0 2 Ike 4 0 0 0 0 4 Ted 2 0 0 0 0 2 Sam 8 0 0 0 0 8 Vern 2 0 0 0 0 2 Gill 10 0 0 0 0 10 Lanny 2 0 0 0 0 2 Mike 0 0 0 0 0 0 Carlos 0 0 0 0 0 0 Alejandro 0 0 3 3 0 6 Paul 2 0 0 0 0 2 Eduardo 0 0 0 0 0 0 John 12 0 0 0 0 12 Next, we will get the total brokerage score but only count representative/gatekeeper once since it is undirected network (the total score is in the 6th column, while the gatekeeper score is in the fourth). strike_gf$raw.nli[, 6] - strike_gf$raw.nli[, 4] Xavier Utrecht Frank Domingo Norm Hal Russ Karl 0 8 0 0 23 2 4 2 Bob Quint Wendle Ozzie Ike Ted Sam Vern 26 4 0 1 4 2 8 2 Gill Lanny Mike Carlos Alejandro Paul Eduardo John 10 2 0 0 3 2 0 12 Now, visualize the network sizing the nodes by total brokerage score. gplot(strike_net, gmode = &quot;graph&quot;, label = network.vertex.names(strike_net), coord = coords, label.col = &quot;black&quot;, label.cex = 0.6, vertex.col = strike_attrs[[&quot;Group&quot;]], label.pos = 5, vertex.cex = scales::rescale(strike_gf$raw.nli[, 6] - strike_gf$raw.nli[, 4], to = c(1, 5)), main = &quot;Gould &amp; Fernandez (Total Brokerage)&quot;) We will hold off for now on creating an interactive table for brokerage but feel free to give it a shot on your own. Thats all for statnet for now. "],["hypothesis-testing-qap-and-cug.html", "13 Hypothesis Testing (QAP and CUG) 13.1 Introduction 13.2 Developing and Testing Working Hypotheses 13.3 Challenging Expectations with Conditional Uniform Graphs (CUGs) 13.4 Quadratic Assignment Procedure (QAP) 13.5 A Note on interpretation: Causality in Networks 13.6 Summary: Lessons Learned", " 13 Hypothesis Testing (QAP and CUG) 13.1 Introduction With network data, we assume that relationships can explain (or be explained by) many of the similarities that we observe between nodes. For example, social network researchers have paid considerable attention to the principles known as homophily and diffusion (McPherson, Smith-Lovin, and Cook 2001; Valente 1995; Shalizi and Thomas 2011). In classical parametric statistics, working with network data presents a problem. Classical (parametric) statistical tests assume that observations (i.e., nodes) are independent from one another, whereas network methods assume interdependence (Krackhardt 1987a). The techniques covered in this chapter offer nonparametric alternatives to the more standard statistical tests. As previously mentioned, autocorrelation measures the interrelatedness of network attributes and can lead an analyst to underestimate the likelihood that the finding is spurious. Such interrelations, therefore, may frequently create the impression that relationships are meaningful much more frequently than they actually are. Nonparametric techniques are designed to greatly reduce such false positives (i.e., Type I errors). Unlike traditional parametric statistics, these nonparametric tests do not test whether the observed measures are likely to exist in a population. Rather, they are meant as a way of testing whether the measures that we calculate are spurious (Krackhardt 1992). The inference, then, should be confined to the particular network being analyzed and not extrapolated to generalize about all networks. In this document, we introduce hypothesis testing and lay the groundwork for several types of explanatory analyses we will discuss in subsequent exercises. We begin with a general explanation concerning the nature of hypothesis testing and then consider some of the challenges of using statistical models with network data. We then introduce and illustrate two approaches for testing hypotheses, namely Conditional Uniform Graphs (CUG) and the Quadratic Assignment Procedure (QAP). We conclude with a brief reflection on causality and a summary of the lessons learned in the chapter. All of the analyses in this chapter use the statnet suite of programs, which incorporates Carter Buttss network and sna packages, among others. When statnet loads, the the packages that it depends upon automatically load as well. Below, we may refer to various functions as residing in statnet, despite the fact that they are better characterized as being independent packages that are integrated into sna or network. The reference to statnet is therefore intended to be encompassing of all three packages. When in doubt, just use the help functions, help() or ?, to learn more about any individual command we cover below. Table 13.1 summarizes the primary functions outlined in this chapter as well as provides short descriptions about their use. library(statnet) Table 13.1: Summary of Chapter Packages and Functions. Package Function Description statnet cugtest() The older version of the function to perform conditional uniform graph (CUG) test. statnet cug.test() The newer version of the function to run CUG tests. statnet gcor() This function allows users to run correlations between matrices (i.e., networks) consisting of the same set of actors. statnet qaptest() The function to conduct quadratic assignment procedure (QAP) tests. statnet netlm() The function to run linear regression using ordinary least squares on social network data. statnet netlogit() The function to run logistic regression on social network data. 13.2 Developing and Testing Working Hypotheses Theoretical considerations should drive the development of research questions that lead to hypotheses. The way you develop this expectation, however, will depend on the nature of what it is you are trying to do. Researchers and academics typically develop their expectations through a review of the available literature in order to leverage theories and background information about social networks, similar situations, and any other pertinent factors. For example, Robins (2015) outlines several major social network-based theories that scholars have examined from various perspectives. Practitioners, on the other hand, often rely on different approaches, given the constraints they face (e.g., time to conduct analysis, knowledge of a client, knowledge in an area). One approach can be rather informal; they can build upon their intuition by working with experts, other analysts, and those in the field who have seen a particular type of network from a unique perspective. In practice, more points of view are generally better for working out the intuitions that drive an analytic plan. Another general approach practitioners take for developing expectations about a network is based on conclusions drawn from exploratory analyses. The previous exercises have described several techniques to explore, and subsequently describe, networks of interest. These approaches, however useful they may be, are limited in at least two important ways that require analysts to turn to hypothesis testing in many cases. First, network analysts cannot generally establish causal relationships among variables of interest when using descriptive techniques alone. For example, they cannot (and should not) state that certain relationships (e.g., communication and kinship) predict others (e.g., trust relations) without testing their hypothesis. Moreover, there may be additional relational and non-relational (i.e., attributes) factors that you would like to incorporate into the analysis. In other words, analysts should consider and simultaneously test several plausible explanations to better understand what may be driving one or more network phenomena. A combination of these two approaches can be ideal for the opportunity that multiple viewpoints provide to account for a range of theories, and different points of view. Here, we consider a few options that are likely to be helpful for those planning to use statistical analysis to test any suspicions developed in the course of background research, or more frequently, during exploratory and descriptive analyses. 13.2.1 Univariate, Bivariate, and Multivariate Statistical Tests This chapter introduces three types of test (univariate, bivariate, and multivariate) and two procedures (conditional uniform graphs and quadratic assignment procedure) that allow us to use them with network data. Generally speaking, univariate tests may be used to verify whether a network-level descriptor (e.g., centralization) differs from what we may expect. Alternatively, bivariate tests are designed to use one variable (represented by a type of network tie) to predict or explain another. Last, multivariate tests use multiple predictors to predict or describe an outcome of interest. Table 13.2: Statistical tests covered in this chapter and the procedures that allow us to use them with network data Test Type Procedure Single-sample t-test Univariate CUG Correlation Bivariate CUG or QAP Multiple regression Multivariate CUG or QAP Logistic regression Multivariate CUG or QAP The testing procedures outlined in this chapter, namely CUG and QAP tests, are similar in that they use simulations in order to generate distributions of hypothetical social networks, that we can use to compare observed social networks. However, they serve different purposes. CUG tests, both univariate and bivariate, serve as powerful approaches to evaluating many of the measures discussed in previous chapters (e.g., centralization) by comparing them with networks of a similar type (e.g., networks of the same size). These tests, which analysts overlook regularly, are handy because they help us examine the commonly used descriptors outlined in previous chapters. While bivariate QAP tests serve the same purpose as bivariate CUG tests, multivariate QAP tests (OLS and logistic) are useful tools to examine relationships among various networks, including binary and weighted networks, while controlling for a social networks pattern of ties. Taken together, these valuable tools provide analysts with relatively straightforward approaches to begin testing hypotheses about social structures. 13.3 Challenging Expectations with Conditional Uniform Graphs (CUGs) Social network researchers often ask themselves whether a particular measure is distinctive, or if it is more or less what one would expect of a network of this sort. That question is generally difficult to answer. Even when one has analyzed a large number of similar networks, such as communication networks, it can be difficult to quantify what to expect, given the functions, interactions, unique culture and history, and other features that may make a particular network somewhat idiosyncratic. Although there have been attempts to aggregate lessons learned about particular types of networks (Gerdes 2015) and several typologies exist, such as scale-free and random networks (Barabasi 2003), there are presently no agreed upon standard benchmarks against which to measure a particular type of network. What we can do, however, is compare some global measure of a particular network to a random assortment of networks that share some structural similarity with the original network. This is the idea behind univariate (involving only one variable) conditional uniform graph (CUG) tests. We can use structural measures such as size, density, or other aspects that characterize a particular network as a reference point, from which to model hypothetical networks. The question we are asking then becomes, is this measure of our network of interest about what we would expect of a network that is this size? or is this about what we would expect of a network with ties that are this dense? In this way, we can at least get an idea of whether a networks structure is relatively unique, as compared with a random assortment of networks that are similar in some way. In classical statistics, we normally gather a sample of independent observations (generally through random sampling) from a population through surveys, available data, or similar methods, and the distribution of those observations allows us to infer something about that population. Because it is currently impossible to collect a random sample of certain network typologies, Leo Katz and James Powell (1957) proposed the next best thing: generate the distribution based on certain known properties of the observed network. The distribution of networks that are generated are considered to be uniform in the sense that all networks of a given description are equally likely. That is to say that the sample of networks that are generated according to (or conditioned on) some network characteristic, such as network size, density, the distribution of dyads, and so on, represent the population of all possible networks that could fall into that description. Although the computing power we currently possess does not allow us to reasonably create all possible permutations of networks that share a particular description, we do not need to do so. Instead, we only need to generate a sample of similar networks, which we can use use to help us infer how typical a particular metric is for a certain type of network. The idea behind conditional uniform graphs (CUGs) is simple in its essence. By executing the commands listed below, you will essentially run through a four-step process in one brief step. The process for estimating a CUG is: First, calculate the summary measure (e.g., density, average degree, dyad census, centralization), or measure of network comparison (e.g., correlation). Next, generate a large number (usually n=1000) of random networks that share (or are conditioned on) a given parameter (e.g., size, number of edges, distribution of edges). Calculate the summary measure for all randomly generated networks. Compare the measure of the network being analyzed with the distribution of measures from the randomly generated networks. When comparing the measure of the network being analyzed with the distribution of measures from the randomly generated networks, we are essentially asking whether the measure we observe in the network under scrutiny occurs rarely enough for us to reject the idea that it is just what one would expect to see in any similarly scaled random network. For such an evaluation, the proportion of measures from the randomly generated networks that are greater than or equal to that of the original network, as well as the proportion that are equal to or less than the original networks measure, will provide an indication of the measures relative rarity (similar to a p-value). Alternatively, one can graph the distribution for a visual comparison between the measure for the original network and the distribution of measures in the randomly generated networks. In either case, the object is to compare the metric generated from the network of interest with the same metric calculated on all of the randomly generated networks. Think of conditional uniform graphs as testing whether a particular measure is something that is unique to a particular network, or whether it falls into the range of what we may expect from any random network of that sort. From a hypothesis-testing point of view, we are testing to see whether we can reject the idea that the network measure is no different from what we would expect at random. The idea of no difference is the null hypothesis, which is what we test with CUGs. Stated somewhat more formally: H0: There is no difference between (a particular global measure) of a given network and what we would get if the same measure was taken over a set of similar networks. Imagine that a particular network seems to be very centralized, in terms of betweenness, around a particular set of nodes. You can report the value of betweenness centralization that you observe in the network. But, if you also suspect that the density of ties in that network may be what explains how the network came to be so dominated by only a few nodes, then it can be very helpful to test that suspicion. The null hypothesis would then read as follows: H0: There is no difference between the measure of betweenness centralization in the network being analyzed and the betweenness centralization of randomly generated networks with the same density. If, and only if, the measure for the network being analyzed is very rarely similar to those of the various randomly generated networks, then we are free to reject the null hypothesis. In rejecting the null, we are free to conclude that the measure of the observed network differs significantly from what we would expect from similar networks. Following traditional understandings of statistical significance, we generally think of rare as being about 5% or less. So, if the measure of the original network is similar to less than 5% of the randomly generated networks, we would consider the difference to be statistically significant, and therefore different from what we would expect to get at random. If, for example, we conditioned the randomly generated networks only on the density of the original network and there is a statistically significant difference, then we can conclude that the density of the network does not explain why the measure is as extreme as it is (G. L. Robins 2013). 13.3.0.1 Example Data The examples in this section utilize networks that were elicited from a class of students from disparate programs who were learning network analysis. In addition to answering questions about who they worked with in the past, who they were presently working with, and with whom they have had classes in the past, the group was asked to develop questions that correspond with three levels of intimacy for them. The three resulting networks were titled know, buddy, and friend. Each is defined by the question used to elicit the particular relationship. Table 13.3: Class Friendship and Interaction Networks Network Name Survey Question know I know this person well enough to feel compelled to greet them in a crowded room, such as a bar or a restaurant. buddy I know this person well enough and I would choose - or have chosen - to spend time with them outside of class. friend I would feel comfortable enough to trust this person with personal information that I would not share with many people. groupWork Who have you studied or worked with in a group in the past? haveClass Who do you have at least one other class with? WorkWith Who do you talk with and work with during this class? Weve stored the files in .rda format (i.e. Rdata). You can load them using the load() function. load(&quot;know.rda&quot;) load(&quot;buddy.rda&quot;) load(&quot;friend.rda&quot;) load(&quot;groupWork.rda&quot;) load(&quot;haveClass.rda&quot;) load(&quot;WorkWith.rda&quot;) 13.3.1 Univariate CUG Tests in statnet Currently, statnet employs two forms of the CUG test function. The developers are phasing in a newer cug.test() function that eventually will be the only choice for running univariate or bivariate CUG tests in the statnet suite. The older cugtest() function (note the missing period in the name) remains the only method for running bivariate CUG tests and still has some functionality that the newer version currently does not exhibit. At its simplest, a CUG test will require three items of information: the network (dat), the global measure or univariate statistic to test (FUN), and the type of conditioning to create the randomly generated networks (cmode). The example below uses only betweenness centralization as the global measure. In some cases, however, global measures such as centralization may require additional arguments that support the function. For example, an analyst who wishes to run a CUG test using inverse closeness centralization as a global measure, will need to identify centralization as the function being tested (FUN = centralization); then, provide a list of arguments specifying the nodal centrality scores (closeness) and type of closeness being computed (suminvdir). Note that these latter two arguments would normally be provided when calculating the closeness centralization for a given graph (e.g., centralization(dat, FUN = closeness, cmode = \"suminvdir\")) using statnet. To communicate all of this, the script should read as follows. cug.test(dat = net, FUN = centralization, FUN.args = list(closeness, cmode = &quot;suminvdir&quot;), cmode = &quot;size&quot;) Other global measures such as transitivity, density, or reciprocity will not require passing additional arguments using FUN.args = list(). Also, note that we condition the randomly generated networks on all three of the conditions implemented in the cug.test() function: size; number of edges; and the distribution of dyads. This choice is for demonstration purposes only. Under normal circumstances, we would just condition the randomly generated networks on any one of the three. Here, we run all three to demonstrate how they differ. Each of the three options condition the randomly generated networks according to the number of nodes in the original network (\"size\"). Conditioning on \"edges\" jointly conditions simulations on size and the number of edges (for undirected networks) or the number of arcs (for directed networks) in a network. If the argument ignore.eval = FALSE is included, then the distribution of tie values will be used when conditioning on \"edges\". The default for this function is ignore.eval = TRUE, meaning that CUG tests will ignore tie values by default. When conditioning on \"dyad.census\", the function will replicate simulations according to the distribution of mutual, null, and (in the case of directed networks) asymmetric arcs in the original test network(s). By default, CUG tests in statnet will randomly generate 1,000 networks. The number of randomly generated networks can be reduced by specifying some smaller number (e.g., reps = 100); producing a smaller number of simulations will produce a quicker, if less precise, test. On one hand, this approach can be important for more intensive functions that take a while to render. On the other, this method can be an advantage for running tests on multiple networks when the measure is slow, or if the networks are large or particularly dense. For publishable-quality tests, however, we recommend to either use the default, or increase it to a much larger number (e.g., 10,000) in order to increase the theoretical sample size and, therefore, improve the precision of the test results. Figure 13.1: A directed network of self-reported friendships between students, sized according to betweenness. Consider the visualization of the directed network in 13.1. The friends network consists of ties structured from one of three questions that students answered to denote close friends in class. Given the networks structure and the betweenness centrality of each node, it seems apparent that three nodes dominate the network: q, r, and b. But is that level of centralization special to this class network, or is this something that we would normally expect for a network this size? Is it something that we would normally expect for a network with this size and number of edges? Is it something that we would normally expect for a network with this size and distribution of dyads? We could state, therefore, the null hypothesis for each as the following: H0: There is no difference between the measure of betweenness centralization in the friendship network and the betweenness centralization measures in networks of this size. H0: There is no difference between the measure of betweenness centralization in the friendship network and the betweenness centralization measures in networks of this size and number of edges. H0: There is no difference between the measure of betweenness centralization in the friendship network and the betweenness centralization measures in networks of this size and distribution of dyads. We can compare each CUG test run on betweenness centralization with a null distribution conditioned on size, edges, and dyad census. # Condition by size cug.test(dat = friend, FUN = centralization, FUN.arg = list(FUN = betweenness), cmode = &quot;size&quot;) # Condition by edges cug.test(dat = friend, FUN = centralization, FUN.arg = list(FUN = betweenness), cmode = &quot;edges&quot;) # Condition by dyad census cug.test(dat = friend, FUN = centralization, FUN.arg = list(FUN = betweenness), cmode = &quot;dyad.census&quot;) We present an example of the raw CUG test output conditioned on size below followed by organized results for all three conditions (see 13.4). Output for a CUG test includes information on the parameters included in the test. In the example below, the first four lines of the output indicate that network size served as the condition upon which to simulate networks; the network data were directed (digraph); the diagonal was not used (meaning that no loops were considered in calculations or simulations); and, the test generated 1,000 simulated networks (the default for this function). The lower portion of the output gives the observed value of betweenness centralization (\\(C_B\\)=0.17) for the network being tested. The next two lines list the proportion of simulated networks with measures that are greater than or equal to the observed value, and the proportion with measures that are less than or equal to the observed measure, respectively. In this case, all the measures in the null distribution of simulated networks were less than or equal to the observed value of betweenness centralization. Normally, we can examine the proportions in much the same manner as a p-value in a standard statistical test. For reporting purposes, p-values cannot theoretically be equal to zero. For that reason, it is a good practice to report zero values of p as some arbitrarily small value such as p&lt;0.0001. The following table presents the results for all three tests. Univariate Conditional Uniform Graph Test Conditioning Method: size Graph Type: digraph Diagonal Used: FALSE Replications: 1000 Observed Value: 0.1700113 Pr(X&gt;=Obs): 0 Pr(X&lt;=Obs): 1 Table 13.4: Output from CUG Tests Conditioning on Size, Edges, and Dyads Betweenness Percent Greater Percent Less 0.17 0.000 1.000 0.17 0.268 0.732 0.17 0.458 0.542 If you so desire, it is possible to produce a graphical representation of the comparison that is being implemented to run the test. This step is an optional one and it will not contribute greatly to the analysis. However, some may find it helpful to be able to see the comparison between the measure taken on the network, and the distribution of measures taken on the various simulations. Generating the graphical representation is a matter of plotting the test. The script below produces three plots, side-by-side. This step is done by using the par(mfrow = c(1, 3)) to set the plot area to have one row and three columns. Each plot consists of the test name and an optional title (main). par(mfrow = c(1, 3)) plot(cugBetSize, main = &quot;Betweenness \\nConditioned on Size&quot;) plot(cugBetEdges, main = &quot;Betweenness \\nConditioned on Edges&quot;) plot(cugBetDyad, main = &quot;Betweenness \\nConditioned on Dyads&quot;) Figure 13.2: CUG Plots Conditioning on Size, Edges, and Dyads The resulting histograms, pictured in 13.2, provide an intuitive representation of how the observed measure of betweenness centralization compares with the distributions of measures of the simulated networks. The vertical red line represents the observed measure and its relationship to the null distributions. In this case, the difference between the original networks betweenness centralization is substantially greater than are those taken of the simulated networks of the same size. The histograms, however, demonstrate no statistically significant differences for networks conditioned on the same size and the same number of edges (density) as the friend network, or networks of the same size and the same number of dyads as the friend network. The latter two histograms illustrate that we cannot rule out the null hypothesis that the friendship network is the same as what we would expect from networks of the same size and density, or networks that are the same size and and have the same number of dyads. We can rule out the null hypothesis that betweenness centralization for the friend network is what we would expect for networks of the same size. This result is likely due to the way density and the number of dyads are not restricted when the networks are conditioned on size alone and were likely very different from the others. 13.3.2 Bivariate Tests Using CUG Analysts can use conditional uniform graph tests with multiple networks for bivariate and multivariate statistical tests as well. A fundamental bivariate test that researchers use frequently is the correlation test. Correlations provide a measure of relationships between variables and can be used to summarize the amount of similarity between pairs of networks. Although correlation is a descriptive measure in statistics, there is an associated test. Before running a correlation test, the correlation value is normally symbolized with the letter r or, when referring to populations, the Greek letter Rho (\\(\\rho\\)). Here, we use r to describe correlations between networks. In either case, correlation reflects the strength and direction of a relationship between two variables. In network analysis, those variables are embodied by networks. Correlation Cheatsheet Correlation is generally expressed as a two digit decimal. Correlations may take any value between negative one (-1) and one (1). -1 and 1 both indicate a perfect relationship (or redundant information!). 0 indicates no relationship. The absolute value of the correlation indicates the strength of the relationship. The sign of the correlation (- / +) value indicates the direction of the relationship. It is also helpful to think of correlations in terms of how strong or weak they are. This way of thinking provides a more intuitive means of expressing what the correlation value indicates about the relationship between variables. 13.5, below, provides some rules of thumb for interpreting and reporting correlation values. Table 13.5: Rule of Thumb for Reading Correlation (Absolute) Values Correlation Value General Interpretation |r| = 1.0 Perfect Correlation 0.80 &lt; |r| &lt; 1.0 Very Strong 0.60 &lt; |r| &lt; 0.80 Strong 0.40 &lt; |r| &lt; 0.60 Moderate 0.20 &lt; |r| &lt; 0.40 Weak 0.00 &lt; |r| &lt; 0.20 Very Weak |r| = 0.00 No Correlation The correlation test was designed to assess whether the correlation value is meaningful in the data that are being used. If the test indicates that the correlation value is not meaningful, then what we have learned is that we cannot rule out the possibility that the finding is spurious and no relationship actually exists. By running a correlation test, we are essentially double-checking to see whether a particular correlation is worth reporting. If the p-value for the correlation test is statistically insignificant (by convention, less than 0.05), then we treat the correlation as indeterminate. (We do not, however, report the value as zero.) Testing correlation between networks in R is fairly straightforward and similar to the tests covered above using centralization. Although the correlation test will provide correlation values, it is a good idea to produce them separately. Before going too far into how to calculate and test a correlation, it is important to highlight an important caveat. Each of the networks should be composed of the same nodes (people, organizations, etc.), and they should be in the same order if they are to be legitimately compared. Correlations between networks consisting of entirely different entities are not meaningful, since we are seeking to compare how similar or different the pattern of ties are for each node in one network with the same nodes in another. In other words, analysts only should calculate correlations between networks that represent differing sets of relationships for the same set of nodes. When statnet compares networks, it compares nodes based on the order in which they appear. The correlation function does not pay attention to vertex names and will not check to make sure that the networks consist of the same set of nodes and in the same order. So, while it is possible to get statnet to run a correlation on two networks of the same size but comprised of entirely different entities (e.g., people and organizations), the resulting correlation would be meaningless. The defaults for the statnets correlation function, gcor(), are listed below. gcor(dat, dat2 = NULL, g1 = NULL, g2 = NULL, diag = FALSE, mode = &quot;digraph&quot;) At its simplest, the gcor() function requires only a list of two or more networks that are comprised of the same node set. The function will produce correlation matrices when used with multiple networks, or a correlation value when using only two networks. As is standard for statnet, the gcor() function defaults to reading the networks as though they are directed (mode). gcor(dat = know, dat2 = buddy) [1] 0.5155708 The g1 and g2 arguments are used to identify which elements (e.g., networks) of a list to use when calculating a correlation value. In the first example below, the correlation between the (directed) know and buddy networks is measured. In the second example, two of the three networks (know and buddy) are again selected, this time from a list of networks. The third example produces a correlation matrix. nets &lt;- list(know, buddy, friend) gcor(dat = nets, g1 = 1, g2 = 2) [1] 0.5155708 To make the output more legible, the third example is nested within the round() function to reduce the output to two digits. 13.6 provides a cleaner version of the correlation matrix of the three relationships. round(gcor(nets), digits = 2) 1 2 3 1 1.00 0.52 0.25 2 0.52 1.00 0.51 3 0.25 0.51 1.00 Table 13.6: Correlation Matrix for Aquaintances, Buddies, and Friends Buddies Close Friends Know Name 0.52 0.25 Buddies 1 0.51 Correlation coefficients help empirically describe relationships. They do not provide, however, an indication of whether an observed measure of a relationship is just what we would expect from a pair of similar networks. Testing the correlation value provides an indication of whether that measure essentially is what we would expect in networks of that sort. The first decision, then, is in deciding what networks of that sort means. Traditional (parametric) correlation tests are designed to take the amount of variation in the sample into account in order to estimate whether the samples observed correlation value is likely to exist in the wider population the sample represents. The null hypothesis is then stated as either no relationship, or the correlation value between the two variables does not exist in the population. Using r as the symbol for correlation, this is more succinctly stated as r=0. For conditional uniform graph tests, we generally mean networks of the same size and maybe some other description. As with CUG tests, the correlation value for the original network pair is compared with the correlation values for all pairs of simulated networks. What we actually test is whether the strength and direction of the relationship (the correlation) that we observe between two networks is generalizable to similar networks. If we fail to reject the null hypothesis, we are saying that we cannot rule out the possibility that there is no difference between the measure we calculated and what we would expect from any two randomly selected networks of a certain type. The following are three alternatives for stating the null hypothesis (\\(H_0\\)) for this test. If we fail to reject it, we are saying that we cannot rule out that the correlation value we observe is spurious. H0: r=0 H0: There is no relationship. (The observed measure is spurious.) H0: The correlation value of the network is the same as the correlation values between randomly simulated networks of similar __________. R users should note a few differences between the two functions.3 One of the major differences between the two functions is the modes used for conditioning the simulated networks. Though similar, the options are not necessarily the same for both functions. The older functions options allow analysts to condition networks based on size alone (order), size and tie probability (density), or size and bootstrapped degree distributions (ties). The default for this function is to condition networks on density. Therefore, if cmode (i.e., condition mode) is not specified, or if a user enters something the function does not recognize (i.e., anything that is not order, density, or ties), cugtest() will condition on density. Table 13.7: Differences in Conditioning between cug.test and cugtest Meaning cug.test (newer) cugtest (older) Number of nodes in the network size (default) order Distribution of edge values in the network ties Distribution of edge values, or number of edges in the network edges Density of the network density (default) Number (or value) of dyads the network dyad.census Another difference between the two is that this older version allows analysts to specify which networks in a list to test. Unlike the gcor() function, which can produce a correlation matrix from a list of multiple networks, statnet does not currently produce a matrix with the results of correlation tests using conditional uniform graphs. Users have to estimate each test individually, as in the example below, or employ in a function to produce such a table. By default, statnet will select the first and second networks in the list (g1 = 1 and g2 = 2). Users have to specify any other combination of networks. In the example below, correlation tests are run on a list of three networks (know, buddy, and friend). The know network is first, buddy is second, and friend is third. The function will see them, therefore, as networks 1, 2, and 3. The name of each object is, of course, up to the individual running the analysis. In this case, we named each test object in a way that helps keep track of which comparison is being run in each test and the conditions upon which the simulations are produced. This approach can be helpful since the output is savable and the network names and type of conditioning are not available in the output. nets &lt;- list(know, buddy, friend) corOrd13 &lt;- cugtest(nets, gcor, g1 = 1, g2 = 3, # The first and third network cmode = &quot;order&quot;) corOrd12 &lt;- cugtest(nets, gcor, g1 = 1, g2 = 2, # The first and second network cmode = &quot;order&quot;) corOrd23 &lt;- cugtest(nets, gcor, g1 = 2, g2 = 3, # The second and third network cmode = &quot;order&quot;) For demonstration purposes, the next output depicts the results of only the first of the three tests. Unlike the newer cug.test() function, cugtest() does not provide information about the mode used for conditioning, whether the network was read as directed or undirected, or whether the diagonal (indicating the presence of loops) was included in the test. What this output provides, however, is still sufficient for performing the test and interpreting the results. Specifically, the output includes the p-values that were calculated in the same manner as the newer version, the number of simulations that were generated (Replications), the correlation value (Test Value), and a summary of the correlation values calculated for the simulated networks. The p-values reflect what can be seen when comparing the correlation value for the know and friend networks with those of the simulated networks. At no point is the correlation between simulated networks greater than 0.25. It is, rather, uniformly much lower. From this, we may conclude that it is safe to reject the null and conclude that the observed correlation between know and friend is not spurious when considered in comparison to networks of this size. corOrd13 &lt;- cugtest(nets, gcor, g1 = 1, g2 = 3, cmode = &quot;order&quot;) summary(corOrd13) CUG Test Results Estimated p-values: p(f(rnd) &gt;= f(d)): 0 p(f(rnd) &lt;= f(d)): 1 Test Diagnostics: Test Value (f(d)): 0.2493995 Replications: 1000 Distribution Summary: Min: -0.1864245 1stQ: -0.0338401 Med: 0.0003852113 Mean: -0.001926882 3rdQ: 0.0279094 Max: 0.1199181 It is also possible to plot the distribution of simulation values using plot(corOrd13), though the plots will appear somewhat different from the newer versions. As with other CUG tests, however, the plots are best only for illustration purposes and unnecessary for inference. 13.4 Quadratic Assignment Procedure (QAP) Quadratic assignment procedure (QAP) is similar to CUG, in that it employs simulation in order to generate a distribution of hypothetical networks. However, whereas CUG controls for a network condition such as size or density, QAP controls for network structure (Mantel 1967; Hubert and Schultz 1976; Krackhardt 1987b, 1988). QAP is useful for running a variety of statistical functions and, until recently, has been one of the most popular (and, therefore, widely accepted) methods for assessing statistical significance with network data. Below, are three of the options available in statnet. QAP is similar to bootstrapping in that it builds its own distribution using network simulations that are based on the network being estimated. This particular simulation process is referred to as a permutation test, and it is designed to mimic all of the scenarios that would be possible if everyone were able to switch places in the social order. In other words, the rows and columns in the network matrix are shuffled (i.e., permuted) randomly while names (and all their corresponding attributes) remain in the original order. This process essentially changes the node with which a particular name and set of attributes is associated without disturbing the underlying structure (See Figure 13.3 for an example of a permuted network). This random rearrangement is repeated to create a large number (usually hundreds or thousands) of simulated networks, which are then analyzed in the same manner as the original network in order to create a distribution of statistical measures that can then be compared to the measures calculated for the observed (i.e., actual) network. As with CUG tests, the proportion of measures in the distribution that are greater than, and less than the measure taken from the original network are then produced to give p-value-type information. If the observed measure occurs sufficiently infrequently (p &lt; 0.05) within the distribution of simulated networks, then we can reject the null and conclude that the observed statistics are not likely to have occurred by chance alone. Figure 13.3: Two Permutations of the Same Network The essential difference, then, between CUG and QAP is that CUG controls for networks of a similar type, whereas QAP controls for the particular pattern of interrelatedness (i.e., network structure) in the original networks. When using QAP, we are ultimately asking whether the result of the test we are running is spurious, given the autocorrelation that is normally inherent in networks. The procedure accomplishes this by observing the interdependencies that are apparent through the network structure, and then essentially reshuffling the nodes along with all their attendant attributes among the various positions within that structure. The idea behind this is that the networks various interdependencies remain in place, but the permuted networks are used to see whether the values we calculate would still be the same if the entities themselves were not interdependent in the same ways. For an analogy, imagine that everyone in a network was moved to some new spot in the social order in the network. If the ties within the network are fairly dense, with a lot of symmetry, it is much less likely that permutations of the network will result in large differences in the social order. This implies that the resulting correlations would not reflect a great deal of difference either. To illustrate, consider an extreme case. A friendship network that forms a clique would not change at all if all the positions were swapped around, since everyone in the clique is already adjacent to everyone else. So, permuting the network likely would not produce any differences in terms of relative positions in the structure of the network. 13.4.1 Bivariate Tests Using QAP: Correlation Correlation values are calculated the same way, regardless of whether one is using CUG or QAP. It is the test and its accompanying assumptions that differ. When calculating the level of correlation between two networks, the first step is to compute the correlation coefficient between corresponding cells. Next, one of the networks is randomly permuted and the correlation is recalculated. This step is carried out hundreds or thousands of times in order to compute the proportion of times that a random measure is larger than or equal to the observed measure calculated in the first step. A low proportion suggests that a correlation between networks is unlikely to have occurred by chance and the correlation value is thus not spurious. Consider, for example, the organizational convergence networks, which describe the connections formed within and between two organizations that are in the process of merging into one. Table 13.8, below, presents the three types of relationships identified as being important to the daily functioning of the institution. One concern that arises when conducting a survey of this sort is whether respondents are reading closely when answering each question and providing information appropriate to each question or just giving the same set of alters each time. Table 13.8: Organizational convergence network Network Name Survey Question commun With whom do you communicate to perform your work? collab With whom do you collaborate? esteem With whom would you like to work in the future? Again, weve stored these as .rda files, which we can read using the load() function. load(&quot;Communication.rda&quot;) # Loads as &quot;commun&quot; load(&quot;Collaboration.rda&quot;) # Loads as &quot;collab&quot; load(&quot;Esteem.rda&quot;) # Loads as &quot;esteem&quot; Because it measures relationships, one can think of correlation as providing a description of how much overlap exists between each of the networks. We may use correlation, therefore, to check whether respondents provided largely the same answers for each of the three questions. We would expect some overlap (i.e., maybe a weak to moderate correlation) if respondents answer each of the questions with a different type of relationship in mind. But, strong, very weak, or negative correlations would be a sign that we did a poor job of eliciting the various relationships. Table 13.9 presents the results of the correlation measures among the three networks. [1] 0.6009021 1 2 3 1 1.0000000 0.6009021 0.4092608 2 0.6009021 1.0000000 0.3891069 3 0.4092608 0.3891069 1.0000000 Table 13.9: Table: Correlation Matrix Collaboration Esteem Communication 0.6 0.41 Collaboration 1 0.39 The correlation matrix in Table 13.9 demonstrates that the relationship between communication and collaboration is moderate to strong. This result makes sense, as we would expect that those who collaborate also communicate. But, not all who communicate useful information are also collaborating in their work. Collaboration and communication are also manifest ties, meaning that they are ties that exist in some tangible sense. The esteem network is more theoretical, and this becomes clear when one considers that the relationships between the manifest, and more tangible, networks correlate only weakly with the more nebulous idea of who people would like to work with in the future. The question that may arise at this point is just how much we should trust these correlation values. The test associated with Pearsons correlation assesses whether the correlation value is real, or meaningful. The qaptest() function operates in a manner that is very similar to that of cugtest(). In its current form, the qaptest() function can simulate only the interaction between one network pair at a time. It is, therefore, necessary to specify which networks are being compared each time. The example below gives examples of how we would test correlation values for each of the network pairs. nets &lt;- list(commun, collab, esteem) cortest12 &lt;- qaptest(nets, gcor, g1 = 1, g2 = 2) cortest13 &lt;- qaptest(nets, gcor, g1 = 1, g2 = 3) cortest23 &lt;- qaptest(nets, gcor, g1 = 2, g2 = 3) The format of the qaptest() functions output is similar to the cugtest() results. The label QAP Test Results indicates the test results, with p-value approximations, the correlation value for the networks being compared, the number of simulated networks that were produced, and the distribution of correlation values between simulated networks. One can read and interpret the output, therefore, in the same manner as with CUG tests. As with other tests, users also can produce the distribution plot for the QAP correlation test. summary(cortest12) QAP Test Results Estimated p-values: p(f(perm) &gt;= f(d)): 0 p(f(perm) &lt;= f(d)): 1 Test Diagnostics: Test Value (f(d)): 0.6009021 Replications: 1000 Distribution Summary: Min: -0.05672636 1stQ: -0.0152203 Med: -0.0006662233 Mean: -0.0001104733 3rdQ: 0.01334881 Max: 0.06347951 13.4.2 Multivariate Models using QAP Testing the significance of univariate network descriptors, as with CUG, or testing bivariate descriptors, which is possible in both CUG and QAP, are powerful for their ability to validate or qualify the results of the measure or a test. Multivariate techniques, on the other hand, provide the option of using multiple variables to predict or explain an outcome. The use of multiple variables offers a powerful advantage over univariate or bivariate techniques, as the multiple predictors hold the potential to explain a great deal more about some outcome of interest. Multivariate techniques offer the advantage of accounting for a great deal more of the complexity that is inherent in social relations. The techniques presented below, and in the following chapters, are means of conducting multivariate analyses with network data. The two techniques presented below are more classical approaches to the problem of conducting multivariate network tests. Although both CUG and QAP may be used to create the null distribution for either approach, we present only the more commonly used used QAP approach. Below are two forms of multivariate regression: logistic regression and ordinary least squares (OLS) regression. The major difference between the two is in how ties in the network functioning as the dependent variable (i.e., the network being predicted or explained) are measured. The type of regression you choose, therefore, should reflect the way ties are measured in the dependent network. For example, in a scenario for testing whether family ties, prior communications, or business ties predict (or explain) operational collaborations between group members, the outcome of interest is operational collaborations. Logistic regression is designed for binary data and is suited for predicting or explaining the odds that a tie will form between any two nodes, given other factors. One should use it when the outcome network has tie values of only one or zero. In terms of the example presented in the QAP correlation section, collaboration only would be measured as one (a collaboration is observed) or zero (there was no collaboration observed). Multiple OLS regression, on the other hand, is best suited for explaining or predicting continuous measures. It is therefore better suited for use with dependent networks with tie values that could take any number. In the operational collaboration example, multiple linear regression would be appropriate if one measures the degree of collaboration (e.g., number of observed collaborations, strength of the collaboration, layers of collaboration) rather than just the simple presence or absence of collaboration ties. Note that the ties within the predictor networks (those networks being used to predict or explain the outcome network) may be either binary or continuous for either method. Numerous options and interpretation caveats go with either regression method. The descriptions below are provided only as a very general introduction. To use either of these tools well, it is advised that users consult any standard statistical text for properly applying either type of regression. Aside from the fact that here we use networks rather than independent observations, the rules for applying these techniques are fairly similar. Multiple regression - both OLS and logistic - allows analysts to use multiple predictor variables to predict or explain what we should expect from some other variable of interest. That is to say, we can use more than just one variable when trying to understand another. This is important when one considers that, in reality, outcomes tend to be much more complex than what a single predictor can capture - no matter how critical or important that predictor may be. Multiple regression makes it possible to account for the relationship between the predictors and the outcome while also controlling for the interrelationships between the predictors themselves. If you estimate three regression models, each with a different predictor, and then a fourth with all three predictors together you will notice a difference between what each of the predictors say about the outcome individually, and what they say when they are all included in the same model. This is because each of the predictors may be themselves related to one another. If predictors are even somewhat related, then the information they contribute about the outcome variable will likely overlap to some degree. When taken together, the redundancies are taken into account and each variables unique contribution is much more precise. Figure 13.4: Uncorrelated and Correlated Predictors in a Regression More advanced tools are presented in subsequent chapters. They are considerably more involved and each is designed with specific analytic goals in mind. The following examples offer a good place to start for those who are just beginning to explore the potential of multivariate analysis. Once you have established a comfort zone around the following tools, you will likely have a much easier time in grasping the nature of ERGMs, SAOMs, latent space models, multilevel models, and other advanced techniques that are now available to network analysts. 13.4.2.1 Multiple OLS Regression - Basics Multiple ordinary least squares (OLS) regression is a powerful technique for estimating a continuous outcome. We generally think of the estimates that a regression produces as either predicting or explaining that outcome. The name, ordinary least squares, refers to the way the prediction fits the patterns evident in the data. For a model to be a good fit, it should, overall, have the least (smallest) squared differences between what the model predicts and what the data show. There are other methods for fitting a regression model, but this is probably the most common and best understood. Fundamentally, multiple regression uses the information contained in a set of independent variables (i.e., predictors) to make some prediction about a dependent variable (i.e., outcome). In theory, the variation in the outcome depends on the behavior of a set of predictors that are relatively independent of one another. Analyst use this tool to predict or explain what we should expect to see in some outcome if there is a change in one or more of the predictors. The generic equation for a regression model is: \\[ Y_i= \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\beta_{3} X_{3} + ... + \\beta_{k} X_{k} + \\varepsilon \\] In a regression model, \\(Y_i\\) represents the outcome, or dependent variable, that is being predicted or explained. The explanations are given by the \\(\\beta_{i} X_{i}\\) chunks of the model. The \\(X_{i}\\)s represent the predictors and the \\(\\beta_{i}\\)s represent the slope, which tells us how much and in what way we should expect the outcome to change for each one unit change in the particular predictor with which it is associated. When we specify (i.e., construct) a regression model, we only select the outcome and predictors (\\(Y = X_1 + X_2\\) or \\(earnings = experience + age\\)). The betas (\\(\\beta\\)) are calculated by estimating the regression. For example, if the beta for the predictor experience (in years) is calculated to be 250 (\\(\\beta_{experience} = 250\\)), then we would expect that, for each additional year of experience, the outcome (in this case, earnings) would increase by 250, holding the other predictors constant. That last bit (holding the other predictors constant) is important. It means that we are using the model as a relatively full description of what may predict or explain changes in an outcome. But, if we use just one of the variables to describe the expected changes in the outcome, then we should also acknowledge that this is what we would expect in the presence of the other variables. In other words, this is just one part of the story of what makes that outcome change. There are a few other elements to the equation that should be noted. The ellipse, followed by \\(+ \\beta_{k} X_{k}\\) is included to point out that it is possible to add any number of predictors to a model, within certain limits. There are a number of important considerations to selecting a reasonable and valid number of predictors for a regression model, and readers are strongly encouraged to seek that information in a text that deals more specifically with regression techniques and considerations. The (\\(\\beta_0\\)) symbol is referred to in the output from estimating a regression as the intercept. It may be interpreted as the value of the outcome when the values of all of the predictors are zero. An error term (\\(\\varepsilon\\)) is also included in the formal regression model. The error is not something that analysts will calculate. It is merely included to make the equation balance and acknowledge that we are only explaining a portion of what makes the outcome vary. There will always be something that we miss in our description (and, thus, in our models). 13.4.2.2 Multiple OLS Regression using QAP Some important differences exist between estimating a multiple regression with standard, randomly selected data and estimating one with network data. One of these considerations involves the number and type of simulations that are used for assessing the model. The example below focuses on introducing quadratic assignment procedure (QAP) for multivariate models. The QAP procedure that was initially introduced by Krackhardt assumes that all of the predictors are independent of one another (uncorrelated). The problem with that assumption is that relationships between predictors are common in multiple regression, but they reduce the accuracy of the original QAP test. Because the assumption of independent predictors is unrealistic in most circumstances, a newer QAP approach called double semi-partialling (Dekker, Krackhardt, and Snijders 2007) was developed to provide a more robust permutation test that better accounts for the correlations between predictors. This was done through a modification of QAP that permutes the regression residuals (the difference between what the model predicts and what the data actually show in each observation) for each of the predictor variables over the network structure. This creates a permutation method that is more effective in reducing Type I error rates than other available QAP approaches. We cannot emphasize enough that this overview of multiple regression and QAP multiple regression is very superficial. Please look into some of the suggested readings at the end of this chapter if you are not already familiar with either method. They can be very powerful analytic techniques, but the analysis also may be very nuanced in its execution if one hopes to produce an accurate interpretation of the output. 13.4.2.3 Options and Defaults in QAP OLS Regression The help section for netlm() offers the following defaults for the function: netlm(y, x, intercept = TRUE, mode = &quot;digraph&quot;, diag = FALSE, nullhyp = c(&quot;qap&quot;, &quot;qapspp&quot;, &quot;qapy&quot;, &quot;qapx&quot;, &quot;qapallx&quot;, &quot;cugtie&quot;, &quot;cugden&quot;, &quot;cuguman&quot;, &quot;classical&quot;), test.statistic = c(&quot;t-value&quot;, &quot;beta&quot;), tol = 1e-7, reps = 1000) The netlm() function requires, at minimum, a dependent variable (y) and at least one predictor (x). The dependent variable must be entered as a matrix of valued ties. This point is important to the analysis, as you will need a continuous dependent variable to run an OLS regression. If a network object, rather than a matrix, is entered as the dependent variable (y), then the tie values will be ignored and the network will be considered to be composed of binary ties. Users will receive no warning about this, and as previously discussed, multiple OLS is designed to predict tie values; thus, a dependent network composed only of 0s and 1s could result in misleading results. By default, the netlm() function reads the input networks as directed and not containing loops (i.e., diagonal values). If you are working with undirected data or data with loops, you will need to modify the mode or diag parameters to reflect those network features. The function will not automatically adjust how it reads in data. With the exceptions of y, x, mode, and diag, it is generally a good idea to leave the default arguments as they are when running a regression. That said, statnet allows users to modify a number of additional parameters that may be helpful under some very specific conditions. Users may change the type or manner in which the networks that are used for hypothesis testing are simulated, the speed and specificity of the test, and some control of the models ability to converge on accurate enough estimates. Users should consider changes to these parameters carefully before making them, as they will usually change major assumptions under which the model is estimated. As with other functions in statnet, the number of permutations defaults to 1000 (reps = 1000). Changes in this parameter will not affect the accuracy of the estimates in the regression output, but it will affect your ability to assess their significance. A larger number will generally make it easier to differentiate between whether the null should or should not be rejected. The tradeoff, especially with larger networks and multiple regression models, is that the regression can take a while to run. In the initial model-fitting phase, where a user may wish to see if they can get a model to run, it is fine to lower the number of permuted replications for the sake of speed. However, in the interest of avoiding a false positive (Type I error), any final models should be run with a larger number of iterations (&gt;1,000). Other defaults include the option of whether to use the intercept, the type of generated networks to use as comparison for testing the null hypothesis, as well as some more nuanced refinements to the network simulations. By default, netlm() includes an intercept in the regression. This is standard for a regression and its removal only should be done under a very limited set of circumstances, as doing so would be unrealistic in most situations. The nullhyp options include a variety of QAP and CUG simulations that will be used to simulate networks to be used to test the null. At the time of this writing, qap (the default) and qapspp function identically and use the double semi-partialling approach discussed earlier. However, the netlm() function also allows users to apply selectively some less-used variations of QAP. In many cases, the other options will have a higher Type I error rate than the double semi-partialling approach. Under some - very limited - conditions, users may wish to independently permute each of the predictors (qapallx), or permute one predictor at a time while holding the others constant (qapx). On most occasions, QAP permutes the predictor variables. If there is a good reason for doing so, users may alternatively elect to permute the outcome (qapy). Under other special conditions, users also are offered a variety of conditional uniform graph simulation methods, such as simulating networks of the same order (cug), density (cugden), or tie distribution (cugtie). There is even an option to run a regression without simulating networks to test the null (classical). However, we do not recommended this under any circumstances since the fact that you are using network data violates the assumptions behind the least squares modeling being used to assess significance, which substantially raises the probability of committing a Type I error. The remaining parameters that may be set offer means of tweaking a regression either to have closer comparisons for hypothesis testing or to produce estimates under difficult circumstances. The test.statistic argument allows the user to select between using a \"t.value\" distribution and a \"beta\" distribution when generating simulated networks. The simulated networks that are generated to test the null hypothesis are drawn from a universe of possible networks, had some things been just a little different. This approach is somewhat like looking at models of alternate realities, each of which is based on the one that we have observed in the networks we are analyzing. Monte Carlo procedures are used to generate the simulated networks. Rather than subscribe to the anything is possible line of thinking (a uniform probability distribution), the simulated networks are based on the originals with most simulations bearing a fairly strong resemblance to the original. They are drawn from a realm of possibilities that has most of the networks possible morphologies being just a little different and progressively fewer being very different. The way that the possible network morphologies or permutations are distributed should reflect the actual probabilities that the networks may take on such structures. The problem with this is that we almost never possess such knowledge. It is very difficult to know what could have been under unlimited hypothetical scenarios. That said, in some cases, we might have some idea that there should be even odds as to whether the network could have deviated in one direction or another. Alternatively, perhaps, we may have prior knowledge that chances are better that the networks would be different in a particular way than another. In such cases, we can design the comparisons to be either evenly distributed around the original network or in a manner that favors a particular morphology over another. That is the purpose behind the two test.statistic options. The t-distribution should be adequate, under most circumstances, when selecting a distribution for the test statistic. The t distribution (\"t.value\") provides a symmetric probability distribution, meaning that any value drawn at random from the distribution is equally likely to be either above or below the distributions mean. The beta distribution (beta), on the other hand, is used when analysts have prior information that gives them reason to believe that certain states are more likely than are others. It can take a range of shapes, including symmetric, with the shape depending on the prior information gained through - in this case - parameters of the network being analyzed. When it takes on a skewed shape, it assigns a greater probability for values falling in one direction rather than the other - greater or less than the median. In either case, the simulated networks affect only the networks that are generated for hypothesis testing purposes. An argument that may affect the regression estimates of the intercept and beta values is tolerance. By default, tolerance is set very close to zero. The tolerance is small because it is used as a parameter that guides the algorithm in assessing whether calculations have begun to improve so little in each iteration that we may judge then to have converged upon the answer. In essence, the tolerance is a standard used as a stopping point for the algorithm. The clear tradeoff is that smaller tolerance values may take longer to converge, but greater values may - sometimes substantially - decrease accuracy. The netlm() function tolerance defaults to tol = 1e-7. We do not recommend users to modify this parameter; it should be adequate under most circumstances. If a model fails to converge, we strongly recommended that users first reevaluate the model, as that is frequently the source of the problem. Possible reasons for failure to converge include multicollinearity or an outcome with too little variation. But, if there is some reason why the model must remain as-is, then the tolerance may be adjusted upward by small increments. However, the upper tolerance limit for the tolerance should be somewhere around 1e-5 or 1e-4 and the output should be regarded with some skepticism at that point. 13.4.2.4 Estimating a QAP OLS Multiple Regression in R For an example of multiple OLS regression, consider the class friendship and interaction networks introduced at the beginning of the chapter. Neither of these networks has valued ties, but we can create a network with tie values by adding network ties together. In this case, we will add three types of ties that express how much of an affinity people have for one another: people they know (know); people they like (buddy); and people they feel close with (friend). When added together, we have a friendship scale that ranges from not friendly (0) to very close (3). This step is accomplished as follows: # Make a matrix to use as a dependent variable friendship &lt;- as.matrix(know) + as.matrix(buddy) + as.matrix(friend) We can now estimate a QAP multiple regression since we have a network with valued ties (friendship) to use as an outcome. For our independent networks, we will use three: one that captures students who worked together in groups (groupWork), one that indicates students who had classes together (haveClass), and one that indicates students who tended to talk with and work with one another (WorkWith). Each of the predictor variables are binary, meaning that all tie values in the networks are either 1 (a tie is present), or 0 (no tie exists). The resulting model may be written as: \\[Y_{friendship}= \\alpha + \\beta_{groupWork} X_{groupWork} + \\beta_{haveClass} X_{haveClass} + \\beta_{WorkWith} X_{WorkWith}\\] When using the netlm() functions defaults, running the model is fairly straightforward. set.seed(8675309) nols &lt;- netlm(friendship, list(groupWork, haveClass, WorkWith)) print(nols) OLS Network Model Coefficients: Estimate Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|) (intercept) 0.09013486 0.799 0.201 0.262 x1 0.54500391 1.000 0.000 0.000 x2 0.49071928 1.000 0.000 0.000 x3 1.01990958 1.000 0.000 0.000 Residual standard error: 0.6979 on 458 degrees of freedom F-statistic: 190.2 on 3 and 458 degrees of freedom, p-value: 0 Multiple R-squared: 0.5547 Adjusted R-squared: 0.5518 We can call the output for a QAP multiple regression using either the print() or summary() commands. The print() command, which we use in our example, generates only the standard regression output, while the summary() command provides additional diagnostic measures, as well as which type of simulated network was used to create the null hypothesis distribution and the number of simulated networks in the distribution. The regression output from netlm() does not retain or include the names of the variables used. When running a number of regressions, this can quickly become a problem, since many of the regressions will look alike. It is, therefore, important to either retain the R script used to run the test, make a note of the order in which you entered the variables, or both. The F-statistic essentially tells us whether the model is more informative than if we were to simply consider the average tie value from the outcome variable (\\(\\mu_{friendship} = 1.10\\)). In this case, the F-statistic is significant (\\(p\\leq 0.05\\)). Thus, we can reject the null hypothesis that there is no difference between the information gained thorough using the model instead of the average tie value and conclude that the model is informative. We may, therefore, move on to consider the estimates that the model produced. In the output from the friendship model, under Coefficients: x1 represents the first variable in the model (groupWork), x2 represents the second (haveClass), and x3 the third (WorkWith). It is apparent from the two-tailed p-values (Pr(&gt;=|b|)) that each of the three predictors is statistically significant (\\(p\\leq 0.05\\)), telling us that all three variables predict (or, if you prefer, explain) the tie values in the outcome network. The intercept, on the other hand, is not statistically significant and therefore cannot be interpreted. We can, however, plug the estimates into the model at this point. \\[Y_{friendship}= 0.09 + 0.55 *X_{groupWork} + 0.49 * X_{haveClass} + 1.02 * X_{WorkWith}\\] The interpretation of the variables in this model generally is done variable by variable, while holding the others constant. For example, we can interpret the variable WorkWith as predicting, the value of a friendship tie would increase by 1.02 if students work and talk together in class, holding the other two variables in the model constant. In other words, if two students share a tie in the WorkWith network, then we would predict that the value of the tie that they share in the friendship network would be at least 1.02, regardless of whether they have ties in the haveClass or the groupWork networks. In addition, because all of the predictors in this network are measured in the same way (all three are binary networks), we can also see that that WorkWith ties appear to be more strongly associated with friendship tie values than the other two types of ties. If we measured the predictor networks on different scales, then we could not draw such a conclusion. Finally, we may wish to note the R-squared value. R-squared is a rule of thumb, and may be regarded an approximate measure of the variation in the outcome that is accounted for by the model. This value gives us an idea of how good our model predicts the outcome we are analyzing. In this case, we could state that this model accounts for roughly 55% of the variation in the value friendship ties in this network. Again, keep in mind that this is just a rule-of-thumb indicator and should not be taken too seriously. 13.4.2.5 Logistic Regression Using QAP Logistic regression was designed to predict or explain binary outcomes. It is, therefore, best used to analyze networks with binary ties (i.e., tie values take a value of 0 or 1). Like multiple OLS regression, the purpose of logistic regression is to describe the relationship between some outcome and the predictor variables that may relate to it. The major difference between the two is that logistic regression is designed for binary outcomes and, therefore, predicts the likelihood of a tie forming in the outcome network. 13.4.2.6 Options and Defaults in QAP Logistic Regression The help section for netlm() offers the following defaults for the function: netlogit(y, x, intercept = TRUE, mode = &quot;digraph&quot;, diag = FALSE, nullhyp = c(&quot;qap&quot;, &quot;qapspp&quot;, &quot;qapy&quot;, &quot;qapx&quot;, &quot;qapallx&quot;, &quot;cugtie&quot;, &quot;cugden&quot;, &quot;cuguman&quot;, &quot;classical&quot;), test.statistic = c(&quot;z-value&quot;,&quot;beta&quot;), tol = 1e-7, reps = 1000) As you can see, there is very little difference between the options in netlogit() and netlm() functions. The two differences are the y variable and the test.statistic. Because logistic regression is designed specifically for binary data, there is no problem with using a network object for the y variable. The netlogit() function will read network variables correctly, as being dichotomous. It remains important, however, to recheck the nature of the data being tested in order to account for directed networks and whether or not loops are present. The other difference, the test statistics available used for modeling the simulated networks, is in the available distributions. The notable difference is the use of the z-distribution, rather than the t-distribution, as the default option. This is because z is better suited for use with dichotomous outcomes than the t-distribution. It is also a symmetric distribution and thus models values above and below the median as being equally likely. The beta-distribution is available in both netlm() and netlogit() since it we can use it to describe dichotomous or continuous data. As mentioned earlier, it allows for either symmetric or skewed distributions. Depending on prior information, as is present in the network data, the beta distribution may give more weight to values either above or below the median. For most users, the test statistic options may not be necessary. Nevertheless, it is still important for those using the netlogit() function to review the options carefully and modify all that apply to the analysis being conducted and the networks being used to do so. For more information on each of the arguments within the netlogit() function, review the options and defaults section under the netlm() section of this text. 13.4.2.7 Estimating a QAP Logistic Regression in R Estimating a logistic regression in R can be deceptively simple. If the observed network is directed and has no loops, then all that may be required is to list the networks being used as outcome and predictors. For an example, consider the organizational convergence network used in the QAP correlation example earlier in the chapter. Assuming that the organization would be interested in explaining why colleagues collaborate on projects, the other two networks could prove useful. As it happens, the networks used in this example are undirected. In addition to the outcome variable (y) and predictor variables (x), it is important for the sake of accuracy to identify the networks as undirected (mode = \"graph\"). All other default settings are applicable to this analysis, so no further specification is necessary. set.seed(8675309) nlog &lt;- netlogit(collab, list(commun, esteem), mode = &quot;graph&quot;) #Examine the results print(nlog) As with OLS QAP regression, the logistic QAP model for networks does not retain or include the names of variables. In the model above, recall that the outcome is collaboration, x1 is the network of communication ties, and x2 is the network of esteem ties. As with netlm(), either print() or summary() will call the regression output, with the latter calling additional diagnostic output. The chi-square test of fit improvement provides a loose indication of model fit. A significant p-value for the chi-square test of fit improvement tends to indicate relatively good model fit. Although this is a good place to start in evaluating the model, there are other indications that also should be considered. In this case, the p-value is less than the conventional 0.05 threshold, indicating that there is some reason to believe that the model may be a good fit. Each of the predictor variables, communication and esteem, are also statistically significant. The netlogit() output provides two forms of estimates. The coefficients listed under Estimate are the log odds of a tie in the output network, and the Exp(b) coefficients are the odds ratios, calculated by taking the exponential function of the log odds. For example, the exponential function of the estimate for community ties may be calculated by entering exp(2.920187). The odds ratios are provided for the users convenience. We can see that the estimates of the log odds for each of the variables is positive, indicating that the likelihood of a tie is more likely than not in each case. We can get an idea of just how much more likely when considering the odds ratios. For example, the odds ratio for communication is 18.5, indicating that - holding all other variables constant - a collaboration tie is 18.5 times more likely between offices that share communication ties than with those that do not. We can see that, as may be expected, communication ties appear to offer greater odds of creating a collaboration tie than do esteem ties in this network. The pseudo R-square values offer a rough approximation of how well the predictors account for what we can observe in the outcome. We should note, however, that pseudo R-square measures are usually fairly conservative and, as with OLS regression, far from exact. 13.5 A Note on interpretation: Causality in Networks Whenever employing statistical techniques to draw inference about the mechanisms that appear to drive the network in some fashion, it is best to be very careful when making statements about causes, regardless of whether or not the interaction, correlation, or other feature has been statistically verified.4 Generally, what analysts should keep in mind when working with social networks is that actual causes will be difficult, at best, to demonstrate (Doreian 2001). Nevertheless, if causes truly are what they are pursuing in their work, a few words of guidance may help. Analysts can build solid analyses from a strong command of the context, culture, and actors under consideration. Background research is the worst topic on which to cut corners when analyzing networks. Preparation in terms of learning about the network will pay dividends when it comes time to interpret the results of any test or measure that one performs on a network. Similarly, preparation in terms of knowing theories that others have formed in their work with the same or somewhat similar networks will help to set up expectations and should alert analysts to situations that are surprising for their deviation from the norm. It is always helpful to know when something unexpected is taking place, and one generally will not recognize such things in the absence of background work. Background research functions primarily as a filter through which to interpret analyses. In order to establish causal mechanisms, four essential factors must be in place (adapted from Shadish and Campbell (2002)) A plausible argument Correlated predictor and outcome Temporal precedence Account for/rule out alternate explanations The first item is relatively straightforward. It sets out that the causal chain should be logically consistent and hold true to the facts concerning the background and actors involved in the network. The next item indicates there must be some relationship between the variables implied by the argument. At this point is where statistics become more important. For there to be a relationship between the assumed cause and its effect, there must be some measurable relationship between the two, such as a correlation. Correlations, however, are necessary but not sufficient grounds for establishing a causal relationship. The next hurdle (item 3) is to establish temporal precedence. The cause must precede the effect, and that precedence must be demonstrable. The last criterion to satisfy on this list is likely to be the most difficult. For one to consider a relationship causal, it is important to account for all of the other possible causes of the outcome in question. That is not to say that analysts have to counter every absurd claim they can invent; only that they should consider all plausible explanations and rule them out if possible. If they cannot rule out all alternate explanations, then they will need to temper their conjectures in light of these alternative explanations. If, and only if, all of the above conditions can be satisfied, analysts may feel at least somewhat justified in making a claim to actual causation. However, they should always keep in mind that there will always remain some possibility - however small - of having made the wrong call. Preparation and rigor will decrease those chances considerably. But they will never disappear. 13.6 Summary: Lessons Learned In this chapter, we have offered a basic introduction to testing hypotheses about networks. We began by exploring the nature of hypothesis testing is and then illustrated it using two statistical models for social network data. As we saw, both models can prove invaluable when analysts are confronted with the problem of too many variables; that is, when they have identified several factors that could be associated with a particular outcome, but cannot distinguish which ones truly are from those that only appear to be. CUG and QAP models are not the only statistical models available for testing hypotheses. Analysts can leverage many other techniques such as exponential random graph models (ERGMs) and stochastic actor-oriented models (SAOMs) to develop and test hypotheses regarding tie formation. The former provide analysts with a way to examine the endogenous and exogenous social processes that give rise to a networks observed patterns at the macro level. They assume that observed social networks are built upon local patterns of ties, which are a function of local social processes. SAOMs are similar in that they assume that observed social networks are built upon local patterns of ties. However, they differ in that they are designed for longitudinal social network data and assume that tie formation reflects the choices of actors who seek to form ties with other actors. We will consider ERGMs and SAOMs in future chapters. Readers should take away (and keep in mind) several important points about hypothesis testing. The first is that hypotheses require more reflection than exploratory questions because they require analysts to identify their independent and dependent variables and the hypothesized relationship between them. This process can be informed by exploratory analysis, but it also must take into account theoretical considerations derived from previous studies and subject matter expertise. The second is that hypothesis testing can provide greater insight into many important factors and processes related to networks that cannot be found using exploratory approaches. Nevertheless, we must still be careful about claiming objective truth from the results of our hypothesis testing. We simply should be more confident about certain dynamics regarding the networks we are studying. Finally, we should consider hypothesis testing as a good investment of our time as opposed to it simply being an overly academic approach. To be sure, analyses that lead to well thought-out hypotheses will generally take more time than basic exploratory approaches. However, they can help prevent many of the errors that can arise from purely descriptive approaches by forcing us to consider alternative explanations and challenge what our intuition as to what we think is true. References "],["exponential-random-graph-models-ergms.html", "14 Exponential Random Graph Models (ERGMs) 14.1 Introduction 14.2 ERGMs With Undirected Data 14.3 ERGMs With Directed Data", " 14 Exponential Random Graph Models (ERGMs) 14.1 Introduction Place a header at the top of your script that tells you what you called it, what it accomplishes, etc. ################################################# # What: ERGMs (statnet) # Created: 02.28.19 # Revised: 01.25.22 ################################################# Exponential random graph models (ERGMs) are popular statistical methods for social networks that allow researchers to examine and test theories regarding a range of effects that may lead to tie formation in a given network. The idea is that social networks are built upon local structures, such as reciprocal pairs and close triads, that can be modeled as network configurations, which ultimately serve as independent variables in a model. This class of models allows researchers to test theories about which endogenous (i.e., structural) and exogenous (e.g., actor attributes, spatial factors, and other networks) factors facilitate the growth of relations among actors in a given network. For instance, a researcher may have a theory (perhaps through data visualization, observation, or research) that preferential attachment is a major social process (i.e., the ways by which actors form ties) within a network. While preferential attachment might be of main theoretical interest, the researcher also may see evidence and hypothesize that social exchange, homophily (e.g., gender-based homophily), and multiplexity contribute to tie formation and maintenance in the observed network. ERGMs allow researchers to test such theories. The power of ERGMs is that they allow the researcher to test these theories simultaneously and at multiple network levels (e.g., individual, dyad, triad, and subgroup). Underlying this approach are several theoretical assumptions about social networks (Lusher, Koskinen, and Robins 2013). First, social networks emerge from local processes. This assumption suggests that such processes take place and produce macro-structures and patterns. Second, network ties are self-organizing in that there are complex dependencies in which the presence of one tie may affect the presence of another. As indicated above, actor attributes and other exogenous factors (e.g., other networks, the spatial distance between actors) can affect tie formation as well. Third, the presence of patterns (e.g., more reciprocity than we would expect by chance) is evidence that there is some sort of underlying process. Finally, no single process is likely to explain tie formation in any given empirical network, which means multiple processes can occur simultaneously, and lead to macro-structures. For instance, reciprocity, triadic closure, homophily, and multiplexity may be at play in an observed network. Thus, an important point is to build a model that takes into account several processes, including both endogenous and exogenous, because it is not always clear what processes are at play. The process for developing and estimating an ERGM can be simplified into a few important steps (Lusher, Koskinen, and Robins 2013: 10). The first thing for a researcher to do is to specify an ERGM based on their theoretical interests or empirically established patterns. For example, a researcher interested in a mutual exchange of information and/or resources can operationalize this process using a configuration for reciprocity, which serves as an independent variable. The key is to find a set of configurations built around theory and incorporate appropriate control variables. The researcher can then apply this model to their observed network, which will lead to the estimation of parameters (and a standard error) for each configuration. After selecting the appropriate parameters, the focus shifts to getting the model to converge, which is often very difficult to do (especially for large networks). Once a model has converged, the parameters allow researchers to make inferences about the network patterns in the data, which in turn permits him or her to make inferences about the types of social processes that are important to the network. For researchers using MPNet and PNet (open-source software not available in R) and examining several models, one can compare them by looking at the Mahalanobis distance produced in a goodness of fit (GOF) report. Similarly, those using statnet can compare models by evaluating the Akaikes Information Criteria (AIC) and Bayesian Information Criteria (BIC) scores (the lower, the better) for each model before moving onto GOF analyses. Finally, one needs to examine how well a model fits the observed data by examining the goodness of fit (GOF) model diagnostic. The basic idea of GOF is to assess how well a model captures features of the data that you did not explicitly model. Lusher et al. (2013) explain that this approach allows us to see if what we modeled is enough to explain the observed network and accounts for general processes observed in many networks. The GOF procedure, given we have a converged model, begins with simulations to generate a distribution of graphs. If the fitted model sufficiently explains a feature of the data, then we can expect that the summary for the corresponding feature in the data is not extreme in the distribution of graphs. General Procedures in R (statnet) The following is an outline of the general procedure for running ERGMs in R with statnet: Import network (and attribute data if necessary) Prepare data Simplify relationships (e.g., dichotomize, remove multiple relationships, remove self-loops, etc.). Set attributes to vertices. Explore your data Visualize to develop hypotheses (if you dont have any already). Calculate descriptive statistics (e.g., centrality), produce histograms for distributions of stats of interest, correlations among attributes, etc. Decide on the type of model and estimate model(s) Standard model, social selection, social influence, etc. Obtain converged model(s) (e.g., mcmc diagnostics) Compare models by evaluating the AIC and BIC scores Run goodness of fit diagnostics Interpret and report results 14.2 ERGMs With Undirected Data 14.2.1 Data and Setup Load statnet. library(statnet) Our undirected data are from active Provisional Irish Republican Army (PIRA) members between 1970 and 1998. These data are drawn from longitudinal and cross-sectional information collected by the International Center for the Study of Terrorism at Pennsylvania State University. The PIRA network comprises the following four types of relationships: (1) involvement in a PIRA activity together, (2) friends before joining the PIRA movement, (3) blood relatives, and (4) related through marriage. Each relation is treated as a tie and coded whether a tie exists between two members or not. Thus, the networks have, conceptually and technically, binary and symmetric relations between members (Gill et al. 2014). In all, network data were collected for six different periods. Here, we will use the final two periods. Period six will serve as our dependent variable. In addition, ties between individuals in period five still active in period six will serve as one of the independent variables. Lets load the data from the two periods. load(&quot;data/pira.RData&quot;) Next, lets visualize the networks. par(mfrow = c(1, 2)) gplot(pira5.net, usearrows = FALSE, main = &quot;PIRA - Period 5&quot;) gplot(pira6.net, usearrows = FALSE, main = &quot;PIRA - Period 6&quot;) 14.2.2 PIRA ERGMs We will start with a simple model and slowly build upon it. While we recommend starting with theory when using ERGMs, in this section we will focus on code and mechanics to get you started. Then, in the following section of this exercise (ERGMs with Directed Data), we will leverage exploratory analysis to develop models more systematically. Estimating ERGMs in statnet is accomplished by using the ergm() function, which is used to fit models. To do so, the analyst must provide a formula (through the aptly named argument formula) of the given network (y) as a function of the terms that represent the network configurations we hypothesize may occur more or less likely than expected by chance (Hunter et al. 2008). This expression then would take the following form: y ~ &lt;model term 1&gt; + &lt;model term 2&gt; + .... Later on in this document, we will explore the possible configuration terms. For now, you may want to keep in mind that you can get a list of the available terms by typing ?'ergm-terms'. As the documentation for these terms is extensive, you may prefer to narrow down the scope by searching for specific terms: search.ergmTerms(keyword = &quot;activity&quot;) We will begin estimating our model using the edges term, which represents the most basic configuration (the number of edges in the network) and controls for the tendency for individuals to form ties. While this term may give some information about such a tendency, it is similar to a constant (a.k.a., intercept) term and is often not interpreted on its own. pira6.01 &lt;- ergm(pira6.net ~ edges) The output is an object of ergm class consisting of multiple elements. Calling the output object (pira6.01) produces a minimal printout with information on the formula used to estimate the ERGM and a score of the coefficients for the model parameters. The summary() function formats the output to present the formula, coefficients, and the values of AIC and BIC scores in an easier to interpret format. pira6.01 Call: ergm(formula = pira6.net ~ edges) Maximum Likelihood Coefficients: edges -3.583 summary(pira6.01) Call: ergm(formula = pira6.net ~ edges) Maximum Likelihood Results: Estimate Std. Error MCMC % z value Pr(&gt;|z|) edges -3.5832 0.1057 0 -33.91 &lt;1e-04 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Null Deviance: 4717.6 on 3403 degrees of freedom Residual Deviance: 845.8 on 3402 degrees of freedom AIC: 847.8 BIC: 854 (Smaller is better. MC Std. Err. = 0) Now, lets add some additional terms. In addition to edges, our model should include variables to account for triadic processes (e.g., open or closed triads) and degree distributions. To that end, weve included the terms kstar (i.e., the tendency for actors to form n ties), gwdegree (i.e., centralization based on high-degree nodes), gwesp (i.e., propensity for higher-order closure whereby nodes with multiple shared contacts form a tie), and dyadcov(i.e., the effect of one type of edge on the dependent network) to account for the desired predictor configurations. Notice that each term is a function with corresponding attributes, which enable the analyst to tune the behavior of each. For example, the kstar term includes an argument (k) that adds one network statistic to the model for each element of the distinct integer vector provided. In the example below, we have specified that the ERGM should include counts of the number of 3 and 4-stars in the network. Once again, a fuller description of these terms can be accessed via the ?'ergm-terms' command. pira6.02 &lt;- ergm(pira6.net ~ edges + kstar(k = 3:4) + gwdegree(decay = 0.693, fixed = TRUE) + gwesp(decay = 0.693, fixed = TRUE) + dyadcov(x = pira5.net)) summary(pira6.02) Call: ergm(formula = pira6.net ~ edges + kstar(k = 3:4) + gwdegree(decay = 0.693, fixed = TRUE) + gwesp(decay = 0.693, fixed = TRUE) + dyadcov(x = pira5.net)) Monte Carlo Maximum Likelihood Results: Estimate Std. Error MCMC % z value Pr(&gt;|z|) edges -11.00185 1.15019 0 -9.565 &lt; 1e-04 *** kstar3 0.63786 0.12021 0 5.306 &lt; 1e-04 *** kstar4 -0.24910 0.05669 0 -4.394 &lt; 1e-04 *** gwdeg.fixed.0.693 5.41450 1.07962 0 5.015 &lt; 1e-04 *** gwesp.fixed.0.693 0.48519 0.18422 0 2.634 0.00844 ** dyadcov.pira5.net 5.48141 0.37414 0 14.651 &lt; 1e-04 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Null Deviance: 4718 on 3403 degrees of freedom Residual Deviance: 442 on 3397 degrees of freedom AIC: 454 BIC: 490.8 (Smaller is better. MC Std. Err. = 0.2355) With our model in hand, lets now turn our attention to assessing model convergence. We want to make sure we develop a model that is a solid representation of our observed data. As such, use the mcmc.diagnostics() function to help us assess if our simulated networks differ from our observed network. In other words, we want to see if our model can reproduce the observed network. If not, we run into the issue of model degeneracy and need to make changes. The mcmc.diagnostics() function takes an ergm object to produce diagnostics on model fit. The extensive output should include a printout of diagnostic information and diagnostic plot for MCMC sampled statistics produced from the fit (Handcock et al. 2021). Here, we will focus on interpreting the plots for demonstration purposes. If the model has converged, these graphs should vary stochastically around the mean of 0, where 0 represents the value for each term in the observed data (Harris 2013). Simply, the trace plots on the left should show a relatively tight fuzzy caterpillar and the distribution of the sample statistics on the right should have a nice bell curve that is centered on zero (or at least close). mcmc.diagnostics(pira6.02) Sample statistics summary: Iterations = 2445312:10010624 Thinning interval = 4096 Number of chains = 1 Sample size per chain = 1848 1. Empirical mean and standard deviation for each variable, plus standard error of the mean: Mean SD Naive SE Time-series SE edges -1.0498 10.636 0.24740 0.6560 kstar3 -14.5005 184.166 4.28409 10.6227 kstar4 -17.6981 255.503 5.94354 13.9889 gwdeg.fixed.0.693 -0.3946 4.032 0.09379 0.1724 gwesp.fixed.0.693 -1.1028 14.381 0.33453 0.9174 dyadcov.pira5.net -0.3139 4.022 0.09356 0.2117 2. Quantiles for each variable: 2.5% 25% 50% 75% 97.5% edges -19.000 -9.000 -2.0000 6.000 21.00 kstar3 -306.000 -152.000 -36.0000 90.250 405.30 kstar4 -398.650 -208.500 -51.0000 128.250 576.48 gwdeg.fixed.0.693 -8.486 -3.085 -0.4639 2.403 7.41 gwesp.fixed.0.693 -26.392 -11.436 -2.3438 7.437 31.64 dyadcov.pira5.net -8.000 -3.000 0.0000 2.000 7.00 Are sample statistics significantly different from observed? edges kstar3 kstar4 gwdeg.fixed.0.693 diff. -1.0497835 -14.5005411 -17.6980519 -0.3946490 test stat. -1.6003375 -1.3650488 -1.2651470 -2.2894470 P-val. 0.1095237 0.1722377 0.2058186 0.0220534 gwesp.fixed.0.693 dyadcov.pira5.net Overall (Chi^2) diff. -1.1028421 -0.3138528 NA test stat. -1.2021793 -1.4827732 10.0808114 P-val. 0.2292941 0.1381347 0.1242034 Sample statistics cross-correlations: edges kstar3 kstar4 gwdeg.fixed.0.693 edges 1.0000000 0.9329188 0.8919462 0.8291020 kstar3 0.9329188 1.0000000 0.9905262 0.6177002 kstar4 0.8919462 0.9905262 1.0000000 0.5767340 gwdeg.fixed.0.693 0.8291020 0.6177002 0.5767340 1.0000000 gwesp.fixed.0.693 0.9005596 0.8986172 0.8647306 0.6335978 dyadcov.pira5.net 0.7039098 0.5915828 0.5472817 0.6324009 gwesp.fixed.0.693 dyadcov.pira5.net edges 0.9005596 0.7039098 kstar3 0.8986172 0.5915828 kstar4 0.8647306 0.5472817 gwdeg.fixed.0.693 0.6335978 0.6324009 gwesp.fixed.0.693 1.0000000 0.6632554 dyadcov.pira5.net 0.6632554 1.0000000 Sample statistics auto-correlation: Chain 1 edges kstar3 kstar4 gwdeg.fixed.0.693 gwesp.fixed.0.693 Lag 0 1.0000000 1.0000000 1.0000000 1.00000000 1.0000000 Lag 4096 0.6393878 0.6075981 0.5463356 0.34813655 0.6652378 Lag 8192 0.5055016 0.4804466 0.4272812 0.23861430 0.5377435 Lag 12288 0.4025102 0.3754633 0.3259815 0.14869759 0.4282905 Lag 16384 0.3218468 0.2962404 0.2563440 0.15001733 0.3386914 Lag 20480 0.2397688 0.2358050 0.2098159 0.09474202 0.2580701 dyadcov.pira5.net Lag 0 1.0000000 Lag 4096 0.6015808 Lag 8192 0.4142869 Lag 12288 0.3020510 Lag 16384 0.2132892 Lag 20480 0.1549427 Sample statistics burn-in diagnostic (Geweke): Chain 1 Fraction in 1st window = 0.1 Fraction in 2nd window = 0.5 edges kstar3 kstar4 gwdeg.fixed.0.693 0.4154 0.1812 0.1011 1.3998 gwesp.fixed.0.693 dyadcov.pira5.net 0.5352 0.5221 Individual P-values (lower = worse): edges kstar3 kstar4 gwdeg.fixed.0.693 0.6778385 0.8561816 0.9194937 0.1615861 gwesp.fixed.0.693 dyadcov.pira5.net 0.5925253 0.6016216 Joint P-value (lower = worse): 0.2363757 . MCMC diagnostics shown here are from the last round of simulation, prior to computation of final parameter estimates. Because the final estimates are refinements of those used for this simulation run, these diagnostics may understate model performance. To directly assess the performance of the final model on in-model statistics, please use the GOF command: gof(ergmFitObject, GOF=~model). Lets try a slightly different model here swapping out gwdegree with altkstar, which is closer to the model estimated using MPNet (see the slides from class). In theory, gwdegree and altkstar should produce the same results, but in practice, the statistic produced by these terms is affected by the lambda and decay parameters that we set. pira6.03 &lt;- ergm(pira6.net ~ edges + kstar(k = 3:4) + altkstar(lambda = 2, fixed = TRUE) + gwesp(decay = 0.693, fixed = TRUE) + dyadcov(x = pira5.net)) summary(pira6.03) Call: ergm(formula = pira6.net ~ edges + kstar(k = 3:4) + altkstar(lambda = 2, fixed = TRUE) + gwesp(decay = 0.693, fixed = TRUE) + dyadcov(x = pira5.net)) Monte Carlo Maximum Likelihood Results: Estimate Std. Error MCMC % z value Pr(&gt;|z|) edges -0.13152 1.13644 0 -0.116 0.9079 kstar3 0.63889 0.11701 0 5.460 &lt;1e-04 *** kstar4 -0.24884 0.05573 0 -4.465 &lt;1e-04 *** altkstar.2 -2.72635 0.52887 0 -5.155 &lt;1e-04 *** gwesp.fixed.0.693 0.47794 0.18737 0 2.551 0.0107 * dyadcov.pira5.net 5.49734 0.36595 0 15.022 &lt;1e-04 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Null Deviance: 4717.6 on 3403 degrees of freedom Residual Deviance: 441.9 on 3397 degrees of freedom AIC: 453.9 BIC: 490.7 (Smaller is better. MC Std. Err. = 0.2431) Compare these results to those estimated by MPNet: Table 14.1: MPNet Estimates. Statistic MPNet Estimate Edge N/A Star3A 0.6262 Star4A -0.2424 Isolated Edges 0.2347 Alternating Star -2.6902 Alternating Triangle 0.4525 Prior Period Ties 5.4827 Once again, run the model convergence diagnostics using the mcmc.diagnostics() function. Does our model represent our data well enough? mcmc.diagnostics(pira6.03) Now, lets do a goodness of fit test and plot the results. Again, GOF allows us to assess whether a model represents important graph features. The function used to calculate GOF is gof(), which calculates p-values for geodesic distance, degree, and reachability summaries to diagnose the goodness-of-fit for a given model. pira6.03.gof &lt;- gof(pira6.03) pira6.03.gof Goodness-of-fit for degree obs min mean max MC p-value degree0 0 0 2.48 8 0.16 degree1 48 31 38.26 49 0.06 degree2 16 13 24.21 33 0.06 degree3 1 2 6.45 14 0.00 degree4 10 0 2.18 7 0.00 degree5 1 0 1.23 5 1.00 degree6 0 0 1.35 5 0.50 degree7 2 0 2.12 6 1.00 degree8 4 0 2.67 8 0.58 degree9 0 0 1.90 7 0.20 degree10 1 0 0.15 2 0.28 Goodness-of-fit for edgewise shared partner obs min mean max MC p-value esp0 58 48 60.10 76 0.72 esp1 22 2 15.67 30 0.44 esp2 2 0 7.90 17 0.16 esp3 7 0 6.69 16 1.00 esp4 2 0 1.31 6 0.68 esp5 1 0 0.23 2 0.36 Goodness-of-fit for minimum geodesic distance obs min mean max MC p-value 1 92 70 91.90 117 1.00 2 209 99 207.63 329 0.90 3 278 85 337.66 664 0.72 4 247 83 380.19 767 0.38 5 164 51 292.11 659 0.28 6 59 38 176.40 441 0.06 7 18 7 97.46 236 0.08 8 0 0 52.57 178 0.04 9 0 0 28.33 125 0.24 10 0 0 14.28 72 0.64 11 0 0 6.46 56 1.00 12 0 0 2.47 41 1.00 13 0 0 0.62 21 1.00 14 0 0 0.16 9 1.00 15 0 0 0.02 2 1.00 Inf 2336 473 1714.74 2926 0.24 Goodness-of-fit for model statistics obs min mean max MC p-value edges 92.00000 70.00000 91.90000 117.00000 1.00 kstar3 465.00000 118.00000 455.79000 909.00000 0.90 kstar4 575.00000 143.00000 560.58000 1221.00000 0.86 altkstar.2 151.25391 78.57031 150.38152 233.75781 0.92 gwesp.fixed.0.693 42.93568 15.00000 42.12749 72.05897 0.86 dyadcov.pira5.net 52.00000 42.00000 51.90000 61.00000 1.00 For simplicity, we will only interpret the last output of the printout (the Goodness-of-fit for model statistics table, see printout below). The model simulations built into the gof() function can compare the network characteristics for simulated networks and the observed network; then, the observed frequencies for each statistic can be compared against those simulated (Harris 2013). The model statistics comparison table includes five columns of information; as an aside, these variables and their interpretation can also be used in comparing the degree, edgewise shared partner, and minimum geodesic distance tables. The first column contains the name for each statistic. The second, obs includes the number of nodes in the observed network with the listed statistic. Notice that the edges statistic should correspond to the number of edges in the network (you can always check using network.edgecount(pira6.net)). The third, fourth, and fifth columns (min, mean, and max) represent the minimum, maximum and average number for a given statistic across the simulated networks. Finally, and perhaps most importantly, the MC p-value column includes the proportion of the simulated values for a given term that are at least as extreme as the observed value. Here, large p-values indicate that the simulated networks are not significantly different from the observed network; on the other hand, small p-values (less than 0.05) would be interpreted as demonstrating a significant difference between the simulated and observed statistic (Harris 2013). In our output, the p-values are close to one, so our model did well at predicting the terms in our model, which we expected. Determining how well a model fits require evaluating the other tables. Generally speaking, the fewer p-values of less than 0.5 the better. obs min mean max MC p-value edges 92.00000 70.00000 91.90000 117.00000 1.00 kstar3 465.00000 118.00000 455.79000 909.00000 0.90 kstar4 575.00000 143.00000 560.58000 1221.00000 0.86 altkstar.2 151.25391 78.57031 150.38152 233.75781 0.92 gwesp.fixed.0.693 42.93568 15.00000 42.12749 72.05897 0.86 dyadcov.pira5.net 52.00000 42.00000 51.90000 61.00000 1.00 To aid the process of determining model fit, we can also plot the output of our GOF evaluation using the plot() function. Note that each plot represents each GOF test. The x-axis on each plot represents a given statistic, while the y-axis represents the proportion of nodes in the observed network with the proportion of nodes in the simulated network with the same characteristics (Harris 2013). The thick black line in each graph represents the value for the observed network, while the grey box shows the 95% confidence interval for the simulated networks. When the observed values fall within the confidence interval, the simulated networks are capturing the configurations in the observed network and we assume the model fits well. par(mfrow = c(2, 2)) plot(pira6.03.gof) 14.2.3 Compare to QAP logistic regression In order to estimate the regression, use the netlogit() function from the sna package. Luckily, the sna package is loaded with statnet. Note that the pira5.net coefficients for logistic regression are quite similar to the one for the two ERGMs that were estimated. nlog &lt;- netlogit(y = pira6.net, x = list(pira5.net), mode = &quot;graph&quot;) print(nlog) Network Logit Model Coefficients: Estimate Exp(b) Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|) (intercept) -4.410371 0.01215067 0.166 0.834 0.166 x1 5.417176 225.24210526 1.000 0.000 0.000 Goodness of Fit Statistics: Null deviance: 4717.56 on 3403 degrees of freedom Residual deviance: 515.7963 on 3401 degrees of freedom Chi-Squared test of fit improvement: 4201.763 on 2 degrees of freedom, p-value 0 AIC: 519.7963 BIC: 532.0611 Pseudo-R^2 Measures: (Dn-Dr)/(Dn-Dr+dfn): 0.5525173 (Dn-Dr)/Dn: 0.8906646 14.3 ERGMs With Directed Data 14.3.1 Data and Setup This analysis relies on data from the Armed Conflict Location and Event Dataset (ACLED), which provides a comprehensive list of political events in Africa by country from 1997 to 2014 (ACLED 2015; Raleigh and Dowd 2015). The dataset contains several types of violent events among rebel groups, political militias, protesters, and civilians, among others. ACLED contains temporal data, including the dates on which events occurred, along with spatial data at various levels of analysis, such as at country, provincial, and municipality levels. The relational data consists of directed, negative or violent relations among rebel groups contained in the ACLED dataset. This analysis derived these relations from two-mode networks connecting various groups via direct conflict on one another. We supplement the ACLED dataset with open-source information about six important attributes or characteristics of 86 rebel organizations. The first is each organizations primary ideology, which captures systems of beliefs that serve as its primary motivation for conflicts, such as whether they adhere to separatist or religious goals (we placed each into a numeric category). The second attribute is each groups home base, which is the primary country from which the group operates or functions. Each groups size (i.e., the most recent estimate of its number of combatants) and founding date (i.e., the year in which the group came into being) are treated as attributes as well. We based some of the group size estimates on conflicting or limited information, which is a well-known challenge when analyzing violent groups. The authors of the original dataset split the difference between min and max estimates in cases where the best estimates for a groups size are ranges (Cunningham, Everton, and Tsolis 2017). For example, we would have coded a group estimated to comprise 10,000 to 15,000 members as 12,500 members. To collect and structure these data, the authors relied mainly on the Big Allied and Dangerous Dataset Version 2 (Asal and Rethemeyer 2016), and the Terrorism Research and Analysis Consortium (TRAC 2016). Finally, we define each groups turf as the location of each rebel organizations ethnic group. Specifically, each groups turf is defined by the geospatial polygon(s) associated with their ethnic group. The original authors did this by using the Ethnicity Felix 2001, a spatial data file consisting of 1,927 polygons, which are based on the Peoples Atlas of Africa by Marc Felix and Charles Meur (2001) and depict the dominant ethnicities and languages for Africa. In this analysis, two rebel groups are considered to share the same turf if their ethnic group was located in the same polygon. In other words, for them to share the same turf they do not have to belong to the same ethnic group. All that matters is whether their respective ethnic groups share the same location. Similarly, two rebel groups were considered to share adjacent turfs if their respective ethnic groups are located in adjacent polygons. Lets import the data, which weve stored as a matrix. Here we will work with directed data, so be sure to use the directed = TRUE argument within the as.network() function to indicate that our data are directed. teo_mat &lt;- as.matrix( read.csv(&quot;data/TEO_Matrix.csv&quot;, row.names = 1, check.names = FALSE) ) teos_net &lt;- as.network(teo_mat, directed = TRUE, ignore.eval = TRUE, loops = FALSE, matrix.type = &quot;adjacency&quot;) Now take a look at your imported network object. teos_net Network attributes: vertices = 86 directed = TRUE hyper = FALSE loops = FALSE multiple = FALSE bipartite = FALSE total edges= 111 missing edges= 0 non-missing edges= 111 Vertex attribute names: vertex.names No edge attributes Lets import the attributes and take a look at them using the head() function. teo_attrs &lt;- read.csv(&quot;data/TEO Attributes.csv&quot;) head(teo_attrs) Node.ID Primary.Ideology Homebase Founding.Year Strength 1 ADFL 1 6 1996 750 2 ADF-NALU 4 20 1996 1000 3 AFRC - 1 1 16 1997 500 4 AFRC - 2 1 16 1998 100 5 AFRC - 3 1 16 1998 100 6 AIAI 4 17 1982 2000 Now, lets set the node attributes based on the teo_attrs object. set.vertex.attribute(teos_net, &quot;PrimaryIdeology&quot;, teo_attrs[, &quot;Primary.Ideology&quot;]) set.vertex.attribute(teos_net, &quot;Homebase&quot;, teo_attrs[, &quot;Homebase&quot;]) set.vertex.attribute(teos_net, &quot;FoundingYear&quot;, teo_attrs[, &quot;Founding.Year&quot;]) set.vertex.attribute(teos_net, &quot;Strength&quot;, teo_attrs[, &quot;Strength&quot;]) list.vertex.attributes(teos_net) [1] &quot;FoundingYear&quot; &quot;Homebase&quot; &quot;na&quot; &quot;PrimaryIdeology&quot; [5] &quot;Strength&quot; &quot;vertex.names&quot; We need to import the dyadic covariates for turf as well, which are stored as two matrix files in text format. Ultimately, we want them in network format. #Read in dyadic covariates (here I will only do ethnicity as turf): eth_adj_turf &lt;- as.network( data.matrix( read.table(&quot;data/TEO Turf (Ethnicity - Adjacent).txt&quot;) ) ) eth_same_turf &lt;- as.network( data.matrix( read.table(&quot;data/TEO Turf (Ethnicity - Same).txt&quot;) ) ) Go ahead and plot the network to see what it looks like. We will save the coordinates as well. coords &lt;- network.layout.fruchtermanreingold(teos_net, layout.par = NULL) gplot(teos_net, coord = coords, main = &quot;TEOs Network&quot;) 14.3.2 Explore Data As you may recall, one way to develop hypotheses is to look for visual patterns and leverage descriptive stats. What we have below are some useful ways to explore your data in the future before estimating ERGMs. Starting with visuals. Do you see any patterns (e.g, clustering based on attributes)? par(mfrow = c(2, 2)) gplot(teos_net, edge.col = &quot;gray&quot;, main = &quot;TEO Network - Ideology&quot;, vertex.col = get.vertex.attribute(teos_net, &quot;PrimaryIdeology&quot;), coord = coords) gplot(teos_net, edge.col = &quot;gray&quot;, main = &quot;TEO Network - Homebase&quot;, vertex.col = get.vertex.attribute(teos_net, &quot;Homebase&quot;), coord = coords) gplot(teos_net, edge.col = &quot;gray&quot;, main = &quot;TEO Network - Founding Year&quot;, vertex.col = get.vertex.attribute(teos_net, &quot;Founding.Year&quot;), coord = coords) gplot(teos_net, edge.col = &quot;gray&quot;, main = &quot;TEO Network - Strength&quot;, vertex.cex = scales::rescale(get.vertex.attribute(teos_net, &quot;Strength&quot;), to = c(0.5, 4)), coord = coords) One way of exploring your data is using the mixingmatrix() function to see relations among the various types of categories of actors. For instance, you can see that the ideology coded as 1 sends 6 ties (i.e., attacks) to organizations coded as adhering to ideology 2. Do any patterns emerge from inspecting this table? mixingmatrix(teos_net, &quot;PrimaryIdeology&quot;) To From 1 2 3 4 5 6 Sum 1 25 6 2 1 0 1 35 2 7 16 2 5 3 0 33 3 1 0 2 0 1 0 4 4 5 3 0 14 3 0 25 5 1 3 1 4 3 0 12 6 0 2 0 0 0 0 2 Sum 39 30 7 24 10 1 111 You can calculate reciprocity and transitivity to inform your model, too. data.frame(&quot;Measure&quot; = c(&quot;Dyadic reciprocity&quot;, &quot;Edgewise reciprocity&quot;, &quot;Transitivity&quot;, &quot;Average degree&quot;), &quot;Score&quot; = round(c(grecip(teos_net), grecip(teos_net, measure = &quot;edgewise&quot;), gtrans(teos_net), mean(degree(teos_net))), digits = 4) ) Measure Score 1 Dyadic reciprocity 0.9866 2 Edgewise reciprocity 0.5586 3 Transitivity 0.1187 4 Average degree 2.5814 Another useful set of descriptive measures with directed data is triad census. When we use the triad.census() function, we can get an idea about social processes involving triads. We can compare the output with the image below. Triad Census triad.census(teos_net) 003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210 [1,] 95787 3906 2491 24 8 29 28 38 4 0 18 0 0 3 4 300 [1,] 0 14.3.3 TEO ERGMs The analysis below consists of three models examining violent tie formation among African TEOs. The micro-configurations selected in the models were based upon Cunningham, Everton, and Tsoliss (2017) original paper, and then the selection of them was further modified based on the presentation by Callaghan, Cunningham, and Everton (2017) at the 1st North American Social Networks Conference (NASNC) in August 2017. The purpose of this section is to demonstrate how to implement ERGMs. Note you can use help('ergm-terms') to see what type of terms or configurations are available to include in your model. Alternatively, vignette('ergm-term-crossRef'), provides you with a cross-referenced HTML version that might be more helpful. It can take a while to become acquainted with these terms. One useful approach is to start with theoretically relevant endogenous effects (i.e., structural) and then add exogenous effects. The first model contains only the former. To be specific, we use the previously noted edges term, which controls for the tendency for individuals to form ties. Additionally, we use the mutual (i.e., the number of pairs of actors {I, J} for which IJ and JI ties both exist), gwodegree (i.e., the geometrically weighted out-degree distribution captures the levels of activity for nodes sending ties), and gwesp (i.e., propensity for higher-order closure whereby nodes with multiple shared contacts form a tie) terms to generate our first model. model_1 &lt;- ergm(teos_net ~ edges + mutual + gwodegree(0.5, fixed = TRUE) + gwesp(0.5, fixed = TRUE), #Here we up the number of max iterations to get convergence.) control = control.ergm(MCMLE.maxit = 40)) Observe the results. summary(model_1) Call: ergm(formula = teos_net ~ edges + mutual + gwodegree(0.5, fixed = TRUE) + gwesp(0.5, fixed = TRUE), control = control.ergm(MCMLE.maxit = 40)) Monte Carlo Maximum Likelihood Results: Estimate Std. Error MCMC % z value Pr(&gt;|z|) edges -5.1176 0.2361 0 -21.672 &lt;1e-04 *** mutual 5.0517 0.3522 0 14.345 &lt;1e-04 *** gwodeg.fixed.0.5 0.1338 0.3836 0 0.349 0.727 gwesp.fixed.0.5 0.6500 0.1425 0 4.561 &lt;1e-04 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Null Deviance: 10133.8 on 7310 degrees of freedom Residual Deviance: 930.7 on 7306 degrees of freedom AIC: 938.7 BIC: 966.3 (Smaller is better. MC Std. Err. = 0.7859) Check for model degeneracy. mcmc.diagnostics(model_1) We can use the gof() function to examine our models. For directed networks, it is a good idea to examine GOF for degree distributions (i.e., idegree), geodesic distributions (i.e., distance), shared partners and edges (i.e., espartners and dspartners), and triad census (i.e., triadcensus), among others (Lusher, Koskinen, and Robins 2013). To do so, we can pass a formula to the GOF argument specifying the terms that should be used to diagnose the GOF of the model. ERGM_1_gof &lt;- gof(model_1, GOF = ~ distance + idegree + espartners + dspartners + triadcensus) Lets plot the results. We can see the model does a fairly good job with most of the configurations. par(mfrow = c(2, 3)) plot(ERGM_1_gof) Now, lets run the second model, which includes actor attributes and dyadic covariates. Once again, we add new terms with relevant attributes to declare the specific configurations desired. In this second model, we use the following terms: nodeifactor with PrimaryIdeology: which captures the number of times a node with an attribute appears as the terminal node on a directed tie (Handcock et al. 2021). Here, we identify the first level (levels = 1), which represents TEOs with Christian primary ideology, as the categorical attribute of interest. Taken together, this term examines whether nodes associated with Christianity have a higher or lower likelihood of receiving links. asymmetric with PrimaryIdeology: this term accounts for the number of pairs of actors {I, J} for which exactly one link, IJ or JI, exists. Here the focus is on nodes with Islamic as the primary ideology. The argument diff = TRUE tells the model to look at pairs with different PrimaryIdeology values. Taken together, this term examines the potential asymmetry between Islamic TEOs and those of different ideologies. nodematch with Homebase: this is a homophily term. Here, we are specifying the attribute (Homebase) we want to examine whether nodes with the same home base have a likelihood to form edges between them. asymmetric with Homebase: much like the prior asymmetric term, this configuration is paired with a node attribute (Homebase). The aim here is to identify whether nodes with the same home bases are more or less likely to initiate edges with geographically proximal nodes. edgecov with eth_adj_turt and eth_same_turf: this term is used to include covariates; here, networks in which TEOs are connected if they share a turf as defined by ethnic group (eth_same_turf) or adjacent turfs (eth_adj_turf). model_2 &lt;- ergm(teos_net ~ edges + mutual + gwodegree (decay = 0.5, fixed = TRUE)+ gwesp(0.5, fixed = TRUE) + nodeifactor(&quot;PrimaryIdeology&quot;, levels = 1) + asymmetric(&quot;PrimaryIdeology&quot;, diff = TRUE, levels = 4) + nodematch(&quot;Homebase&quot;, diff = FALSE) + asymmetric(&quot;Homebase&quot;, diff = FALSE) + edgecov(eth_adj_turf) + edgecov(eth_same_turf), control = control.ergm(MCMLE.maxit = 40)) Examine your model. summary(model_2) Call: ergm(formula = teos_net ~ edges + mutual + gwodegree(decay = 0.5, fixed = TRUE) + gwesp(0.5, fixed = TRUE) + nodeifactor(&quot;PrimaryIdeology&quot;, levels = 1) + asymmetric(&quot;PrimaryIdeology&quot;, diff = TRUE, levels = 4) + nodematch(&quot;Homebase&quot;, diff = FALSE) + asymmetric(&quot;Homebase&quot;, diff = FALSE) + edgecov(eth_adj_turf) + edgecov(eth_same_turf), control = control.ergm(MCMLE.maxit = 40)) Monte Carlo Maximum Likelihood Results: Estimate Std. Error MCMC % z value Pr(&gt;|z|) edges -6.52243 0.34706 0 -18.793 &lt;1e-04 *** mutual 6.26134 0.61058 0 10.255 &lt;1e-04 *** gwodeg.fixed.0.5 0.51201 0.41423 0 1.236 0.2164 gwesp.fixed.0.5 0.03979 0.17029 0 0.234 0.8153 nodeifactor.PrimaryIdeology.1 -0.29553 0.20630 0 -1.433 0.1520 asymmetric.PrimaryIdeology.4 0.50450 0.57254 0 0.881 0.3782 nodematch.Homebase 1.02950 0.23057 0 4.465 &lt;1e-04 *** asymmetric.Homebase 1.94452 0.35685 0 5.449 &lt;1e-04 *** edgecov.eth_adj_turf 0.40838 0.20042 0 2.038 0.0416 * edgecov.eth_same_turf 1.31433 0.20995 0 6.260 &lt;1e-04 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Null Deviance: 10133.8 on 7310 degrees of freedom Residual Deviance: 679.3 on 7300 degrees of freedom AIC: 699.3 BIC: 768.3 (Smaller is better. MC Std. Err. = 0.5851) Check for model degeneracy. mcmc.diagnostics(model_2) Now lets use the gof() function to examine our second model. ERGM_2_gof &lt;- gof(model_2, GOF = ~distance + idegree + espartners + dspartners + triadcensus) Lets plot the results. We can see the model does a fairly good job with most of the configurations. par(mfrow = c(2, 3)) plot(ERGM_2_gof) Finally, lets look at the model comprising only significant configurations. model_3 &lt;- ergm(teos_net ~ edges + mutual + nodematch(&quot;Homebase&quot;, diff = FALSE) + asymmetric(&quot;Homebase&quot;, diff = FALSE) + edgecov(eth_adj_turf) + edgecov(eth_same_turf), control = control.ergm(MCMLE.maxit = 40)) summary(model_3) Call: ergm(formula = teos_net ~ edges + mutual + nodematch(&quot;Homebase&quot;, diff = FALSE) + asymmetric(&quot;Homebase&quot;, diff = FALSE) + edgecov(eth_adj_turf) + edgecov(eth_same_turf), control = control.ergm(MCMLE.maxit = 40)) Monte Carlo Maximum Likelihood Results: Estimate Std. Error MCMC % z value Pr(&gt;|z|) edges -6.3156 0.2737 0 -23.079 &lt;1e-04 *** mutual 6.2503 0.6284 0 9.947 &lt;1e-04 *** nodematch.Homebase 0.9415 0.2232 0 4.218 &lt;1e-04 *** asymmetric.Homebase 1.9787 0.3703 0 5.343 &lt;1e-04 *** edgecov.eth_adj_turf 0.3817 0.1937 0 1.971 0.0488 * edgecov.eth_same_turf 1.3031 0.2175 0 5.990 &lt;1e-04 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Null Deviance: 10133.8 on 7310 degrees of freedom Residual Deviance: 684.3 on 7304 degrees of freedom AIC: 696.3 BIC: 737.7 (Smaller is better. MC Std. Err. = 0.5309) Check for model degeneracy. We will show the full output here because we will discuss the results shortly. mcmc.diagnostics(model_3) Now lets use the gof() function to examine our third model. par(mfrow = c(2, 3)) ERGM_3_gof &lt;- gof(model_3, GOF = ~ distance + idegree + espartners + dspartners + triadcensus) Lets plot the results. As with the first two, we can see the model does a fairly good job with most of the configurations. par(mfrow = c(2, 3)) plot(ERGM_3_gof) This examination of inter-TEO violence in Africa suggests several interesting findings. We will focus, however, on interpreting Models 2 and 3 because the AIC and BIC measures are smaller for these models. To help with interpretation, we can convert the results to odds ratios. We will do this for Model 3 only. Note that your odds ratios might be slightly different than what you see below because they are based on the estimates. ORs &lt;- data.frame(&quot;Terms&quot; = names(model_3$coefficients), &quot;Odds&quot; = format(exp(model_3$coefficients), scientific = FALSE, digits = 2), row.names = NULL) ORs Terms Odds 1 edges 0.0018 2 mutual 518.1699 3 nodematch.Homebase 2.5638 4 asymmetric.Homebase 7.2330 5 edgecov.eth_adj_turf 1.4648 6 edgecov.eth_same_turf 3.6807 The structural parameters for edges and mutual (i.e., reciprocity) ties are statistically significant in both models. Again, we do not need to interpret the edges coefficient. The mutual variable represents a propensity for reciprocal violence among groups, which indicates that a group will respond violently against another group that has attacked it at some point. When holding other variables constant, the odds ratio indicates that a victim group is over 500 times more likely to reciprocate violence against its attacker than a group that would attack another without a prior connection. The gwesp and gwodegree configurations are not statistically significant in Model 2 and are excluded from Model 3. In terms of exogenous effects, both the nodeifactor for Christian ideology (nodeifactor.PrimaryIdeology.1) and the asymmetric for Islamist ideology (asymmetric.PrimaryIdeology.4) are not significant in our ERGM results and are excluded from Model 3. Yet, the nodematch (nodematch.Homebase) and asymmetric for home base (asymmetric.Homebase) actor covariates are positive and statistically significant in both models. When holding all other variables constant, the odds ratios for these processes indicate that groups from the same location are approximately 2-3 times more likely to engage in violence with one another than those who are from different locations. Moreover, they are about 7 times more likely to initiate violence against a geographically proximate group than attack groups from other locations. Both dyadic covariates are also positive and statistically significant. These latter results suggest that both adjacent turf and shared turf, based on ethnicity as well as what we saw with human-made political boundaries, such as states, leads to violence among African TEOs. Specifically, groups with the same ethnicity are approximately 1.5 times more likely to engage in violence with one another than those with different ethnicity, as well as 3-4 times more likely when they share political boundaries. 14.3.4 Appendix: ERGM Package and PNet Comparison We provide this appendix to demonstrate some important differences between PNet and the ergm package. Note estimates in bold indicate statistically significant effects and parentheses negative estimates. The differences may be due to several factors. Specifically, the programs differ in how they operationalize configurations as well as what options they make available. For instance, ergm calls what PNet calls Alternating Out Stars as gwodegree. For your reference, here is a table comparing the variables used in this exercise. Table 14.2: Comparison of Model Variables. PNet ergm Reciprocity Mutual Alternating Out Star Gwodegree Alternating Triangle - T GWESP Alternating Triangle - U Unknown Christian Receiver Nodeifactor.PrimaryIdeology.1 Islamic Interaction Asymmetric.PrimaryIdeology.4 Homebase Mismatch Nodematch.Homebase - Implemented as the opposite of mismatch Homebase Mismatch Reciprocity Asymmetric.Homebase Adjacent Turf Edgecov.eth_adj_turf Shared Turf Edgecov.eth_same_turf Model Comparison References "],["basic-base-r-and-tidyverse-data-manipulation-cheat-sheet.html", "15 Basic Base R and Tidyverse Data Manipulation Cheat Sheet 15.1 Basic Base R and dplyr Functions 15.2 Installation 15.3 Loading 15.4 Import and View Data 15.5 Extracting Variables/Columns 15.6 Creating Filters/Extracting Rows 15.7 Arrange Rows 15.8 Making a New Column/Variable 15.9 Rename Variables 15.10 Summarizing 15.11 Combining Data 15.12 Piping Multiple Variables", " 15 Basic Base R and Tidyverse Data Manipulation Cheat Sheet This documents purpose is to serve as a simple reference guide comparing a handful of basic functions in base R and the dplyr package, which is part of the tidyverse. It is designed to help you begin to understand some basic differences between the two as you learn R and come across different styles on the web and elsewhere. It does not provide you with all data manipulation (or wrangling and carpentry) techniques in R or in the tidyverse. Several other packages exist that are part of the tidyverse, such as tidyr, stringr, and purrr, and will support you with key functions and procedures depending on the type of data with which you are working and your analytic needs. In fact, we will most certainly work with these packages during the quarter. An excellent place to start exploring the tidyverse is https://bookdown.org, which contains useful references for a variety of topics. Finally, you can switch back and forth between base R and tidyverse in your analysis; they are not mutually exclusive. However, several advantages exist in staying consistent in your code. 15.1 Basic Base R and dplyr Functions   15.2 Installation The first step is to make sure that you have installed and loaded the necessary packages. # No installation required   install.packages(&quot;tidyverse&quot;) # Alternatively, only install dplyr and readr install.packages(&quot;dplyr&quot;) install.packages(&quot;readr&quot;) 15.3 Loading You can either install and load tidyverse, or you can load specific packages within it, such as dplyr. Though it is not required for this demonstration, we will use :: to call dplyr before using one of its functions, in part, because some packages have the same name for different functions and we think this is a useful practice when using multiple packages during an analysis, which will be the case most of time. # No loading required.   # You may load the whole tidyverse library(tidyverse) # Alternatively load packages from the tidyverse library(dplyr) library(readr) The tidyverse often utilizes piping (%&gt;%) to execute an action. You can think of the %&gt;% as saying and then followed by a function or action. The use of this operator is not required, but we will use it here. For example, you can think of a data frame object as a noun in a sentence, and the functions as verbs. Compare the following two morning routines to see the advantages of using %&gt;%to write programming instructions: go_work(get_ready(eat(wake_up(you))))   you %&gt;% # &#39;and then...&#39; wake_up() %&gt;% # &#39;and then...&#39; eat() %&gt;% # &#39;and then...&#39; get_ready() %&gt;% # &#39;and then...&#39; go_work() # &#39;and then...&#39; Which sentence makes more sense in understanding the sequence of events? Some prefer the syntax on the right as it improves readability. 15.4 Import and View Data Go ahead and load the data set for this walk through, which is a Twitter data set pulled from Twitters open API that focuses on the Popular Mobilization Forces (PMF) in Iraq. Some people feel more comfortable using base R functions to import data with read.csv(). df &lt;- read.csv(&quot;data/twitter_pmf.csv&quot;, header = TRUE) Here are some simple functions to view your data. # Get the dimensions of a data frame. dim(df) # Print column names in your data set. colnames(df) # Print a few rows of your data set. head(df) # Top rows tail(df) # Bottom rows # Get an object summary summary(df) # Observe your data set in a separate window. View(df)   To import csv files using the tidy framework, use the readr packages read_csv() function. tb &lt;- readr::read_csv(&quot;data/twitter_pmf.csv&quot;, col_names = TRUE) Inspect you data with glimpse() and others. # View the data in the console. tb %&gt;% dplyr::glimpse() # Describe dimensions. tb %&gt;% dplyr::dim_desc() # Use base R descriptive functions. tb %&gt;% head() tb %&gt;% colnames() tb %&gt;% summary() Note that in this cheat sheet we are not assigning the output from each operation into a new object. If the printed output in the console is not enough to compare the differences between the base R and tidy grammar, you may want to assign the output to new objects and inspect it in the viewer like so: my_df_head &lt;- head(df) View(my_df_head)   tb %&gt;% head() %&gt;% View() 15.5 Extracting Variables/Columns Extracting variables in base R uses the [ accessor in combination with c() to extract the desired columns. df[, c(&quot;screen_name&quot;, &quot;retweet_count&quot;, &quot;location&quot;, &quot;followers_count&quot;)] You can extract by column index in base R. The numbers indicate the column numbers (e.g., screen_name is column 4 in our data). # Combine indexes with c() df[, c(4, 14, 74, 78)]   Use dplyrs select() function to extract a handful of columns of interest. Note that variable names are unquoted and separated by commas. tb %&gt;% dplyr::select(screen_name, retweet_count, location, followers_count) You can use a column index as well. Note you can just declare the column indexes inside the select() function. tb %&gt;% dplyr::select(4, 14, 74, 78) You could take a look at the top 5 rows of the selected variables by combining functions. # Combine indexes with c() head(df[, c(4, 14, 78)], n = 5) screen_name retweet_count followers_count 1 warmediateam 1 3073 2 warmediateam 2 3073 3 warmediateam 2 3073 4 warmediateam 3 3073 5 warmediateam 3 3073   tb %&gt;% dplyr::select(4, 14, 78) %&gt;% head(n = 5) # A tibble: 5 x 3 screen_name retweet_count followers_count &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 warmediateam 1 3073 2 warmediateam 2 3073 3 warmediateam 2 3073 4 warmediateam 3 3073 5 warmediateam 3 3073 15.6 Creating Filters/Extracting Rows Here we will create a filter for all tweets that have been retweeted 10 or more times. Filtering in base R requires the [ accessor. # The $ accessor to gets the vector. df[df$retweet_count &gt;= 10, ] Additionally, the subset() function produces the same results. subset(df, retweet_count &gt;= 10)   Create filters using the filter() function. tb %&gt;% dplyr::filter(retweet_count &gt;= 10) You may use multiple filtering criteria by chaining the arguments with the | (or) and &amp; (and) operators. For instance, add a second filter to return only tweets from the account warmediateam. subset(df, retweet_count &gt;= 10 &amp; screen_name == &quot;warmediateam&quot;)   tb %&gt;% dplyr::filter(retweet_count &gt;= 10 &amp; screen_name == &quot;warmediateam&quot;) For a much clearer print out, combine the filtering techniques with the variable selection methods: # Pass limited data frame to subset(). subset(df[, c(4, 14, 74, 78)], retweet_count &gt;= 10 &amp; screen_name == &quot;warmediateam&quot;)   tb %&gt;% dplyr::filter(retweet_count &gt;= 10 &amp; screen_name == &quot;warmediateam&quot;) %&gt;% dplyr::select(4, 14, 74, 78) You could take a look at the top rows of the filtered output combining functions. head( subset(df[, c(4, 14, 78)], retweet_count &gt;= 10 &amp; screen_name == &quot;warmediateam&quot;), n = 5) screen_name retweet_count followers_count 17 warmediateam 22 3073 31 warmediateam 13 3073 33 warmediateam 13 3073 36 warmediateam 10 3073 37 warmediateam 10 3073   tb %&gt;% dplyr::filter(retweet_count &gt;= 10 &amp; screen_name == &quot;warmediateam&quot;) %&gt;% dplyr::select(4, 14, 78) %&gt;% head(n = 5) # A tibble: 5 x 3 screen_name retweet_count followers_count &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 warmediateam 22 3073 2 warmediateam 13 3073 3 warmediateam 13 3073 4 warmediateam 10 3073 5 warmediateam 10 3073 15.7 Arrange Rows Here we will arrange that data frame alphabetically by screen_name. Arranging in base R requires the [ accessor and the order() function. # Use of the $ accessor to order a variable. df[order(df$screen_name), ]   In dplyr use the arrange() function to sort in ascending order. tb %&gt;% dplyr::arrange(screen_name) You can clean up the printout by combining functions to select relevant variables and examine the top 5 rows. head( df[order(df$screen_name), c(&quot;screen_name&quot;, &quot;retweet_count&quot;)], n = 5) screen_name retweet_count 3729 5qxsN9lLdn1Nanl 0 3730 5qxsN9lLdn1Nanl 36 3731 5qxsN9lLdn1Nanl 0 3732 5qxsN9lLdn1Nanl 24 3733 5qxsN9lLdn1Nanl 0   tb %&gt;% dplyr::arrange(screen_name) %&gt;% dplyr::select(screen_name, retweet_count) %&gt;% head(n = 5) # A tibble: 5 x 2 screen_name retweet_count &lt;chr&gt; &lt;dbl&gt; 1 5qxsN9lLdn1Nanl 0 2 5qxsN9lLdn1Nanl 36 3 5qxsN9lLdn1Nanl 0 4 5qxsN9lLdn1Nanl 24 5 5qxsN9lLdn1Nanl 0 You may reverse the order of the variables arranged. Set the decreasing argument to TRUE. df[order(df$screen_name, decreasing = TRUE), ]   Include the desc() helper function. tb %&gt;% dplyr::arrange(desc(screen_name)) Once again, for a much clearer print out, combine the arranging techniques with the variable selection methods. head( df[order(df$screen_name, decreasing = TRUE), c(4, 14)], n = 5) screen_name retweet_count 4036 zaidaliallawi 0 4037 zaidaliallawi 0 4038 zaidaliallawi 0 4039 zaidaliallawi 0 4040 zaidaliallawi 0   tb %&gt;% dplyr::arrange(desc(screen_name)) %&gt;% dplyr::select(4, 14) %&gt;% head(n = 5) # A tibble: 5 x 2 screen_name retweet_count &lt;chr&gt; &lt;dbl&gt; 1 zaidaliallawi 0 2 zaidaliallawi 0 3 zaidaliallawi 0 4 zaidaliallawi 0 5 zaidaliallawi 0 15.8 Making a New Column/Variable The new variable we will create here, retweet_success, is for demonstration purposes. The variable will be generated by dividing the retweet_count by the followers_count. In base R, creating a new column requires using the $ accessor. # Assign the new variable into a data frame df$retweet_success &lt;- df$retweet_count/ df$followers_count   To create a column in dplyr use the mutate() function. tb %&gt;% dplyr::mutate(retweet_success = retweet_count/ followers_count) Once again, you may want to combine functions to get a clearer print out. Here, we will combine how to create and arrange a new variable, and select a handful of columns. # Create new variable df$retweet_success &lt;- df$retweet_count/ df$followers_count # Rearrange, select, and return top 5 head( df[order(df$retweet_success, decreasing = TRUE), c(&quot;screen_name&quot;, &quot;retweet_success&quot;)], n = 5) screen_name retweet_success 3930 MzcCY48mFeyS1ly Inf 2241 San_Patricio_BN 3342.643 9591 M_ska98 1613.333 4981 b76142 560.000 4148 8KXS2Iq3JAPEDvi 245.500   tb %&gt;% # Create new variable dplyr::mutate(retweet_success = retweet_count/ followers_count) %&gt;% # Rearrange, select, and return top 5 dplyr::arrange(desc(retweet_success)) %&gt;% dplyr::select(screen_name, retweet_success) %&gt;% head(n = 5) # A tibble: 5 x 2 screen_name retweet_success &lt;chr&gt; &lt;dbl&gt; 1 MzcCY48mFeyS1ly Inf 2 San_Patricio_BN 3343. 3 M_ska98 1613. 4 b76142 560 5 8KXS2Iq3JAPEDvi 246. 15.9 Rename Variables Lets now explore how to rename variables. Begin by taking a look at the column names, a straightforward way of doing so is using the base Rs colnames() function. colnames(df)   tb %&gt;% colnames() Now we will select several columns, and then rename them. names(df)[names(df) == &quot;screen_name&quot;] &lt;- &quot;Screen_Name&quot; names(df)[names(df) == &quot;retweet_count&quot;] &lt;- &quot;N_Retweets&quot; head(df[, c(&quot;Screen_Name&quot;,&quot;N_Retweets&quot;)], n = 5) Screen_Name N_Retweets 1 warmediateam 1 2 warmediateam 2 3 warmediateam 2 4 warmediateam 3 5 warmediateam 3   tb %&gt;% dplyr::select(screen_name, retweet_count) %&gt;% dplyr::rename(Screen_Name = screen_name, N_Retweets = retweet_count) %&gt;% head(n = 5) # A tibble: 5 x 2 Screen_Name N_Retweets &lt;chr&gt; &lt;dbl&gt; 1 warmediateam 1 2 warmediateam 2 3 warmediateam 2 4 warmediateam 3 5 warmediateam 3 15.10 Summarizing Summary statistics are really useful in describing your data. This can be done both in base R and in dplyr. Here we will explore how to execute simple summaries first; then, we will move on to calculate group summaries. 15.10.1 Simple Summary To summarize in base R, you will need generic functions to calculate these statistics. For example, mean(), median(), sum(), etc. # Create a data frame with the summary. data.frame(rt_avg = mean(df$retweet_count)) rt_avg 1 245.2969   To summarize data in dplyr, use the summarize() function to compute a requested summary (e.g., mean(), median(), n(), etc.). tb %&gt;% dplyr::summarize(rt_avg = mean(retweet_count)) # A tibble: 1 x 1 rt_avg &lt;dbl&gt; 1 245. 15.10.2 Group Summary Summarizing can be expanded by computing the statistics by groups. For instance, here we will get the retweet count of each individual; thus, the group would be each screen_name . Use the aggregate() function to slip the data into subsets and then proceed to compute summary statistics for each. aggregate(x = df$retweet_count, by = list(screen_name = df$screen_name), FUN = mean)   Group a data frame with group_by() and perform group operations by adding the summarize() function. tb %&gt;% dplyr::group_by(screen_name) %&gt;% dplyr::summarize(rt_avg = mean(retweet_count)) Once again, you may want to combine some functions to make your output more legible. out &lt;- aggregate(x = df$retweet_count, by = list(screen_name=df$screen_name), FUN = mean) head(out[order(out$x, decreasing = TRUE), ], n = 5) screen_name x 102 TheEisaAli 4909.646 43 EmmaDaly 4481.640 36 DCrising21 2916.490 97 San_Patricio_BN 1581.717 74 MikeyKayNYC 1506.260   tb %&gt;% dplyr::group_by(screen_name) %&gt;% dplyr::summarize(rt_avg = mean(retweet_count)) %&gt;% arrange(desc(rt_avg)) %&gt;% head(n = 5) # A tibble: 5 x 2 screen_name rt_avg &lt;chr&gt; &lt;dbl&gt; 1 TheEisaAli 4910. 2 EmmaDaly 4482. 3 DCrising21 2916. 4 San_Patricio_BN 1582. 5 MikeyKayNYC 1506. 15.11 Combining Data Up to this point, we have only covered single table functions. However, when data arrives in many pieces you may need to combine these to complete your analysis. Here we will bring a second data set, which contains social network analysis metrics for each user account listed in the initial data set. Once again, you can read the new data set with base R or readr. sna_df &lt;- read.csv(&quot;data/SNA_Stats.csv&quot;)   sna_tb &lt;- readr::read_csv(&quot;data/SNA_Stats.csv&quot;) Inspect both data sets: dim(sna_df) [1] 4146 7 colnames(sna_df) [1] &quot;screen_name&quot; &quot;componentnumber&quot; [3] &quot;Eccentricity&quot; &quot;closnesscentrality&quot; [5] &quot;harmonicclosnesscentrality&quot; &quot;betweenesscentrality&quot; [7] &quot;modularity_class&quot;   sna_tb %&gt;% dplyr::dim_desc() [1] &quot;[4,146 x 7]&quot; sna_tb %&gt;% colnames() [1] &quot;screen_name&quot; &quot;componentnumber&quot; [3] &quot;Eccentricity&quot; &quot;closnesscentrality&quot; [5] &quot;harmonicclosnesscentrality&quot; &quot;betweenesscentrality&quot; [7] &quot;modularity_class&quot; In order to connect the tables, a pair of variables called keys are required. One way to identify keys, is to seek for identically named variables in both data sets. This may not always be viable if the keys are named differently in each data set. Matching data frame names can be accomplished as follows: intersect(names(df), names(sna_df))   names(tb) %&gt;% dplyr::intersect(names(sna_df)) 15.11.1 Left Join What you see below is a join that retains all rows from the first table. Use merge() to join. Note that all.x = TRUE tells R to keep all observations from the first table. merge(df, sna_df, by = &quot;screen_name&quot;, all.x = TRUE)   The left_join() function retains all rows from the tb data frame, while adding data from sna_tb. tb %&gt;% dplyr::left_join(sna_tb, by = &quot;screen_name&quot;) Take a look at the dimensions of the output. dim( merge(df, sna_df, by = &quot;screen_name&quot;, all.x = TRUE)) [1] 10542 100   tb %&gt;% dplyr::left_join(sna_tb, by = &quot;screen_name&quot;) %&gt;% dplyr::dim_desc() [1] &quot;[10,542 x 97]&quot; 15.11.2 Right Join A right join retains all rows from the second table. Use merge() to join. Note that all.y = TRUE tells R to keep all observations from the second table. merge(df, sna_df, by = &quot;screen_name&quot;, all.y = TRUE)   The right_join() function retains all rows from the sna_tb data frame, while adding tb. tb %&gt;% dplyr::right_join(sna_tb, by = &quot;screen_name&quot;) Take a look at the dimensions of the output. dim( merge(df, sna_df, by = &quot;screen_name&quot;, all.y = TRUE)) [1] 14065 100   tb %&gt;% dplyr::right_join(sna_tb, by = &quot;screen_name&quot;) %&gt;% dplyr::dim_desc() [1] &quot;[14,065 x 97]&quot; 15.11.3 Inner Join An inner join retain rows with matches in both tables. SNA_Stats.csv, for example, may not include isolates so we would expect fewer results after comparing dimensions. Use the merge() function to inner join. merge(df, sna_df, by = &quot;screen_name&quot;)   inner_join() merges based on screen_name. tb %&gt;% dplyr::inner_join(sna_tb, by = &quot;screen_name&quot;) Take a look at the dimensions of the output. dim( merge(df, sna_df, by = &quot;screen_name&quot;)) [1] 10027 100   tb %&gt;% dplyr::inner_join(sna_tb, by = &quot;screen_name&quot;) %&gt;% dplyr::dim_desc() [1] &quot;[10,027 x 97]&quot; 15.11.4 Full Join A full join retains all rows in both data sets, regardless of matches. Use the merge() function to full join. Note the all = TRUE argument. merge(df, sna_df, by = &quot;screen_name&quot;, all = TRUE)   The full_join() function will merge based on shared screen_name. tb %&gt;% dplyr::full_join(sna_tb, by = &quot;screen_name&quot;) Take a look at the dimensions of the output. dim( merge(df, sna_df, by = &quot;screen_name&quot;, all = TRUE)) [1] 14580 100   tb %&gt;% dplyr::full_join(sna_tb, by = &quot;screen_name&quot;) %&gt;% dplyr::dim_desc() [1] &quot;[14,580 x 97]&quot; 15.12 Piping Multiple Variables As you can see, the %&gt;% operator is a great way to execute multiple actions in a few lines of code. This last example is meant to show how much can be done by chaining multiple functions with %&gt;%. For example, say we wanted to identify the most retweeted users in our original data set. To do so, we will have to group rows by screen_name using group_by(), then add the number of retweets per account, filter() by the average number of retweets (n = 245), arrange() the total, and finally rename the variables. tb %&gt;% dplyr::group_by(screen_name) %&gt;% dplyr::summarise(retweet_total = sum(retweet_count)) %&gt;% dplyr::filter(retweet_total &gt;= 245) %&gt;% dplyr::arrange(desc(retweet_total)) %&gt;% dplyr::rename(Screen_Name = screen_name, N_Retweets = retweet_total) # A tibble: 89 x 2 Screen_Name N_Retweets &lt;chr&gt; &lt;dbl&gt; 1 TheEisaAli 486055 2 EmmaDaly 448164 3 DCrising21 291649 4 MikeyKayNYC 150626 5 realsohelbahjat 145546 6 San_Patricio_BN 145518 7 m_al_asiri 81549 8 amnesty 80147 9 Protectthenhs 72937 10 YouTube 62145 # ... with 79 more rows We can clean up the output to only present the top accounts by adding dplyrs top_n() function. tb %&gt;% dplyr::group_by(screen_name) %&gt;% dplyr::summarise(retweet_total = sum(retweet_count)) %&gt;% dplyr::filter(retweet_total &gt;= 245) %&gt;% dplyr::arrange(desc(retweet_total)) %&gt;% dplyr::rename(Screen_Name = screen_name, N_Retweets = retweet_total) %&gt;% dplyr::top_n(15) # A tibble: 15 x 2 Screen_Name N_Retweets &lt;chr&gt; &lt;dbl&gt; 1 TheEisaAli 486055 2 EmmaDaly 448164 3 DCrising21 291649 4 MikeyKayNYC 150626 5 realsohelbahjat 145546 6 San_Patricio_BN 145518 7 m_al_asiri 81549 8 amnesty 80147 9 Protectthenhs 72937 10 YouTube 62145 11 nafarrao 44631 12 SulomeAnderson 43000 13 IraqiSecurity 38478 14 USEmbBaghdad 28770 15 The_H16 27462 "],["key-sna-functions-and-visualization-cheat-sheet.html", "16 Key SNA Functions and Visualization Cheat Sheet 16.1 igraph 16.2 statnet", " 16 Key SNA Functions and Visualization Cheat Sheet This documents purpose is to serve as a simple reference guide comparing a handful of basic, visualization-related functions in igraph and statnet. It designed to help you begin to understand some basic differences between the two packages as you learn R, but this is far from an exhaustive list of functions in the two programs. We will use the same data set for both - igraph and statnet - sections, which is a network comprised of violent interactions among African Violent Extremist Organizations (VEOs). Since you are likely to work with edge lists and matrices, we have included code for importing both. # Read edge list teo_el &lt;- read.csv(&quot;data/TEO_EL.csv&quot;) # Read matrix teo_mat &lt;- as.matrix( read.csv(&quot;data/TEO_Matrix.csv&quot;, header = TRUE, row.names = 1, check.names = FALSE) ) Take a look at the class for each object: class(teo_el) [1] &quot;data.frame&quot; class(teo_mat) [1] &quot;matrix&quot; &quot;array&quot; 16.1 igraph Assuming you have installed it already, we will load igraph first. library(igraph) 16.1.1 Creating a graph object in igraph Importing either object requires functions designed to work with the object class. For importing data.frames you can use the aptly named graph_from_data_frame() function. Transforming a matrix into a graph object in igraph requires the graph_from_adjacency_matrix() function. First, import the edge list: g_from_el &lt;- graph_from_data_frame(d = teo_el, # Is your data directed? directed = FALSE, # Will you include a data.frame with node # attributes? vertices = NULL) Now, import the matrix: g_from_mat &lt;- graph_from_adjacency_matrix(adjmatrix = teo_mat, # How should the matrix be interpreted? mode = &quot;undirected&quot;, # Create a weighted graph? weighted = NULL, # Should the diagonal be zeroed out? diag = FALSE) Inspect the newly created objects for their class: class(g_from_el) [1] &quot;igraph&quot; class(g_from_mat) [1] &quot;igraph&quot; As you can see, there are no differences between the two graphs, though they originated from differing data formats. difference(g_from_el, g_from_mat) IGRAPH d047084 UN-- 83 0 -- + attr: name (v/c) + edges from d047084 (vertex names): 16.1.2 Visualization Parameters and Layouts in igraph The plot function permits you to see the network data visually by recognizing the igraph class. teo_ig &lt;- graph_from_data_frame(d = teo_el, directed = FALSE, vertices = NULL) plot(teo_ig) This could be greatly improved! Now lets consider making some adjustments. Table 1 provides a summary of commonly used plotting parameters in igraph. See igraphs manual, Katya Ognyanovas excellent tutorial on SNA in igraph (https://kateto.net/networks-r-igraph), and igraphs website (https://igraph.org/r/) for a more comprehensive list of options. Table 1: Summary of Selected igraph Plotting Parameters Parameter Short Description vertex.color Adjusts node color. vertex.size Parameter for node size. Default is 15. vertex.shape Parameter for node shape (e.g., sphere, circle, square). Default is circle. vertex.label Parameter for adjusting and setting node labels. Use NA to omit. vertex.label.font Parameter for node font. Font: 1=plain, 2=bold, 3=italic, 4=bold italic, 5=symbol vertex.label.family Adjusts font family. Default is serif. vertex.label.cex Parameter for changing font size. vertex.label.color Parameter for adjusting node label colors. Default is black. edge.color Parameter for setting edge color. edge.width Sets edge width (default = 1). arrow.mode Sets arrow aesthetics: 0=no arrow, 1=back, 2=forward, 3=both. edge.arrow.size Sets edge arrow size (default = 1). edge.curved Edge curvature (ranges from 0-1). plot(teo_ig, # Modify vertices vertex.color = &quot;lightgreen&quot;, vertex.size = 10, vertex.shape = &quot;sphere&quot;, vertex.label.font = 0.25, label.family = &quot;Courier&quot;, vertex.label.cex = .75, vertex.label.color = &quot;darkblue&quot;, # Modify edges edge.color = &quot;black&quot;, edgewidth = 3, arrow.mode = 3, edge.arrow.size = 0.25, edge.curved = 0.5) The graph layout can vary. igraph provides multiple layout algorithms (e.g., Kamada-Kawai, circle, etc.), which can be usually located by typing the prefix layout_ or layout_with_ (e.g., layout_with_kk, layout_in_circle(), etc.). Note you can store a layout as an object (e.g., coords &lt;- layout_with_dh(teo_g)) and subsequently pass it to the plot() function as a parameter for the layout argument (e.g., plot(teog_g, layout = coords)), which we will do below. coords &lt;- layout_with_kk(teo_ig) Some of the commonly used layout options are outlined below, which you can find in igraphs help section. Table 2: Summary of Selected igraph Layout Parameters Parameter Short Description layout_with_dh Places vertices of a graph on the plane, according to the simulated annealing algorithm by Davidson and Harel. layout_in_circle Places vertices on a circle, in the order of their vertex ids.. layout_nicely This function tries to choose an appropriate graph layout algorithm for the graph, automatically, based on a simple algorithm. layout_with_fr Places vertices on the plane using the force-directed layout algorithm by Fruchterman and Reingold. layout_on_sphere Places vertices on a sphere, approximately uniformly, in the order of their vertex ids. layout_with_gem Places vertices on the plane using the GEM force-directed layout algorithm. layout_with_graphopt A force-directed layout algorithm, that scales relatively well to large graphs. layout_with_kk Places the vertices on the plane, or in the 3d space, based on a physical model of springs. layout_with_lgl A layout generator for larger graphs. layout_with_mds Multidimensional scaling of some distance matrix defined on the vertices of a graph. For instance, the visual below depicts the network using Fruchterman Reingold. Note weve turned off the labels so you can see the structure more clearly. plot(teo_ig, main = &quot;TEO Fruchterman Reingold Layout&quot;, layout = layout_with_fr, vertex.color = &quot;lightgreen&quot;, vertex.size = 10, vertex.shape = &quot;sphere&quot;, vertex.label = NA, edge.color = &quot;black&quot;, edgewidth = 3, arrow.mode = 3, edge.arrow.size = 0.25, edge.curved = 0.5) Now, plot the network using Kamada-Kawai. plot(teo_ig, main = &quot;TEO Kamada-Kawai Layout&quot;, layout = layout_with_kk, vertex.color = &quot;lightgreen&quot;, vertex.size = 10, vertex.shape = &quot;sphere&quot;, vertex.label = NA, edge.color = &quot;black&quot;, edgewidth = 3, arrow.mode = 3, edge.arrow.size = 0.25, edge.curved = 0.5) We can look at these side-by-side using par(mfrow = c(1, 2)), which tells igraph to create multiple plots along a single row with two columns. par(mfrow = c(1,2)) plot(teo_ig, layout = layout_with_fr, vertex.color = &quot;lightgreen&quot;, vertex.size = 10, vertex.shape = &quot;sphere&quot;, vertex.label = NA, edge.color = &quot;black&quot;, edgewidth = 3, arrow.mode = 3, edge.arrow.size = 0.25, edge.curved = 0.5, main = &quot;FR Layout&quot;) plot(teo_ig, # Use the stored coordinates layout = coords, vertex.color = &quot;lightgreen&quot;, vertex.size = 10, vertex.shape = &quot;sphere&quot;, vertex.label = NA, edge.color = &quot;black&quot;, edgewidth = 3, arrow.mode = 3, edge.arrow.size = .25, edge.curved = .5, main = &quot;KK Layout&quot;) # Add a legend to plot, for information use ?legend legend(x = 0, y = -2, legend = &quot;VEOs&quot;, pch = 21, pt.bg = &quot;lightgreen&quot;, pt.cex = 2, cex = 0.8, bty = &quot;n&quot;, ncol = 1) We will detach igraph before moving onto statnet. detach(&quot;package:igraph&quot;, unload = TRUE) 16.2 statnet Assuming you have installed it already, we will load statnet first. library(statnet) 16.2.1 Creating a graph object in statnet For importing data.frames you can use the aptly named as.network() function setting the argument matrix.type to \"edgelist\". Transforming a matrix into a graph object in network requires the as.network() function, but the argument must be set to matrix.type = \"adjacency\". First, import the edge list: g_from_el &lt;- as.network(teo_el, matrix.type = &quot;edgelist&quot;, directed = FALSE) Now, import the matrix: g_from_mat &lt;- as.network(teo_el, matrix.type = &quot;adjacency&quot;, directed = FALSE) Inspect the newly created objects for their class: class(g_from_el) [1] &quot;network&quot; class(g_from_mat) [1] &quot;network&quot; 16.2.2 Visualization Parameters and Layouts in statnet The gplot() function permits you to see the network data visually by recognizing the network class. teo_net &lt;- as.network(teo_el, matrix.type = &quot;edgelist&quot;, directed = FALSE) gplot(teo_net) This could be greatly improved! Table 3 provides a summary of commonly used plotting parameters in statnet. See the sna packages manual for an exhaustive list. Table 2: Summary of Selected sna Plotting Parameters Parameter Short Description vertex.col Adjusts node color. Red is default. vertex.cex Parameter for node size. displaylabels Parameter to turn on or turn off node labels (True or False). boxed.labels Indicate if you want labels to be enclosed in boxes. label.bg Background color for label boxes. label.pos Parameter for positioning labels. See manual for specifics. label.cex Parameter for changing font size. Default is 1. label.col Parameter for adjusting node label colors. Default is black. edge.col Parameter for setting edge color. edge.lwd Sets edge width. usearrows Parameter to turn on or turn off edge arrows (True or False). displayisolates Parameter to show or hide arrows (True or False). usecurve Edge curvature on or off. Now, plot the graph with some changes to the parameters. gplot(teo_net, # Modify vertex parameters vertex.col = &quot;lightgreen&quot;, vertex.cex = 1.5, displaylabels = TRUE, label.pos = 5, label.cex = 0.5, label.col = &quot;Blue&quot;, # Modify edge parameters edge.col = &quot;Gray&quot;, displayisolates = FALSE, usecurve = TRUE) You can change the layout in **statnet* as well. Here we will use mode to adjust our network layouts. Table 2: Summary of Selected sna Layout Parameters Parameter Short Description mode = \"spring\" Places vertices of a graph on the plane, according to the simulated annealing algorithm by Davidson and Harel. mode = \"springrepulse\" Places vertices on a circle, in the order of their vertex ids.. mode = \"kamadakawai\" This function tries to choose an appropriate graph layout algorithm for the graph, automatically, based on a simple algorithm. mode = \"fruchtermanreingold\" Places vertices on the plane using the force-directed layout algorithm by Fruchterman and Reingold.. mode = \"mds\" Multidimensional scaling of some distance matrix defined on the vertices of a graph. For instance, the visual below depicts the network using Fruchterman Reingold. Note weve turned off the labels so you can see the structure more clearly. gplot(teo_net, vertex.col = &quot;lightgreen&quot;, vertex.cex = 1.5, displaylabels = TRUE, label.pos = 5, label.cex = .5, label.col = &quot;Blue&quot;, edge.col = &quot;Gray&quot;, displayisolates = FALSE, usecurve = TRUE, mode = &quot;fruchtermanreingold&quot;) Now, with Kamada-Kawai. gplot(teo_net, vertex.col = &quot;lightgreen&quot;, vertex.cex = 1.5, displaylabels = TRUE, label.pos = 5, label.cex = .5, label.col = &quot;Blue&quot;, edge.col = &quot;Gray&quot;, displayisolates = FALSE, usecurve = TRUE, mode = &quot;kamadakawai&quot;) Keep in mind that you can use the gplot.layout.* functions to store coordinates and use them later. fr &lt;- gplot.layout.fruchtermanreingold(teo_net, layout.par = NULL) kk &lt;- gplot.layout.kamadakawai(teo_net, layout.par = NULL) par(mfrow = c(1, 2), mar = c(0, 0, 0, 0)) gplot(teo_net, vertex.col = &quot;lightgreen&quot;, vertex.cex = 1.5, displaylabels = FALSE, edge.col = &quot;Gray&quot;, displayisolates = FALSE, usecurve = TRUE, # Pass along the layout coord = fr) gplot(teo_net, vertex.col = &quot;lightgreen&quot;, vertex.cex = 1.5, displaylabels = FALSE, edge.col = &quot;Gray&quot;, displayisolates = FALSE, usecurve = TRUE, # Pass along the layout coord = kk) "],["references.html", "17 References", " 17 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
